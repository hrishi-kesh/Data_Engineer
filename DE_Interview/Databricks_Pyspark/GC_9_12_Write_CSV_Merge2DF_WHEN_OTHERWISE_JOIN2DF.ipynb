{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd0eadc-2201-4744-9756-99bb4a09173b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|   name|salary|dept_name|\n",
      "+-------+------+---------+\n",
      "|Charlie|  6000|       HR|\n",
      "|  David|  8000|       IT|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Save emp_data and dept_data in csv or write these df into csv and also find out the second highest salary department wise and write that into the csv as well\n",
    "\n",
    "# GC9 How to write into CSV | Databricks Tutorial |\n",
    "\n",
    "###This one for on premise jupyter: in any folder path type cmd and then jupyter notebook then jupyter notebook will open in that folder\n",
    "from pyspark.sql.functions import col, rank, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Sample data for employees and departments\n",
    "emp_data = [\n",
    "    (1, \"Alice\", 5000, 1),\n",
    "    (2, \"Bob\", 7000, 1),\n",
    "    (3, \"Charlie\", 6000, 1),\n",
    "    (4, \"David\", 8000, 2),\n",
    "    (5, \"Eve\", 9000, 2),\n",
    "    (6, \"Frank\", 7500, 2),\n",
    "]\n",
    "\n",
    "dept_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"IT\"),\n",
    "]\n",
    "\n",
    "# Define schema for employee and department data\n",
    "emp_columns = [\"id\", \"name\", \"salary\", \"dept_id\"]\n",
    "dept_columns = [\"dept_id\", \"dept_name\"]\n",
    "\n",
    "# Create DataFrames\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "window_spec=Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "ranked_df=emp_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "\n",
    "second_highest_df=ranked_df.filter(col(\"rnk\")==2)\n",
    "\n",
    "joined_df=second_highest_df.join(dept_df,\"dept_id\").select(col(\"name\"),col(\"salary\"),col(\"dept_name\")).show()\n",
    "\n",
    "emp_df.write.mode(\"overwrite\").csv(\"dbfs:/FileStore/EMP_DATA.csv\")# we can use append mode also\n",
    "\n",
    "dept_df.write.mode(\"overwrite\").csv(\"dbfs:/FileStore/DEPT_DATA.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9003ba75-4153-477f-800a-867a885854ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-----+\n",
      "|StudentName|Department|City|Marks|\n",
      "+-----------+----------+----+-----+\n",
      "|      Sagar|       CSE|  UP|   80|\n",
      "|     Shivam|        IT|  MP|   86|\n",
      "|       Muni|      Mech|  AP|   70|\n",
      "+-----------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GC10 How to merge two DataFrame using PySpark | Databricks Tutorial \n",
    "\n",
    "simpleData=[\n",
    "    (\"Sagar\",\"CSE\",\"UP\",80),\n",
    "    (\"Shivam\",\"IT\",\"MP\",86)]\n",
    "\n",
    "columns= [\"StudentName\",\"Department\",\"City\",\"Marks\"]\n",
    "df_1=spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df_1.show()\n",
    "\n",
    "simpleData_2=[\n",
    "    (\"Sagar\",\"CSE\",\"UP\",80),\n",
    "    (\"Muni\",\"Mech\",\"AP\",70)]\n",
    "\n",
    "columns= [\"Student_Name\",\"Department_Name\",\"City\",\"Marks\"]\n",
    "df_2=spark.createDataFrame(data=simpleData_2, schema=columns)\n",
    "df_2.show()\n",
    "\n",
    "df_1.union(df_2).show() #It is just like unionall\n",
    "\n",
    "df_3=df_1.union(df_2).distinct() #It will show unique rows. And union only apply when there is same no. of columns and datatypes also should be same.\n",
    "#It shows Column name of that df which comes 1st in union\n",
    "df_3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8621f0-118c-4e44-90ba-bb0d1b0fe756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-----+--------------+\n",
      "|StudentName|Department|City|Marks|         State|\n",
      "+-----------+----------+----+-----+--------------+\n",
      "|      Sagar|       CSE|  UP|   80| Uttar Pradesh|\n",
      "|     Shivam|        IT|  MP|   86|Madhya Pradesh|\n",
      "|       Muni|      Mech|  AP|   70|Andhra Pradesh|\n",
      "+-----------+----------+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GC11 How to use WHEN Otherwise in PySpark  | Databricks Tutorial |\n",
    "\n",
    "from pyspark.sql.functions import when,col\n",
    "df_4=df_3.withColumn(\"State\",when(col(\"City\")==\"UP\",\"Uttar Pradesh\").when(col(\"City\")==\"AP\",\"Andhra Pradesh\").when(col(\"City\")==\"MP\",\"Madhya Pradesh\").otherwise(\"Unknown\"))\n",
    "df_4.show()\n",
    "\n",
    "df_5=df_3.select(col(\"*\"),when(col(\"City\")==\"UP\",\"Uttar Pradesh\").when(col(\"City\")==\"AP\",\"Andhra Pradesh\").when(col(\"City\")==\"MP\",\"Madhya Pradesh\").otherwise(\"Unknown\").alias(\"State\"))\n",
    "df_5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260b305f-3df4-4215-a1eb-681a6801443d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+----+-----+\n",
      "| ID|Student_Name|Department_Name|City|Marks|\n",
      "+---+------------+---------------+----+-----+\n",
      "|  1|       Sagar|            CSE|  UP|   80|\n",
      "|  2|      Shivam|             IT|  MP|   86|\n",
      "|  3|        Muni|           Mech|  AP|   70|\n",
      "+---+------------+---------------+----+-----+\n",
      "\n",
      "+---+-----------+--------------+----+-----+\n",
      "| ID|StudentName|DepartmentName|City|Marks|\n",
      "+---+-----------+--------------+----+-----+\n",
      "|  1|      Sagar|           CSE|  UP|   80|\n",
      "|  3|       Muni|          Mech|  AP|   70|\n",
      "+---+-----------+--------------+----+-----+\n",
      "\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "| ID|Student_Name|Department_Name|City|Marks| ID|StudentName|DepartmentName|City|Marks|\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "|  1|       Sagar|            CSE|  UP|   80|  1|      Sagar|           CSE|  UP|   80|\n",
      "|  3|        Muni|           Mech|  AP|   70|  3|       Muni|          Mech|  AP|   70|\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "| ID|Student_Name|Department_Name|City|Marks| ID|StudentName|DepartmentName|City|Marks|\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "|  1|       Sagar|            CSE|  UP|   80|  1|      Sagar|           CSE|  UP|   80|\n",
      "|  3|        Muni|           Mech|  AP|   70|  3|       Muni|          Mech|  AP|   70|\n",
      "+---+------------+---------------+----+-----+---+-----------+--------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# GC12 How to join two DataFrames in PySpark | Databricks Tutorial |\n",
    "simpleData=[\n",
    "    (1,\"Sagar\",\"CSE\",\"UP\",80),\n",
    "    (2,\"Shivam\",\"IT\",\"MP\",86),\n",
    "    (3,\"Muni\",\"Mech\",\"AP\",70)]\n",
    "\n",
    "columns= [\"ID\",\"Student_Name\",\"Department_Name\",\"City\",\"Marks\"]\n",
    "df_1=spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df_1.show()\n",
    "\n",
    "simpleData_2=[\n",
    "    (1,\"Sagar\",\"CSE\",\"UP\",80),\n",
    "    (3,\"Muni\",\"Mech\",\"AP\",70)]\n",
    "\n",
    "columns= [\"ID\",\"StudentName\",\"DepartmentName\",\"City\",\"Marks\"]\n",
    "df_2=spark.createDataFrame(data=simpleData_2, schema=columns)\n",
    "df_2.show()\n",
    "\n",
    "df_1.join(df_2,df_1.ID==df_2.ID,\"inner\").show()  #we can use left right full self cross, if we don't give inner it will consider as inner by default\n",
    "\n",
    "df_1.alias(\"A\").join(df_2.alias(\"B\"),col(\"A.ID\")==col(\"B.ID\"),\"inner\").show() #another way of writting same prog\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GC_9_12_Write_CSV_Merge2DF_WHEN_OTHERWISE_JOIN2DF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
