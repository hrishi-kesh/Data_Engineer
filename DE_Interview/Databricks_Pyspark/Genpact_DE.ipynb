{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8158a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+\n",
      "|store_id|customer_id|purchase_amt|\n",
      "+--------+-----------+------------+\n",
      "|       1|        101|         800|\n",
      "|       2|        101|        1000|\n",
      "|       3|        101|        1000|\n",
      "|       3|        102|        1000|\n",
      "+--------+-----------+------------+\n",
      "\n",
      "Oracle Version: Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production\n",
      "(1, 101, 800)\n",
      "(2, 101, 1000)\n",
      "(3, 101, 1000)\n",
      "(3, 102, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Q1: Select top purchasing customer for each store based on total purchase amount\n",
    "# ctrl+/ or ctrl+shift+/ to comment all selected lines\n",
    "# Columns: store_id, customer_id, purchase_amt\n",
    "# Table: sales\n",
    "\n",
    "# Sample Input:\n",
    "# store_id, customer_id, purchase_amt\n",
    "# 1, 101, 500\n",
    "# 1, 101, 300\n",
    "# 1, 102, 500\n",
    "# 2, 101, 1000\n",
    "# 2, 102, 900\n",
    "# 3, 101, 1000\n",
    "# 3, 102, 1000\n",
    "\n",
    "# Expected Output:\n",
    "# 1, 101, 800\n",
    "# 2, 101, 1000\n",
    "# 3, 101, 1000\n",
    "# 3, 102, 1000\n",
    "\n",
    "# PySpark Solution:\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum as _sum, dense_rank\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Step 1: Aggregate total purchase amount per customer per store\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"TopPurchasingCustomer\").getOrCreate()\n",
    "\n",
    "# Create sample data as per the input\n",
    "data = [\n",
    "    Row(store_id=1, customer_id=101, purchase_amt=500),\n",
    "    Row(store_id=1, customer_id=101, purchase_amt=300),\n",
    "    Row(store_id=1, customer_id=102, purchase_amt=500),\n",
    "    Row(store_id=2, customer_id=101, purchase_amt=1000),\n",
    "    Row(store_id=2, customer_id=102, purchase_amt=900),\n",
    "    Row(store_id=3, customer_id=101, purchase_amt=1000),\n",
    "    Row(store_id=3, customer_id=102, purchase_amt=1000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(data)\n",
    "\n",
    "agg_df = sales_df.groupBy(\"store_id\", \"customer_id\").agg(_sum(\"purchase_amt\").alias(\"purchase_amt\"))\n",
    "\n",
    "# Step 2: Define window to rank customers by purchase_amt within each store\n",
    "window_spec = Window.partitionBy(\"store_id\").orderBy(col(\"purchase_amt\").desc())\n",
    "\n",
    "# Step 3: Add rank column\n",
    "ranked_df = agg_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "\n",
    "# Step 4: Filter to get top purchasing customers per store (can be ties)\n",
    "result_df = ranked_df.filter(col(\"rnk\") == 1).select(\"store_id\", \"customer_id\", \"purchase_amt\")\n",
    "result_df.show()\n",
    "\n",
    "# Oracle SQL Solution (for reference):\n",
    "# You can run this query using cx_Oracle after setting up your connection.\n",
    "\n",
    "import cx_Oracle\n",
    "import sys\n",
    "\n",
    "# Database connection details\n",
    "dsn_tns = cx_Oracle.makedsn('localhost', 1521, service_name='orcl')\n",
    "try:\n",
    "    conn = cx_Oracle.connect(user='Sivaacademy', password='securecode', dsn=dsn_tns)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check Oracle version\n",
    "    cursor.execute(\"SELECT * FROM v$version WHERE banner LIKE 'Oracle%'\")\n",
    "    version = cursor.fetchone()\n",
    "    print(f\"Oracle Version: {version[0]}\")\n",
    "\n",
    "    # Create global temporary table (compatible with Oracle 12c and later)\n",
    "    cursor.execute(\"\"\" DROP TABLE sales_temp PURGE\"\"\")\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE GLOBAL TEMPORARY TABLE sales_temp (\n",
    "            store_id NUMBER,\n",
    "            customer_id NUMBER,\n",
    "            purchase_amt NUMBER\n",
    "        ) ON COMMIT PRESERVE ROWS\n",
    "    \"\"\")\n",
    "\n",
    "    # Bulk insert sample data\n",
    "    sample_data = [\n",
    "        (1, 101, 500),\n",
    "        (1, 101, 300),\n",
    "        (1, 102, 500),\n",
    "        (2, 101, 1000),\n",
    "        (2, 102, 900),\n",
    "        (3, 101, 1000),\n",
    "        (3, 102, 1000)\n",
    "    ]\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT INTO sales_temp (store_id, customer_id, purchase_amt)\n",
    "        VALUES (:1, :2, :3)\n",
    "    \"\"\", sample_data)\n",
    "\n",
    "    # Commit inserts\n",
    "    conn.commit()\n",
    "\n",
    "    # Execute query\n",
    "    oracle_query = \"\"\"\n",
    "        WITH cte AS (\n",
    "            SELECT \n",
    "                store_id, \n",
    "                customer_id, \n",
    "                SUM(purchase_amt) AS purchase_amt, \n",
    "                DENSE_RANK() OVER (PARTITION BY store_id ORDER BY SUM(purchase_amt) DESC) AS rnk \n",
    "            FROM sales_temp \n",
    "            GROUP BY store_id, customer_id\n",
    "        )\n",
    "        SELECT store_id, customer_id, purchase_amt \n",
    "        FROM cte \n",
    "        WHERE rnk = 1\n",
    "    \"\"\"\n",
    "    cursor.execute(oracle_query)\n",
    "\n",
    "    # Fetch and print results\n",
    "    for row in cursor:\n",
    "        print(row)\n",
    "\n",
    "except cx_Oracle.DatabaseError as e:\n",
    "    error, = e.args\n",
    "    print(f\"Oracle Error {error.code}: {error.message}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    sys.exit(1)\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifth word: words\n",
      "Length of fifth word: 5\n",
      "Fifth word: words\n",
      "Length of fifth word: 5\n"
     ]
    }
   ],
   "source": [
    "#Q2 : Read a file and extract the fifth word from the fifth line\n",
    "# Read fifth line and extract the fifth word\n",
    "# And print the length of the fifth word and the word itself\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Text_File_Reading\").getOrCreate()\n",
    "\n",
    "with open(\"D:/Data_Engineer/DE_Interview/input.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "#Check if file has at least 5 lines - len() to get count of lines\n",
    "if len(lines) >= 5:\n",
    "    #strip() removes whitespace, split() converts string to list of words\n",
    "    fifth_line = lines[4].strip()\n",
    "    words = fifth_line.split()\n",
    "    #Check if fifth line has at least 5 words - len() for word count\n",
    "    if len(words) >= 5:\n",
    "        fifth_word = words[4]\n",
    "        print(f\"Fifth word: {fifth_word}\")\n",
    "        print(f\"Length of fifth word: {len(fifth_word)}\")\n",
    "    else:\n",
    "        print(\"Less than 5 words in line 5\")\n",
    "else:\n",
    "    print(\"Less than 5 lines in the file\")\n",
    "\n",
    "# Rewrite above code in pyspark Read the text file into a DataFrame\n",
    "# Read fifth line and extract the fifth word\n",
    "# And print the length of the fifth word and the word itself\n",
    "# Read text file into a DataFrame; each line becomes a row with a single column 'value' (string)\n",
    "text_df = spark.read.text(\"D:/Data_Engineer/DE_Interview/input.txt\")\n",
    "#Select 'value' column (the text of each line), limit to first 5 rows, and collect results to driver\n",
    "# - spark.read.text: Creates a DataFrame where each row is a line from the file, stored in 'value' column\n",
    "# - select(\"value\"): Picks only the 'value' column (text of each line)\n",
    "# - limit(5): Restricts DataFrame to first 5 rows (lines)\n",
    "# - collect(): Action that retrieves all rows as a list of Row objects to the driver (local Python)\n",
    "five_line_df = text_df.select(\"value\").limit(5).collect()\n",
    "\n",
    "# Check if at least 5 lines exist in the collected data\n",
    "# - len(five_line_df): Counts rows in the collected list (number of lines)\n",
    "# - Python logic since collect() brings data to driver\n",
    "if len(five_line_df) >= 5:\n",
    "    # Extract the 5th line's text (index 4, as lists are 0-based) from the 'value' column\n",
    "    # - five_line_df[4]: 5th Row object\n",
    "    # - [\"value\"]: Accesses the string in the 'value' column\n",
    "    fifth_line = five_line_df[4][\"value\"]\n",
    "    \n",
    "    # Split the 5th line into words using whitespace as delimiter\n",
    "    # - split(): Python string method, creates list of words\n",
    "    words = fifth_line.split()\n",
    "    \n",
    "    # Check if the 5th line has at least 5 words\n",
    "    # - len(words): Counts words in the split list\n",
    "    if len(words) >= 5:\n",
    "        # Get the 5th word (index 4, 0-based)\n",
    "        fifth_word = words[4]\n",
    "        \n",
    "        # Print the 5th word\n",
    "        print(f\"Fifth word: {fifth_word}\")\n",
    "        \n",
    "        # Print the length of the 5th word\n",
    "        # - len(fifth_word): Python string length\n",
    "        print(f\"Length of fifth word: {len(fifth_word)}\")\n",
    "    else:\n",
    "        # Handle case where 5th line has fewer than 5 words\n",
    "        print(\"Less than 5 words in line 5\")\n",
    "else:\n",
    "    # Handle case where file has fewer than 5 lines\n",
    "    print(\"Less than 5 lines in the file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc29786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "|address                            |age|email               |first_name|last_name|mobile    |\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "|{San Diego, USA, CA, 741 Beach Rd} |34 |amanda.a@email.com  |Amanda    |Anderson |9517538520|\n",
      "|{Los Angeles, USA, CA, 456 Oak Ave}|28 |jane.smith@email.com|Jane      |Smith    |9876543210|\n",
      "|{Chicago, USA, IL, 789 Pine Rd}    |35 |michael.j@email.com |Michael   |Johnson  |5551234567|\n",
      "|{Phoenix, USA, AZ, 654 Maple Dr}   |32 |robert.b@email.com  |Robert    |Brown    |7894561230|\n",
      "|{Houston, USA, TX, 321 Elm St}     |26 |sarah.w@email.com   |Sarah     |Williams |4567891230|\n",
      "|{Seattle, USA, WA, 147 Birch Rd}   |31 |david.m@email.com   |David     |Miller   |1597534560|\n",
      "|{Denver, USA, CO, 258 Spruce Ave}  |27 |lisa.w@email.com    |Lisa      |Wilson   |3579514260|\n",
      "|{Boston, USA, MA, 369 Palm St}     |33 |james.t@email.com   |James     |Taylor   |7531598520|\n",
      "|{New York, USA, NY, 123 Main St}   |30 |john.doe@email.com  |John      |Doe      |1234567890|\n",
      "|{Miami, USA, FL, 987 Cedar Ln}     |29 |emily.d@email.com   |Emily     |Davis    |3216549870|\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "\n",
      "+----------------------------------+---+-------------------+----------+---------+----------+\n",
      "|address                           |age|email              |first_name|last_name|mobile    |\n",
      "+----------------------------------+---+-------------------+----------+---------+----------+\n",
      "|{San Diego, USA, CA, 741 Beach Rd}|34 |amanda.a@email.com |Amanda    |Anderson |9517538520|\n",
      "|{Chicago, USA, IL, 789 Pine Rd}   |35 |michael.j@email.com|Michael   |Johnson  |5551234567|\n",
      "|{Phoenix, USA, AZ, 654 Maple Dr}  |32 |robert.b@email.com |Robert    |Brown    |7894561230|\n",
      "|{Seattle, USA, WA, 147 Birch Rd}  |31 |david.m@email.com  |David     |Miller   |1597534560|\n",
      "|{Boston, USA, MA, 369 Palm St}    |33 |james.t@email.com  |James     |Taylor   |7531598520|\n",
      "+----------------------------------+---+-------------------+----------+---------+----------+\n",
      "\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "|address                            |age|email               |first_name|last_name|mobile    |\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "|{San Diego, USA, CA, 741 Beach Rd} |34 |amanda.a@email.com  |Amanda    |Anderson |9517538520|\n",
      "|{Seattle, USA, WA, 147 Birch Rd}   |31 |david.m@email.com   |David     |Miller   |1597534560|\n",
      "|{Miami, USA, FL, 987 Cedar Ln}     |29 |emily.d@email.com   |Emily     |Davis    |3216549870|\n",
      "|{Boston, USA, MA, 369 Palm St}     |33 |james.t@email.com   |James     |Taylor   |7531598520|\n",
      "|{Los Angeles, USA, CA, 456 Oak Ave}|28 |jane.smith@email.com|Jane      |Smith    |9876543210|\n",
      "|{New York, USA, NY, 123 Main St}   |30 |john.doe@email.com  |John      |Doe      |1234567890|\n",
      "|{Denver, USA, CO, 258 Spruce Ave}  |27 |lisa.w@email.com    |Lisa      |Wilson   |3579514260|\n",
      "|{Chicago, USA, IL, 789 Pine Rd}    |35 |michael.j@email.com |Michael   |Johnson  |5551234567|\n",
      "|{Phoenix, USA, AZ, 654 Maple Dr}   |32 |robert.b@email.com  |Robert    |Brown    |7894561230|\n",
      "|{Houston, USA, TX, 321 Elm St}     |26 |sarah.w@email.com   |Sarah     |Williams |4567891230|\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+\n",
      "\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+---------------+\n",
      "|address                            |age|email               |first_name|last_name|mobile    |full_name      |\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+---------------+\n",
      "|{San Diego, USA, CA, 741 Beach Rd} |34 |amanda.a@email.com  |Amanda    |Anderson |9517538520|Amanda Anderson|\n",
      "|{Los Angeles, USA, CA, 456 Oak Ave}|28 |jane.smith@email.com|Jane      |Smith    |9876543210|Jane Smith     |\n",
      "|{Chicago, USA, IL, 789 Pine Rd}    |35 |michael.j@email.com |Michael   |Johnson  |5551234567|Michael Johnson|\n",
      "|{Phoenix, USA, AZ, 654 Maple Dr}   |32 |robert.b@email.com  |Robert    |Brown    |7894561230|Robert Brown   |\n",
      "|{Houston, USA, TX, 321 Elm St}     |26 |sarah.w@email.com   |Sarah     |Williams |4567891230|Sarah Williams |\n",
      "|{Seattle, USA, WA, 147 Birch Rd}   |31 |david.m@email.com   |David     |Miller   |1597534560|David Miller   |\n",
      "|{Denver, USA, CO, 258 Spruce Ave}  |27 |lisa.w@email.com    |Lisa      |Wilson   |3579514260|Lisa Wilson    |\n",
      "|{Boston, USA, MA, 369 Palm St}     |33 |james.t@email.com   |James     |Taylor   |7531598520|James Taylor   |\n",
      "|{New York, USA, NY, 123 Main St}   |30 |john.doe@email.com  |John      |Doe      |1234567890|John Doe       |\n",
      "|{Miami, USA, FL, 987 Cedar Ln}     |29 |emily.d@email.com   |Emily     |Davis    |3216549870|Emily Davis    |\n",
      "+-----------------------------------+---+--------------------+----------+---------+----------+---------------+\n",
      "\n",
      "+----------+--------------------+-----------+\n",
      "|first_name|email               |city       |\n",
      "+----------+--------------------+-----------+\n",
      "|Amanda    |amanda.a@email.com  |San Diego  |\n",
      "|Jane      |jane.smith@email.com|Los Angeles|\n",
      "|Michael   |michael.j@email.com |Chicago    |\n",
      "|Robert    |robert.b@email.com  |Phoenix    |\n",
      "|Sarah     |sarah.w@email.com   |Houston    |\n",
      "|David     |david.m@email.com   |Seattle    |\n",
      "|Lisa      |lisa.w@email.com    |Denver     |\n",
      "|James     |james.t@email.com   |Boston     |\n",
      "|John      |john.doe@email.com  |New York   |\n",
      "|Emily     |emily.d@email.com   |Miami      |\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q3. Create a pyspark program to create a json file containing Person information like first_name, last_name, age, mobile, email, address(contains street, city, state, country) and write it to a file. And handle all kinds of interview question on json file like read, write, update, delete, filter, sort, etc. write the file and read the file from the same location.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, to_json, struct, concat_ws\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"JSON Personal_data write and read \").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Define schema for Person information\n",
    "# schema = StructType([\n",
    "#     StructField(\"name\", StringType(), True, {\"description\": \"person's name\", \"example\": \"Alice\"})\n",
    "# ])  True is nullable, False is not nullable, {\"description\": \"person's name\", \"example\": \"Alice\"}} is metadata\n",
    "\n",
    "person_schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"mobile\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# | PySpark Data Type               | Python Equivalent          | Example               |\n",
    "# | ------------------------------- | -------------------------- | --------------------- |\n",
    "# | `StringType()`                  | `str`                      | `\"John\"`              |\n",
    "# | `IntegerType()`                 | `int`                      | `25`                  |\n",
    "# | `LongType()`                    | `int` (big integers)       | `9876543210`          |\n",
    "# | `FloatType()`                   | `float`                    | `3.14`                |\n",
    "# | `DoubleType()`                  | `float` (double-precision) | `2.71828`             |\n",
    "# | `BooleanType()`                 | `bool`                     | `True`, `False`       |\n",
    "# | `DateType()`                    | `datetime.date`            | `2023-01-01`          |\n",
    "# | `TimestampType()`               | `datetime.datetime`        | `2023-01-01 10:00:00` |\n",
    "# | `BinaryType()`                  | `bytes`                    | Binary data           |\n",
    "# | `DecimalType(precision, scale)` | `decimal.Decimal`          | `Decimal(\"123.45\")`   |\n",
    "\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (\"John\", \"Doe\", 30, \"1234567890\", \"john.doe@email.com\",\n",
    "     {\"street\": \"123 Main St\", \"city\": \"New York\", \"state\": \"NY\", \"country\": \"USA\"}),\n",
    "    (\"Jane\", \"Smith\", 28, \"9876543210\", \"jane.smith@email.com\",\n",
    "     {\"street\": \"456 Oak Ave\", \"city\": \"Los Angeles\", \"state\": \"CA\", \"country\": \"USA\"}),\n",
    "    (\"Michael\", \"Johnson\", 35, \"5551234567\", \"michael.j@email.com\",\n",
    "     {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\", \"state\": \"IL\", \"country\": \"USA\"}),\n",
    "    (\"Sarah\", \"Williams\", 26, \"4567891230\", \"sarah.w@email.com\",\n",
    "     {\"street\": \"321 Elm St\", \"city\": \"Houston\", \"state\": \"TX\", \"country\": \"USA\"}),\n",
    "    (\"Robert\", \"Brown\", 32, \"7894561230\", \"robert.b@email.com\",\n",
    "     {\"street\": \"654 Maple Dr\", \"city\": \"Phoenix\", \"state\": \"AZ\", \"country\": \"USA\"}),\n",
    "    (\"Emily\", \"Davis\", 29, \"3216549870\", \"emily.d@email.com\",\n",
    "     {\"street\": \"987 Cedar Ln\", \"city\": \"Miami\", \"state\": \"FL\", \"country\": \"USA\"}),\n",
    "    (\"David\", \"Miller\", 31, \"1597534560\", \"david.m@email.com\",\n",
    "     {\"street\": \"147 Birch Rd\", \"city\": \"Seattle\", \"state\": \"WA\", \"country\": \"USA\"}),\n",
    "    (\"Lisa\", \"Wilson\", 27, \"3579514260\", \"lisa.w@email.com\",\n",
    "     {\"street\": \"258 Spruce Ave\", \"city\": \"Denver\", \"state\": \"CO\", \"country\": \"USA\"}),\n",
    "    (\"James\", \"Taylor\", 33, \"7531598520\", \"james.t@email.com\",\n",
    "     {\"street\": \"369 Palm St\", \"city\": \"Boston\", \"state\": \"MA\", \"country\": \"USA\"}),\n",
    "    (\"Amanda\", \"Anderson\", 34, \"9517538520\", \"amanda.a@email.com\",\n",
    "     {\"street\": \"741 Beach Rd\", \"city\": \"San Diego\", \"state\": \"CA\", \"country\": \"USA\"})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "person_df = spark.createDataFrame(data, schema=person_schema)\n",
    "\n",
    "# Write to JSON file\n",
    "person_df.write.mode(\"overwrite\").json(\"D:/Data_Engineer/DE_Interview/person_data.json\")\n",
    "\n",
    "# Read JSON file\n",
    "df_read = spark.read.json(\"D:/Data_Engineer/DE_Interview/person_data.json\")\n",
    "\n",
    "# Example operations:\n",
    "# 1. Filter by age\n",
    "filtered_df = df_read.filter(col(\"age\") > 30)\n",
    "\n",
    "# 2. Sort by name\n",
    "sorted_df = df_read.orderBy(\"first_name\")\n",
    "\n",
    "# 3. Update - Add full name column\n",
    "updated_df = df_read.withColumn(\"full_name\", \n",
    "    concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# 4. Select specific fields\n",
    "selected_df = df_read.select(\"first_name\", \"email\", \"address.city\")\n",
    "\n",
    "# Display results\n",
    "df_read.show(truncate=False)\n",
    "filtered_df.show(truncate=False)\n",
    "sorted_df.show(truncate=False)\n",
    "updated_df.show(truncate=False)\n",
    "selected_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e1fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|dept_name|highest_salary|\n",
      "+---------+--------------+\n",
      "|       HR|        800000|\n",
      "|  Finance|        550000|\n",
      "|       IT|        600000|\n",
      "+---------+--------------+\n",
      "\n",
      "+---------+------------+--------------+\n",
      "|dept_name|total_salary|employee_count|\n",
      "+---------+------------+--------------+\n",
      "|       HR|     2200000|             3|\n",
      "|  Finance|     1040000|             2|\n",
      "|       IT|     2640000|             5|\n",
      "+---------+------------+--------------+\n",
      "\n",
      "+-------------+----------------+-----+------+\n",
      "|    Full_Name|       job_title|  YOE|salary|\n",
      "+-------------+----------------+-----+------+\n",
      "| Priya Sharma|         Manager|10.29|800000|\n",
      "|  Mike Wilson|  Senior Manager| 8.92|750000|\n",
      "|  Jessica Lee|Senior Developer| 7.55|650000|\n",
      "| Suresh Kumar| Senior Engineer| 6.98|600000|\n",
      "|     John Doe|       Developer|  6.2|550000|\n",
      "|Lisa Anderson|       Developer| 6.04|540000|\n",
      "| Ramesh Singh|   Data_Engineer| 5.41|500000|\n",
      "+-------------+----------------+-----+------+\n",
      "\n",
      "+---------+------------------+\n",
      "|dept_name|           avg_age|\n",
      "+---------+------------------+\n",
      "|       HR|             34.01|\n",
      "|  Finance|             34.37|\n",
      "|       IT|31.863999999999997|\n",
      "+---------+------------------+\n",
      "\n",
      "+---------+------+-------------+------+-----------+\n",
      "|dept_name|emp_id|    Full_Name|salary|salary_band|\n",
      "+---------+------+-------------+------+-----------+\n",
      "|       IT|     9|Lisa Anderson|540000|        Mid|\n",
      "|       IT|     7|   Emma Brown|520000|        Mid|\n",
      "|       IT|     5| Sarah Miller|480000|      Entry|\n",
      "|       IT|     2| Suresh Kumar|600000|        Mid|\n",
      "|       IT|     1| Ramesh Singh|500000|      Entry|\n",
      "|       HR|    13|  Jessica Lee|650000|        Mid|\n",
      "|       HR|     6|  Mike Wilson|750000|     Senior|\n",
      "|       HR|     3| Priya Sharma|800000|     Senior|\n",
      "|  Finance|    11|   Amy Wright|490000|      Entry|\n",
      "|  Finance|     4|     John Doe|550000|        Mid|\n",
      "+---------+------+-------------+------+-----------+\n",
      "\n",
      "+------------+-----+\n",
      "|manager_name|count|\n",
      "+------------+-----+\n",
      "|        NULL|    2|\n",
      "|      Suresh|    4|\n",
      "|       Priya|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4. Create a pyspark program to create a json file containing employee information like emp_id, first_name, last_name, DOB, email, job_title, hire_date, salary, mgr_id, deptid and write it to a file. And handle all kinds of interview question on json file like read, write, update, delete, filter, sort, etc. write the file and read the file from the same location.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import col, count,max, avg, sum, when, datediff,round, current_date, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "spark = SparkSession.builder.appName(\"JSON emp dept data write and read\").getOrCreate()\n",
    "\n",
    "# Create sample data as per the input\n",
    "#data = [\n",
    "#    Row(store_id=1, customer_id=101, purchase_amt=500)] # This way we doing when data is less and here we need to mention column names and its value\n",
    "#sales_df = spark.createDataFrame(data) #directly creating dataframe from row object\n",
    "\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"DOB\", DateType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"job_title\", StringType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"mgr_id\", IntegerType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create dept_schema for Department information\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), False),\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "emp_data = [\n",
    "    (1, \"Ramesh\", \"Singh\", datetime(1995, 1, 5), \"rameshsingh@gmail.com\", \"Data_Engineer\", datetime(2020, 1, 5), 500000, 2, 1),\n",
    "    (2, \"Suresh\", \"Kumar\", datetime(1992, 3, 15), \"suresh@gmail.com\", \"Senior Engineer\", datetime(2018, 6, 10), 600000, 3, 1),\n",
    "    (3, \"Priya\", \"Sharma\", datetime(1990, 7, 20), \"priya@gmail.com\", \"Manager\", datetime(2015, 2, 15), 800000, None, 2),\n",
    "    (4, \"John\", \"Doe\", datetime(1988, 12, 10), \"john@gmail.com\", \"Developer\", datetime(2019, 3, 20), 550000, 2, 3),\n",
    "    (5, \"Sarah\", \"Miller\", datetime(1993, 5, 25), None, \"Data Analyst\", datetime(2021, 1, 15), 480000, 3, 1),\n",
    "    (6, \"Mike\", \"Wilson\", datetime(1991, 9, 30), \"mike@gmail.com\", \"Senior Manager\", datetime(2016, 7, 1), 750000, None, 2),\n",
    "    (7, \"Emma\", \"Brown\", datetime(1994, 4, 12), \"emma@gmail.com\", \"Engineer\", datetime(2020, 11, 30), 520000, 2, 1),\n",
    "    (1, \"Ramesh\", \"Singh\", datetime(1995, 1, 5), \"rameshsingh@gmail.com\", \"Data_Engineer\", datetime(2020, 1, 5), 500000, 2, 1),  # Duplicate\n",
    "    (8, None, \"Taylor\", datetime(1989, 8, 18), \"taylor@gmail.com\", \"Architect\", datetime(2017, 9, 15), 680000, 3, 3),\n",
    "    (9, \"Lisa\", \"Anderson\", datetime(1992, 11, 22), \"lisa@gmail.com\", \"Developer\", datetime(2019, 5, 20), 540000, 2, 1),\n",
    "    (10, \"David\", \"Clark\", None, \"david@gmail.com\", \"Engineer\", datetime(2021, 3, 10), 510000, 2, 2),\n",
    "    (11, \"Amy\", \"Wright\", datetime(1993, 2, 28), \"amy@gmail.com\", \"Data Analyst\", datetime(2020, 8, 1), 490000, 3, 3),\n",
    "    (12, \"Tom\", \"Harris\", datetime(1990, 6, 15), \"tom@gmail.com\", None, datetime(2018, 4, 25), 580000, 2, 1),\n",
    "    (13, \"Jessica\", \"Lee\", datetime(1991, 12, 5), \"jessica@gmail.com\", \"Senior Developer\", datetime(2017, 11, 12), 650000, 3, 2),\n",
    "    (6, \"Mike\", \"Wilson\", datetime(1991, 9, 30), \"mike@gmail.com\", \"Senior Manager\", datetime(2016, 7, 1), 750000, None, 2)  # Duplicate\n",
    "]\n",
    "\n",
    "dept_data = [\n",
    "    (1, \"IT\", \"Bangalore\"),\n",
    "    (2, \"HR\", \"Mumbai\"),\n",
    "    (3, \"Finance\", \"Delhi\"),\n",
    "    (4, \"Marketing\", None),\n",
    "    (5, \"Operations\", \"Chennai\")\n",
    "]\n",
    "\n",
    "# Create DataFrames and clean data \n",
    "emp_df = spark.createDataFrame(emp_data, schema=emp_schema)\n",
    "dept_df = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "\n",
    "emp_df.write.mode(\"overwrite\").json(\"D:/Data_Engineer/DE_Interview/emp_data.json\")\n",
    "dept_df.write.mode(\"overwrite\").json(\"D:/Data_Engineer/DE_Interview/dept_data.json\")\n",
    "\n",
    "# Remove duplicates from emp_df based on emp_id and remove null values\n",
    "clean_df = emp_df.dropDuplicates([\"emp_id\"]).filter(\n",
    "    col(\"first_name\").isNotNull() & \n",
    "    col(\"last_name\").isNotNull() & \n",
    "    col(\"DOB\").isNotNull() & \n",
    "    col(\"job_title\").isNotNull() & \n",
    "    col(\"hire_date\").isNotNull() & \n",
    "    col(\"salary\").isNotNull() & \n",
    "    col(\"dept_id\").isNotNull()\n",
    ")\n",
    "\n",
    "#Derived Columns\n",
    "derived_df = clean_df.withColumn(\"Full_Name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\\\n",
    "    .withColumn(\"age\", round((datediff(current_date(), col(\"DOB\"))/365.25),2))\\\n",
    "    .withColumn(\"YOE\", round((datediff(current_date(), col(\"hire_date\"))/365.25),2))\n",
    "# Analysis Questions:\n",
    "# 1. Find highest salary department wise and dept_name and emp_name\n",
    "# First get max salary per department\n",
    "max_salaries = derived_df.join(\n",
    "    dept_df,\n",
    "    derived_df.dept_id == dept_df.dept_id\n",
    ").groupBy(\"dept_name\").agg(\n",
    "    max(\"salary\").alias(\"highest_salary\")\n",
    ")\n",
    "\n",
    "# 2. Find the total salary department wise dept_name, total_salary and count number of employee department wise\n",
    "dept_salary = derived_df.join(\n",
    "    dept_df,\n",
    "    derived_df.dept_id == dept_df.dept_id\n",
    ").groupBy(\"dept_name\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    count(\"emp_id\").alias(\"employee_count\")\n",
    ")\n",
    "\n",
    "# 3. List out the employee having experience more than 5 years\n",
    "experienced_emp = derived_df.filter(col(\"YOE\") > 5).select(\n",
    "    \"Full_Name\",\n",
    "    \"job_title\",\n",
    "    \"YOE\",\n",
    "    \"salary\"\n",
    ").orderBy(\"YOE\", ascending=False)\n",
    "\n",
    "# 4. Department-wise average age of employees\n",
    "dept_age = derived_df.join(\n",
    "    dept_df,\n",
    "    derived_df.dept_id == dept_df.dept_id\n",
    ").groupBy(\"dept_name\").agg(\n",
    "    avg(\"age\").alias(\"avg_age\")\n",
    ")\n",
    "\n",
    "# 5. Department-wise salary distribution\n",
    "salary_distribution = derived_df.join(\n",
    "    dept_df,\n",
    "    derived_df.dept_id == dept_df.dept_id\n",
    ").select(\n",
    "    \"dept_name\",\"emp_id\", \"Full_Name\", \"salary\",\n",
    "    when(col(\"salary\") <= 500000, \"Entry\")\n",
    "    .when(col(\"salary\") <= 650000, \"Mid\")\n",
    "    .otherwise(\"Senior\").alias(\"salary_band\")\n",
    ")\n",
    "\n",
    "# 6. Employees reporting to each manager\n",
    "manager_subordinates = derived_df.alias(\"emp\").join(\n",
    "    derived_df.select(\"emp_id\", \"first_name\").withColumnRenamed(\"first_name\", \"manager_name\").alias(\"mgr\"),\n",
    "    col(\"emp.mgr_id\") == col(\"mgr.emp_id\"),\n",
    "    \"left\"\n",
    ").groupBy(\"manager_name\").count()\n",
    "\n",
    "\n",
    "# Display additional results\n",
    "max_salaries.show()\n",
    "dept_salary.show()\n",
    "experienced_emp.show()\n",
    "dept_age.show()\n",
    "salary_distribution.show()\n",
    "manager_subordinates.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
