{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e12a150-d545-4509-9cb9-47081d6d4dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------+-----------+-------+\n|empno|name   |sal |Deptno|nexthighest|saldiff|\n+-----+-------+----+------+-----------+-------+\n|1    |Radha  |3000|10    |2000       |1000   |\n|2    |Kirshna|2000|10    |1000       |1000   |\n|3    |rama   |1000|10    |0          |1000   |\n|1    |Venkata|6000|20    |4000       |2000   |\n|2    |Laxmana|4000|20    |2000       |2000   |\n|3    |Laxmi  |2000|20    |0          |2000   |\n+-----+-------+----+------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "These are input output please write databricks pyspark script\n",
    "empno   \t\tname \t\tsal \tDeptno\t\n",
    "1     \t\t\tRadha \t\t3000\t 10 \t \n",
    "2      \t\t\tKirshna\t\t2000\t 10\t \n",
    "3\t\t\t    rama\t\t1000     10\t            \n",
    "1     \t\t\tVenkata \t6000\t 20 \t \t\t\n",
    "2      \t\t\tLaxmana\t\t4000\t 20\t           \n",
    "3\t\t\tLaxmi\t\t2000     20\n",
    "Output Dataset : \n",
    "\n",
    "\n",
    "empno   \t\tname \t\tsal \tDeptno\tnexthighest  saldiff\n",
    "1     \t\t\tRadha \t\t3000\t 10 \t 2000\t\t1000\n",
    "2      \t\t\tKirshna\t\t2000\t 10\t 1000           1000\n",
    "3\t\t\trama\t\t1000     10\t 0              1000\n",
    "1     \t\t\tVenkata \t6000\t 20 \t 4000\t\t2000\n",
    "2      \t\t\tLaxmana\t\t4000\t 20\t 2000           2000\n",
    "3\t\t\tLaxmi\t\t2000     20\t 0              2000\n",
    "\n",
    "'''\n",
    "from pyspark.sql.functions import col, lead, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"Radha\", 3000, 10),\n",
    "    (2, \"Kirshna\", 2000, 10),\n",
    "    (3, \"rama\", 1000, 10),\n",
    "    (1, \"Venkata\", 6000, 20),\n",
    "    (2, \"Laxmana\", 4000, 20),\n",
    "    (3, \"Laxmi\", 2000, 20)\n",
    "]\n",
    "\n",
    "columns = [\"empno\", \"name\", \"sal\", \"Deptno\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "window_spec=Window.partitionBy(\"Deptno\").orderBy(col(\"sal\").desc())\n",
    "#Calculate next highest salary using `lead`\n",
    "df_with_next_highest=df.withColumn(\"nexthighest\",lead(\"sal\").over(window_spec))\n",
    "#Fill null values with 0 (for cases where no next highest salary exists)\n",
    "df_with_next_highest=df_with_next_highest.fillna({\"nexthighest\": 0})\n",
    "df_final=df_with_next_highest.withColumn(\"saldiff\",col(\"sal\")-col(\"nexthighest\"))\n",
    "df_final.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2580eafb-be95-47e8-95f3-ded0e0ccdce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. How to Improve Data Load Performance in S3 or Delta Lake in Databricks\n",
    "To optimize the loading of data into S3 or Delta Lake, consider the following strategies:\n",
    "\n",
    "a) Partitioning and Bucketing\n",
    "Partitioning: Split large tables by a key (e.g., date or region) to reduce the data scanned.\n",
    "Bucketing: Hash-based grouping can improve joins and aggregations.\n",
    "b) Optimize File Sizes\n",
    "Ensure that file sizes are around 128 MB to 1 GB. Too many small files lead to inefficiencies (small files issue).\n",
    "c) Auto Optimize\n",
    "Enable Auto Optimize in Databricks to automatically compact small files when writing to Delta Lake.\n",
    "\n",
    "SET spark.databricks.delta.autoOptimize.optimizeWrite = true;\n",
    "d) Data Skipping\n",
    "Delta Lake supports data skipping with statistics, so keeping the metadata up-to-date helps improve queries.\n",
    "e) Z-Ordering\n",
    "Use Z-Ordering to co-locate related data. This helps queries that filter on multiple columns.\n",
    "\n",
    "OPTIMIZE delta_table_name ZORDER BY (column1, column2);\n",
    "f) Caching\n",
    "Cache frequently accessed data using Spark’s caching mechanisms.\n",
    "2. How to Improve Performance with Many Small Tables in Databricks\n",
    "Small tables can degrade performance if not handled properly. Here's how to manage:\n",
    "\n",
    "a) Combining Small Tables\n",
    "Combine small tables into larger tables or views to reduce metadata overhead.\n",
    "b) Broadcast Joins\n",
    "Use broadcast joins for small tables (under 10 MB) to avoid shuffling.\n",
    "\n",
    "spark.sql.autoBroadcastJoinThreshold = 10485760  -- 10 MB threshold\n",
    "c) Delta Lake Compaction\n",
    "Compact small files using Delta’s OPTIMIZE command:\n",
    "\n",
    "OPTIMIZE delta_table;\n",
    "d) Databricks Auto Loader\n",
    "Use Auto Loader for incremental loads if small files arrive frequently. This minimizes repeated processing of the same data.\n",
    "e) Caching and Partitioning\n",
    "Cache frequently accessed small tables and partition them appropriately.\n",
    "3. Monitoring and Optimizing Databricks Job Performance\n",
    "To monitor and optimize job performance, Databricks offers several tools:\n",
    "\n",
    "a) Job Monitoring Dashboard\n",
    "Use the Databricks Job Dashboard to view job run status, logs, and errors.\n",
    "Navigate to Jobs > Click a specific job > Check Runs to monitor execution details.\n",
    "b) Ganglia or Spark UI\n",
    "Spark UI provides detailed insights into job stages, task execution, and shuffle operations.\n",
    "c) Databricks Metrics\n",
    "Enable Cluster Metrics to monitor CPU, memory, and disk usage for each cluster node.\n",
    "You can access this under the \"Metrics\" tab for a running job.\n",
    "d) Delta Lake Transaction Logs\n",
    "Delta Lake maintains logs that allow you to track writes and updates. Analyze these logs for performance bottlenecks.\n",
    "e) Optimizing Jobs\n",
    "Adaptive Query Execution (AQE): Enable AQE to optimize joins and shuffles at runtime.\n",
    "\n",
    "spark.sql.adaptive.enabled = true;\n",
    "Caching: Cache tables or intermediate results to reduce recomputation.\n",
    "f) Data Compaction and Z-Ordering\n",
    "Regularly compact small files and apply Z-Ordering as explained above.\n",
    "g) Auto Termination\n",
    "Configure Auto Termination for clusters to save costs on idle clusters.\n",
    "h) Alerting\n",
    "Set up job failure alerts using Databricks alerts to get notifications via email or Slack when a job fails.\n",
    "These practices will help ensure efficient job monitoring and data load performance in Databricks.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CitiusTech_DE_Interview",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
