# PySpark solutions

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_replace, udf
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder.appName("StringCleaning").getOrCreate()

# Sample data
data = [("Hello123! This is a #sample string with 456 special @characters.",)]
df = spark.createDataFrame(data, ["text"])

# Method 1: Using regex with Spark SQL functions
def clean_text_with_spark_functions(dataframe, column_name):
    """
    Removes special characters and numbers using Spark's built-in functions
    """
    return dataframe.withColumn(
        "cleaned_text", 
        regexp_replace(col(column_name), "[^a-zA-Z\\s]", "")
    )

# Method 2: Using UDF without regex
def remove_special_chars_numbers(text):
    """
    Helper function to remove special characters and numbers without regex
    """
    if text is None:
        return None
    
    result = ""
    for char in text:
        if char.isalpha() or char.isspace():
            result += char
    return result

# Register the UDF
clean_text_udf = udf(remove_special_chars_numbers, StringType())

def clean_text_with_udf(dataframe, column_name):
    """
    Removes special characters and numbers using a UDF
    """
    return dataframe.withColumn("cleaned_text", clean_text_udf(col(column_name)))

# Example usage
if __name__ == "__main__":
    # Using Spark functions (with regex)
    cleaned_df1 = clean_text_with_spark_functions(df, "text")
    print("Cleaned with Spark functions (regex):")
    cleaned_df1.show(truncate=False)
    
    # Using UDF (without regex)
    cleaned_df2 = clean_text_with_udf(df, "text")
    print("Cleaned with UDF (no regex):")
    cleaned_df2.show(truncate=False)
