{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx6lriUHQlja"
      },
      "outputs": [],
      "source": [
        "#Coforge DE Interview skills(Spark and SQL) 11 jan 24\n",
        "\n",
        "#1. What is spark architecture?\n",
        "\"\"\"Apache Spark Architecture:\n",
        "1. Driver: The driver is the main control process that creates the SparkContext, connects to the cluster, and coordinates the execution of tasks. It translates the user program into tasks and schedules them to run on executors.\n",
        "2. Executors: Executors are worker nodes that run individual tasks in a distributed manner. Each executor has its own JVM and runs tasks assigned by the driver. Executors also provide in-memory storage for RDDs that are cached by user programs through SparkContext.\n",
        "3. Cluster Manager: The cluster manager is responsible for managing the resources across the cluster. Spark supports various cluster managers like Standalone, Apache Mesos, kubernates and Hadoop YARN. The cluster manager allocates resources to the driver and executors.\n",
        "4. RDD (Resilient Distributed Dataset): RDD is the fundamental data structure of Spark. It is an immutable distributed collection of objects that can be processed in parallel. RDDs support two types of operations: transformations (e.g., map, filter) and actions (e.g., count, collect).\n",
        "5. DAG (Directed Acyclic Graph): Spark creates a logical execution plan in the form of a DAG. The DAG represents the sequence of transformations applied to the data. The DAG scheduler divides the operators into stages of tasks and submits them to the task scheduler.\n",
        "6. Task Scheduler: The task scheduler is responsible for scheduling tasks on the executors. It takes the stages from the DAG scheduler and assigns tasks to executors based on data locality and resource availability.\n",
        "In summary, Spark architecture consists of a driver, executors, cluster manager, RDDs, DAG, and task scheduler, working together to process large-scale data in a distributed and fault-tolerant manner.\n",
        "\"\"\"\n",
        "\n",
        "#How does Spark distribute tasks for processing a 1 GB file, and how are tasks and executors allocated during execution?\n",
        "'''Spark divides the 1 GB file into partitions (default is 128 MB per partition for HDFS), creating about 8 partitions for a 1 GB file. Each partition becomes a task.\n",
        "\n",
        "Executors: Each executor processes multiple tasks based on available cores.\n",
        "Tasks per Executor: If an executor has 4 cores, it can run 4 tasks in parallel.'''\n",
        "\n",
        "# What is spark-submit explain in details?\n",
        "'''\n",
        "spark-submit is a command-line tool used to launch Spark applications on a cluster or locally. It handles resource allocation, configuration, and submission of jobs to Spark clusters like YARN, Kubernetes, Mesos, or standalone mode.\n",
        "\n",
        "Key Responsibilities:\n",
        "Distribute Application Code: Sends your Spark application JAR or Python script to the cluster.\n",
        "Resource Management: Allocates memory and CPU resources for the driver and executors.\n",
        "Configuration Management: Sets properties like application name, Spark master, deploy mode, and more.\n",
        "Job Execution: Manages the execution of tasks and monitors job progress.\n",
        "Command Syntax:\n",
        "\n",
        "spark-submit \\\n",
        "  --master <master-url> \\\n",
        "  --deploy-mode <deploy-mode> \\\n",
        "  --class <main-class> \\\n",
        "  --executor-memory <memory> \\\n",
        "  --executor-cores <cores> \\\n",
        "  application.jar <application-arguments>\n",
        "Common Parameters:\n",
        "Parameter\tDescription\n",
        "--master\tSpecifies the cluster manager (local, yarn, etc.)\n",
        "--deploy-mode\tDeployment mode (client or cluster)\n",
        "--class\tMain class of the application (for Java/Scala)\n",
        "--executor-memory\tMemory per executor (e.g., 2G)\n",
        "--executor-cores\tNumber of cores per executor\n",
        "--num-executors\tNumber of executors to launch\n",
        "--conf\tSet configuration properties (e.g., spark.executor.memory)\n",
        "Example Usage:\n",
        "Running a Spark Application Locally:\n",
        "\n",
        "spark-submit\n",
        "  --master local[2]\n",
        "  my_spark_app.py\n",
        "\n",
        "Running on a YARN Cluster:\n",
        "\n",
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --executor-memory 4G \\\n",
        "  --executor-cores 4 \\\n",
        "  my_spark_app.py\n",
        "Execution Modes:\n",
        "Client Mode: Driver runs on the machine where spark-submit is executed. Suitable for local testing.\n",
        "Cluster Mode: Driver runs on a worker node within the cluster. Ideal for large-scale production jobs.\n",
        "\n",
        "Important Points:\n",
        "Driver Program: Manages job execution and task scheduling.\n",
        "Executors: Run the actual tasks on worker nodes and store intermediate data.\n",
        "Partitions: Spark splits input data into smaller partitions for distributed processing.\n",
        "By leveraging spark-submit, Spark achieves scalability and parallel processing across large datasets efficiently.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjAsnjbVQljc",
        "outputId": "40bdd486-3096-431d-bed5-611b18872a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7839, 'KING', 'PRESIDENT', None, datetime.datetime(1981, 11, 17, 0, 0), 5100.0, None, 20)\n"
          ]
        }
      ],
      "source": [
        "# Synechron Interview Questions on Database for Data Engineer\n",
        "\n",
        "# Make Oracle DB connection in Python and write a query?\n",
        "\n",
        "#%pip install cx_Oracle\n",
        "import cx_Oracle\n",
        "\n",
        "dsn_tns=cx_Oracle.makedsn('localhost','1521',service_name='orcl')\n",
        "conn=cx_Oracle.connect(user='SivaAcademy',password='pass',dsn=dsn_tns) #while connecting only give req pass\n",
        "c=conn.cursor()\n",
        "c.execute('select * from emp')\n",
        "print(c.fetchone()) # we can use fetchall() for all or  for row in c:\n",
        "    #print(row)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTIaz-ieQljd"
      },
      "outputs": [],
      "source": [
        "# 1. Find the sales amount each year, product-wise  [Sales_data table]\n",
        "# Solution 1\n",
        "query='''WITH cte AS\n",
        "          (\n",
        "    SELECT product_id,\n",
        "           EXTRACT(YEAR FROM sales_date) AS sale_year,\n",
        "           SUM(amount) OVER (PARTITION BY product_id, EXTRACT(YEAR FROM sales_date)) AS cumulative_sum,\n",
        "           DENSE_RANK() OVER (PARTITION BY product_id ORDER BY EXTRACT(YEAR FROM sales_date)) AS rn\n",
        "    FROM Sales_data\n",
        ")\n",
        "SELECT product_id, sale_year, cumulative_sum\n",
        "FROM cte\n",
        "GROUP BY product_id, sale_year, cumulative_sum\n",
        "ORDER BY product_id, sale_year'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryy3iHQbQljd",
        "outputId": "d6fb9c74-d977-49bb-90a3-4ef2a9c85f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('P1', 2023, 1301.15)\n",
            "('P10', 2023, 1085.16)\n",
            "('P2', 2023, 1310.25)\n",
            "('P3', 2023, 1235.66)\n",
            "('P4', 2023, 1064.48)\n",
            "('P5', 2023, 1259.51)\n",
            "('P6', 2023, 1310.55)\n",
            "('P7', 2023, 1154.54)\n",
            "('P8', 2023, 1220.01)\n",
            "('P9', 2023, 1383.75)\n"
          ]
        }
      ],
      "source": [
        "# solution 2\n",
        "query='''SELECT product_id AS prod_id,\n",
        "    EXTRACT(YEAR FROM sales_date) AS sale_year,\n",
        "    SUM(amount) AS total_sales\n",
        "FROM Sales_data\n",
        "GROUP BY product_id, EXTRACT(YEAR FROM sales_date)\n",
        "ORDER BY product_id, sale_year'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n",
        "'''\n",
        "EXTRACT(YEAR FROM sale_dt): 2025\n",
        "TO_CHAR(sale_dt, 'YYYY'): '2025' (as a string)\n",
        "TRUNC(sale_dt, 'YYYY'): 01-JAN-2025 (the first day of the year)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmEpVwo8Qljd",
        "outputId": "73ce8b95-1249-453a-d146-7d94cd37e830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('P6',)\n",
            "('P8',)\n",
            "('P5',)\n",
            "('P1',)\n",
            "('P2',)\n",
            "('P3',)\n",
            "('P9',)\n",
            "('P10',)\n",
            "('P4',)\n",
            "('P7',)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nAnalysis:\\nSuppose I have written this query\\nSELECT DISTINCT product_id \\nFROM Sales_data \\nWHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\\n\\nthis will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\\n\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2. Find products that were not sold in the last 7 months.\n",
        "query='''SELECT product_id\n",
        "FROM Sales_data\n",
        "WHERE product_id NOT IN (\n",
        "    SELECT DISTINCT product_id\n",
        "    FROM Sales_data\n",
        "    WHERE sales_date >= ADD_MONTHS(SYSDATE, -7)) group by product_id'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n",
        "\n",
        "'''\n",
        "Analysis:\n",
        "Suppose I have written this query\n",
        "SELECT DISTINCT product_id\n",
        "FROM Sales_data\n",
        "WHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\n",
        "\n",
        "this will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crsPWArwQlje"
      },
      "outputs": [],
      "source": [
        "#Berribot Interview Questions on ETL and pyspark for Wipro\n",
        "# Schedule AWS databrics spark job daily in midnight script\n",
        "'''\n",
        "To schedule an AWS Databricks Spark job to run daily at midnight, you can use Databricks Workflows (preferred) or an external scheduling tool like AWS Lambda or CloudWatch. Below is an example script for each approach:\n",
        "\n",
        "1. Using Databricks Workflows (Recommended)\n",
        "Create a Databricks Job:\n",
        "\n",
        "Go to Databricks Workspace.\n",
        "Navigate to Workflows > Jobs.\n",
        "Click Create Job.\n",
        "Add your notebook or Spark job details.\n",
        "Configure job clusters and libraries if required.\n",
        "Schedule the Job:\n",
        "\n",
        "In the Schedule tab, set the schedule to:\n",
        "Daily\n",
        "Start time: 00:00\n",
        "Save the workflow.\n",
        "2. Using AWS CloudWatch with Databricks REST API\n",
        "Prerequisites:\n",
        "A Databricks personal access token.\n",
        "Databricks job ID or notebook path.\n",
        "Python Script:\n",
        "\n",
        "import requests\n",
        "\n",
        "# Databricks Workspace URL\n",
        "DATABRICKS_HOST = \"https://<databricks-instance>.cloud.databricks.com\"\n",
        "\n",
        "# Databricks Personal Access Token\n",
        "DATABRICKS_TOKEN = \"<your-databricks-token>\"\n",
        "\n",
        "# Job ID\n",
        "JOB_ID = \"<your-job-id>\"\n",
        "\n",
        "# Trigger Job Function\n",
        "def trigger_job():\n",
        "    url = f\"{DATABRICKS_HOST}/api/2.1/jobs/run-now\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"job_id\": JOB_ID}\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Job triggered successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to trigger job: {response.content}\")\n",
        "\n",
        "# Run the function\n",
        "trigger_job()\n",
        "Schedule via AWS CloudWatch:\n",
        "Open AWS Management Console.\n",
        "Go to CloudWatch > Rules.\n",
        "Create a new rule:\n",
        "Event Source: Schedule.\n",
        "Schedule Expression: cron(0 0 * * ? *) (midnight daily).\n",
        "Target:\n",
        "Choose AWS Lambda.\n",
        "Add the Python script to your Lambda function.\n",
        "3. Using Crontab on EC2\n",
        "If you prefer running the script from an EC2 instance:\n",
        "\n",
        "SSH into the EC2 instance.\n",
        "Edit the crontab:\n",
        "\n",
        "crontab -e\n",
        "Add the following entry to trigger the script at midnight:\n",
        "\n",
        "0 0 * * * python3 /path/to/trigger_databricks_job.py\n",
        "Which approach do you prefer? I can guide you further.'''\n",
        "\n",
        "\n",
        "#Write AWS databrics notebook to calculate rolling average over time periods for q time series data set.\n",
        "\n",
        "#Input : time period '7 days' dataset: 'stock_prices''\n",
        "\n",
        "#Output: dataframe with 7-day rolling averages\n",
        "\n",
        "'''Here is a Databricks notebook code that calculates the 7-day rolling average for a time series dataset (stock_prices) using PySpark.\n",
        "\n",
        "Code for the Databricks Notebook\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, window\n",
        "\n",
        "# Initialize SparkSession (this is usually already set up in Databricks)\n",
        "spark = SparkSession.builder.appName(\"RollingAverage\").getOrCreate()\n",
        "\n",
        "# Sample dataset: Replace this with your actual dataset\n",
        "data = [\n",
        "    (\"2025-01-01\", 100),\n",
        "    (\"2025-01-02\", 110),\n",
        "    (\"2025-01-03\", 105),\n",
        "    (\"2025-01-04\", 120),\n",
        "    (\"2025-01-05\", 130),\n",
        "    (\"2025-01-06\", 125),\n",
        "    (\"2025-01-07\", 140),\n",
        "    (\"2025-01-08\", 145),\n",
        "    (\"2025-01-09\", 150),\n",
        "    (\"2025-01-10\", 155)\n",
        "]\n",
        "\n",
        "# Define schema and create a DataFrame\n",
        "columns = [\"date\", \"price\"]\n",
        "stock_prices_df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Convert 'date' column to TimestampType\n",
        "from pyspark.sql.functions import to_date\n",
        "stock_prices_df = stock_prices_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Calculate 7-day rolling average\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Define window specification\n",
        "rolling_window = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "result_df = stock_prices_df.withColumn(\"7_day_avg\", avg(\"price\").over(rolling_window))\n",
        "\n",
        "# Show the results\n",
        "result_df.show()\n",
        "Explanation of the Code\n",
        "Dataset Initialization:\n",
        "\n",
        "Replace the data variable with your actual dataset from stock_prices.\n",
        "Date Conversion:\n",
        "\n",
        "Convert the date column to DateType for proper time series handling.\n",
        "Define Window Specification:\n",
        "\n",
        "Use Window.orderBy(\"date\").rowsBetween(-6, 0) to create a rolling window of 7 days (current row and 6 previous rows).\n",
        "Calculate Rolling Average:\n",
        "\n",
        "Use the avg function to compute the 7-day rolling average.\n",
        "Result:\n",
        "\n",
        "A new column 7_day_avg is added to the DataFrame showing the rolling average.\n",
        "Sample Output\n",
        "date\tprice\t7_day_avg\n",
        "2025-01-01\t100\t100.0\n",
        "2025-01-02\t110\t105.0\n",
        "2025-01-03\t105\t105.0\n",
        "2025-01-04\t120\t108.75\n",
        "2025-01-05\t130\t113.0\n",
        "2025-01-06\t125\t115.0\n",
        "2025-01-07\t140\t118.57\n",
        "2025-01-08\t145\t125.0\n",
        "2025-01-09\t150\t130.0\n",
        "2025-01-10\t155\t135.71\n",
        "Let me know if you'd like help loading the actual dataset from S3 or a database!'''\n",
        "\n",
        "\n",
        "#Partition a large dataframe in databrics by a given column and write to different parquet files.\n",
        "\n",
        "#Input: column: 'region'\n",
        "#Output: Partitioned parquet files by 'region'\n",
        "\n",
        "#Eg 2 Input: column: 'category'\n",
        "#Output: Partioned Parquet files by 'category'\n",
        "\n",
        "'''\n",
        "Here’s how to partition a large DataFrame in Databricks by a given column (e.g., region or category) and write it to different Parquet files:\n",
        "\n",
        "Code for Partitioning and Writing to Parquet\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession (this is usually pre-configured in Databricks)\n",
        "spark = SparkSession.builder.appName(\"PartitionedParquet\").getOrCreate()\n",
        "\n",
        "# Sample dataset: Replace this with your actual dataset\n",
        "data = [\n",
        "    (\"East\", \"Electronics\", 100),\n",
        "    (\"West\", \"Furniture\", 200),\n",
        "    (\"East\", \"Clothing\", 150),\n",
        "    (\"North\", \"Electronics\", 300),\n",
        "    (\"South\", \"Clothing\", 250),\n",
        "    (\"West\", \"Electronics\", 400),\n",
        "    (\"East\", \"Furniture\", 350)\n",
        "]\n",
        "\n",
        "# Define schema and create a DataFrame\n",
        "columns = [\"region\", \"category\", \"sales\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "(((((((\n",
        "We can create separate explicit schema\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define schema explicitly\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "\n",
        "# Apply the schema\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.printSchema()\n",
        ")))))))\n",
        "\n",
        "# Partition column: Replace 'region' with 'category' as needed\n",
        "partition_column = \"region\"  # Change to \"category\" for the second example\n",
        "\n",
        "# Output path for Parquet files (e.g., S3, DBFS)\n",
        "output_path = \"dbfs:/mnt/output/partitioned_data\"  # Change as required\n",
        "\n",
        "# Write DataFrame to partitioned Parquet files\n",
        "df.write.partitionBy(partition_column).parquet(output_path)\n",
        "\n",
        "print(f\"Data written to Parquet files partitioned by {partition_column}\")\n",
        "Explanation\n",
        "Input Dataset:\n",
        "\n",
        "Replace the sample data with your actual DataFrame from a source like S3, Delta Lake, or a SQL database.\n",
        "Partitioning Column:\n",
        "\n",
        "Set the partition_column to the column by which you want to partition the Parquet files (e.g., region or category).\n",
        "Output Path:\n",
        "\n",
        "Specify the output_path where the partitioned Parquet files should be written. This could be:\n",
        "dbfs:/mnt/output/... for Databricks File System.\n",
        "s3://bucket-name/... for AWS S3.\n",
        "Partition and Write:\n",
        "\n",
        "Use the partitionBy() method to group data by the specified column, creating a folder for each unique value in the column.\n",
        "Parquet files for each partition are written inside the corresponding folder.\n",
        "Output Directory Structure\n",
        "If partition_column = 'region', the Parquet files are saved as:\n",
        "\n",
        "\n",
        "/mnt/output/partitioned_data/\n",
        "├── region=East/\n",
        "│   ├── part-00000.parquet\n",
        "│   └── part-00001.parquet\n",
        "├── region=West/\n",
        "│   ├── part-00000.parquet\n",
        "│   └── part-00001.parquet\n",
        "├── region=North/\n",
        "│   ├── part-00000.parquet\n",
        "├── region=South/\n",
        "│   ├── part-00000.parquet\n",
        "For partition_column = 'category', the folder names will follow category=Electronics, category=Furniture, etc.\n",
        "\n",
        "Verify Written Data\n",
        "To verify the Parquet files:\n",
        "\n",
        "\n",
        "# Load the partitioned Parquet files\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "\n",
        "# Show data to verify\n",
        "partitioned_df.show()\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZSOhpXjQljf"
      },
      "outputs": [],
      "source": [
        "#You are a data engineer explain this project in detail use Pyspark and python pandas and no SQL whereever required. And also focused on horizon insight database oracle\n",
        "'''\n",
        "I'll help explain this Horizon Insights data engineering project and provide implementation suggestions using PySpark and Pandas. Let me break down the architecture and provide some code examples.\n",
        "\n",
        "The project appears to be a comprehensive data platform serving since 2017 with impressive metrics: 800+ regular users, 5 queries/second average throughput, and 18TB+ of accumulated data. Let me break this down into key components:\n",
        "\n",
        "1. Data Sources & Ingestion:\n",
        "Let's handle the various data sources using PySpark:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HorizonInsights\") \\\n",
        "    .config(\"spark.jars\", \"oracle-jdbc.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Function to read from various sources\n",
        "def ingest_service_data(source_type, path):\n",
        "    if source_type == \"jenkins\":\n",
        "        # Read Jenkins JSON data\n",
        "        df = spark.read.json(path)\n",
        "    elif source_type == \"jira\":\n",
        "        # Read Jira REST API data\n",
        "        df = spark.read.json(path)\n",
        "    elif source_type == \"qtest\":\n",
        "        # Read qTest data\n",
        "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
        "    return df\n",
        "```\n",
        "\n",
        "2. Data Transformation & Loading:\n",
        "Given the metrics (140K+ stories/month, 18K+ bugs/month), we need robust transformation:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "def transform_metrics_data(df, metric_type):\n",
        "    if metric_type == \"stories\":\n",
        "        # Transform stories data\n",
        "        df = df.withColumn(\"month\", month(col(\"created_date\"))) \\\n",
        "               .withColumn(\"year\", year(col(\"created_date\"))) \\\n",
        "               .groupBy(\"month\", \"year\") \\\n",
        "               .agg(count(\"story_id\").alias(\"story_count\"))\n",
        "\n",
        "    elif metric_type == \"jenkins_jobs\":\n",
        "        # Transform Jenkins jobs data\n",
        "        df = df.withColumn(\"status\", when(col(\"result\") == \"SUCCESS\", 1).otherwise(0)) \\\n",
        "               .groupBy(\"job_name\") \\\n",
        "               .agg(\n",
        "                   sum(\"status\").alias(\"successful_builds\"),\n",
        "                   count(\"*\").alias(\"total_builds\")\n",
        "               )\n",
        "    return df\n",
        "```\n",
        "\n",
        "3. Oracle Database Integration:\n",
        "For the Horizon Insights DB, which appears to be Oracle-based:\n",
        "\n",
        "```python\n",
        "def write_to_oracle(df, table_name):\n",
        "    # Oracle connection properties\n",
        "    oracle_properties = {\n",
        "        \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
        "        \"url\": \"jdbc:oracle:thin:@//hostname:port/service\",\n",
        "        \"user\": \"username\",\n",
        "        \"password\": \"password\"\n",
        "    }\n",
        "\n",
        "    # Write to Oracle\n",
        "    df.write \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", oracle_properties[\"url\"]) \\\n",
        "        .option(\"dbtable\", table_name) \\\n",
        "        .option(\"user\", oracle_properties[\"user\"]) \\\n",
        "        .option(\"password\", oracle_properties[\"password\"]) \\\n",
        "        .mode(\"append\") \\\n",
        "        .save()\n",
        "```\n",
        "\n",
        "4. Reporting Pipeline:\n",
        "For the reporting requirements (MetricsHQ, SDLC, Compliance):\n",
        "\n",
        "```python\n",
        "def generate_metrics_report():\n",
        "    # Create a reporting dataframe\n",
        "    metrics_df = pd.DataFrame()\n",
        "\n",
        "    # Get Jenkins metrics\n",
        "    jenkins_metrics = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            DATE_TRUNC('month', timestamp) as month,\n",
        "            COUNT(*) as total_jobs,\n",
        "            SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_jobs\n",
        "        FROM jenkins_jobs\n",
        "        GROUP BY DATE_TRUNC('month', timestamp)\n",
        "    \"\"\")\n",
        "\n",
        "    # Convert to pandas for reporting\n",
        "    metrics_df['jenkins_success_rate'] = jenkins_metrics.toPandas()['successful_jobs'] / \\\n",
        "                                       jenkins_metrics.toPandas()['total_jobs']\n",
        "\n",
        "    return metrics_df\n",
        "```\n",
        "\n",
        "Key Features to Note:\n",
        "1. Volume Handling:\n",
        "- Processing 470K+ Jenkins jobs/month\n",
        "- Managing 350K+ Jobs/Month from Tower\n",
        "- Handling 300K+ qTest runs/month\n",
        "\n",
        "2. Performance Optimization:\n",
        "```python\n",
        "# Configure Spark for optimal performance\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
        "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
        "```\n",
        "\n",
        "3. Data Quality Checks:\n",
        "```python\n",
        "def data_quality_check(df, table_name):\n",
        "    # Check for nulls\n",
        "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "    # Check for duplicates\n",
        "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
        "\n",
        "    # Log quality metrics\n",
        "    print(f\"Quality checks for {table_name}\")\n",
        "    null_counts.show()\n",
        "    print(f\"Duplicate records: {duplicate_count}\")\n",
        "```\n",
        "\n",
        "The system seems to be processing a significant amount of DevOps and testing data, with various downstream consumers like Jira, Release Manager, and TechABS. The architecture uses SSIS for data ingestion, suggesting this is likely an enterprise-level solution.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzf_9ihtQljf"
      },
      "outputs": [],
      "source": [
        "# Thoughtfocus Interview Questions on Spark  for Data Engineer\n",
        "\n",
        "#1. How to handle skewed data in spark job to prevent performance issues?\n",
        "'''\n",
        "To handle skewed data in Spark jobs and prevent performance issues, consider these strategies:\n",
        "\n",
        "Data Repartitioning\n",
        "\n",
        "Use repartition() or coalesce() to redistribute data more evenly\n",
        "Specify a custom partitioning strategy based on key distribution\n",
        "Salting Technique\n",
        "Add a random salt to skewed keys before aggregation\n",
        "Helps break up data hotspots and distribute processing load\n",
        "\n",
        "Broadcast Join for Small Datasets\n",
        "Use broadcast() for smaller datasets to reduce shuffle overhead\n",
        "Prevents skew in join operations with significantly different-sized tables\n",
        "Adaptive Query Execution\n",
        "Enable Spark's adaptive query execution (AQE)\n",
        "Dynamically coalesces and optimizes shuffle partitions at runtime\n",
        "\n",
        "Skew Hint in Spark SQL\n",
        "Use SKEW HINT to provide optimizer hints about data distribution\n",
        "Helps Spark manage skewed data more intelligently during joins\n",
        "Example of salting technique:\n",
        "\n",
        "def salt_key(key):\n",
        "    return f\"{key}_{random.randint(0, num_salt_buckets)}\"\n",
        "\n",
        "df = df.withColumn(\"salted_key\", salt_key(df.original_key))\n",
        "These approaches can significantly improve Spark job performance when dealing with unevenly distributed data.\n",
        "'''\n",
        "\n",
        "#1. What is diff bw spark session and spark context?\n",
        "'''\n",
        "Key differences between SparkSession and SparkContext:\n",
        "SparkSession:\n",
        "\n",
        "High-level entry point for Spark functionality\n",
        "Introduced in Spark 2.0\n",
        "Combines SparkContext, SQLContext, and HiveContext\n",
        "Single point of entry for DataFrame and SQL operations\n",
        "Easier to use, more comprehensive API\n",
        "\n",
        "SparkContext:\n",
        "\n",
        "Low-level API for creating RDDs\n",
        "Core entry point for Spark core functionality\n",
        "Manages cluster resources and coordination\n",
        "Existed before SparkSession\n",
        "Still used internally by SparkSession\n",
        "\n",
        "SparkSession (preferred in modern Spark)\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "# SparkContext (can be accessed from SparkSession)\n",
        "sc = spark.sparkContext\n",
        "'''\n",
        "\n",
        "#3. How do you handle backpressure in Spark Streaming applications to manage data load effectively?\n",
        "'''\n",
        "Handling backpressure in Spark Streaming involves several key strategies:\n",
        "\n",
        "Trigger-based Backpressure\n",
        "\n",
        "\n",
        "Configure maxOffsetsPerTrigger to limit data processed per batch\n",
        "Prevents overwhelming processing resources\n",
        "\n",
        "\n",
        "Rate Limiting\n",
        "\n",
        "\n",
        "Use spark.streaming.receiver.maxRate to control input rate\n",
        "Dynamically adjust receiver rate based on processing capabilities\n",
        "\n",
        "\n",
        "Adaptive Batch Processing\n",
        "\n",
        "\n",
        "Enable adaptive query execution (AQE)\n",
        "Automatically adjust partition sizes and execution plans\n",
        "\n",
        "\n",
        "Windowing and Micro-batching\n",
        "\n",
        "\n",
        "Use sliding windows to control data ingestion\n",
        "Break large data streams into manageable micro-batches\n",
        "\n",
        "Example configuration:\n",
        "pythonCopyspark_conf = SparkConf().set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
        "                        .set(\"spark.streaming.backpressure.initialRate\", 100)\n",
        "                        .set(\"spark.streaming.receiver.maxRate\", 200)\n",
        "\n",
        "ssc = StreamingContext(sc, batchDuration=1, conf=spark_conf)\n",
        "\n",
        "Custom Backpressure Handling\n",
        "\n",
        "Implement custom rate controllers\n",
        "Monitor processing time and dynamically adjust input rates\n",
        "\n",
        "Key goal: Balance data ingestion with processing capacity to prevent system overload.\n",
        "'''\n",
        "\n",
        "#4. What is delta lake and how does it improve data reliability and performance in Spark applications?\n",
        "'''\n",
        "Delta Lake is an open-source storage layer that enhances Spark data lakes with key improvements:\n",
        "\n",
        "ACID Transactions\n",
        "\n",
        "Ensures data integrity\n",
        "Supports concurrent writes and reads\n",
        "Atomic and consistent operations\n",
        "\n",
        "Time Travel & Versioning\n",
        "\n",
        "Allows querying historical data versions\n",
        "Enables easy rollbacks and data recovery\n",
        "Tracks complete change history\n",
        "\n",
        "Performance Optimizations\n",
        "\n",
        "Supports data compaction\n",
        "Provides efficient metadata management\n",
        "Enables file-level optimization\n",
        "\n",
        "Schema Evolution\n",
        "\n",
        "Allows dynamic schema updates\n",
        "Supports adding/removing columns seamlessly\n",
        "Maintains backward compatibility\n",
        "\n",
        "Writing Delta Table\n",
        "df.write.format(\"delta\").save(\"/path/to/delta_table\")\n",
        "\n",
        "# Reading with time travel\n",
        "spark.read.format(\"delta\") \\\n",
        "     .option(\"versionAsOf\", 1) \\\n",
        "     .load(\"/path/to/delta_table\")\n",
        "'''\n",
        "\n",
        "#5. What is difference between cache and persist in Spark?\n",
        "'''\n",
        "Key differences between cache() and persist() in Spark:\n",
        "cache():\n",
        "\n",
        "Default storage level: MEMORY_ONLY\n",
        "Stores RDD/DataFrame in memory\n",
        "Least configurable persistence option\n",
        "Equivalent to persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "persist():\n",
        "\n",
        "More flexible storage level configuration\n",
        "Options include:\n",
        "\n",
        "MEMORY_ONLY\n",
        "MEMORY_AND_DISK\n",
        "DISK_ONLY\n",
        "Off-heap storage\n",
        "\n",
        "Allows explicit memory/disk trade-offs\n",
        "\n",
        "Cache (memory only)\n",
        "df.cache()\n",
        "\n",
        "# Persist with custom storage level\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "Both methods help optimize performance by avoiding redundant computations, but persist() offers more granular control.\n",
        "'''\n",
        "\n",
        "#6. What is difference between repartition and coalesce in Spark?\n",
        "'''\n",
        "Key differences between repartition() and coalesce() in Spark:\n",
        "repartition():\n",
        "\n",
        "Full shuffle of entire dataset\n",
        "Can increase or decrease partition count\n",
        "More expensive operation\n",
        "Provides uniform data distribution\n",
        "\n",
        "coalesce():\n",
        "\n",
        "Minimizes data movement\n",
        "Reduces partition count efficiently\n",
        "Recommended for decreasing partitions\n",
        "No full data shuffle\n",
        "Preserves existing partitioning\n",
        "\n",
        "Repartition (full shuffle)\n",
        "df.repartition(10)\n",
        "\n",
        "# Coalesce (minimal data movement)\n",
        "df.coalesce(5)\n",
        "'''\n",
        "#7. What is difference between partitioning and bucketing in Spark SQL?\n",
        "'''\n",
        "Partitioning: Divides data into directories based on column values\n",
        "Bucketing: Distributes data into fixed number of buckets for optimization\n",
        "\n",
        "Comprehensive Example:\n",
        "Partitioning\n",
        "df.write \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .save(\"/path/partitioned_data\")\n",
        "\n",
        "# Bucketing\n",
        "df.write \\\n",
        "    .bucketBy(10, \"user_id\") \\\n",
        "    .sortBy(\"timestamp\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .save(\"/path/bucketed_data\")\n",
        "\n",
        "# Query Optimization\n",
        "# Partitioning: Fast for filtering partitioned columns\n",
        "# Bucketing: Efficient for joins and aggregations on high-cardinality columns\n",
        "'''\n",
        "\n",
        "#8. What is the difference between broadcast join and shuffle join in Spark?\n",
        "'''\n",
        "Broadcast Join vs Shuffle Join in Spark:\n",
        "Broadcast Join:\n",
        "\n",
        "Small table is sent to all executor nodes\n",
        "No data shuffling required\n",
        "Efficient for small dimension tables\n",
        "Lower network overhead\n",
        "Best when one table is significantly smaller\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Broadcast join small dimension table\n",
        "result = large_df.join(\n",
        "    broadcast(small_df),\n",
        "    \"join_key\"\n",
        ")\n",
        "Shuffle Join:\n",
        "\n",
        "Data shuffled across all partitions\n",
        "Both tables partitioned and redistributed\n",
        "High network and shuffle overhead\n",
        "Suitable for large tables\n",
        "Default join strategy\n",
        "\n",
        "Performance Comparison:\n",
        "\n",
        "Broadcast Join: O(small_table_size)\n",
        "Shuffle Join: O(large_table1_size * large_table2_size)\n",
        "\n",
        "Key Recommendation: Use broadcast join for small tables to optimize performance.\n",
        "'''\n",
        "\n",
        "#9. What is the difference between broadcast variable and accumulator in Spark?\n",
        "'''\n",
        "Broadcast Variable vs Accumulator in Spark:\n",
        "Broadcast Variable:\n",
        "\n",
        "Read-only shared variable across cluster\n",
        "Efficiently distribute large, immutable data\n",
        "Cached on each executor\n",
        "Used for lookup tables, reference data\n",
        "Low communication overhead\n",
        "\n",
        "Broadcast lookup dictionary\n",
        "lookup_dict = sc.broadcast({\n",
        "    'A': 'High',\n",
        "    'B': 'Medium',\n",
        "    'C': 'Low'\n",
        "})\n",
        "\n",
        "def transform_data(row):\n",
        "    return lookup_dict.value.get(row.category, 'Unknown')\n",
        "\n",
        "df.rdd.map(transform_data)\n",
        "Accumulator:\n",
        "\n",
        "Mutable shared variable for aggregation\n",
        "Supports distributed counter/sum operations\n",
        "Only driver can read final accumulated value\n",
        "Useful for debugging, tracking metrics\n",
        "Supports numeric and custom types\n",
        "\n",
        "Count specific conditions\n",
        "error_count = sc.accumulator(0)\n",
        "\n",
        "def process_data(row):\n",
        "    if row.has_error:\n",
        "        error_count.add(1)\n",
        "    return row\n",
        "\n",
        "df.rdd.map(process_data).collect()\n",
        "print(error_count.value)\n",
        "Key Difference: Broadcast for read-only data distribution, Accumulators for aggregating metrics.\n",
        "'''\n",
        "#10. Difference between narrow and wide ?\n",
        "'''\n",
        "Narrow transformations involve operations that can be done within a single partition\n",
        "(no shuffle).\n",
        "Examples: map(), filter(), union(), sample()\n",
        "Wide transformations require data to be shuffled between partitions, which can be more\n",
        "resource-intensive.\n",
        "Examples: groupBy(), reduceByKey(), join(), distinct()\n",
        "'''\n",
        "#11. What is Out of Memory?\n",
        "'''\n",
        "Out of memory (OOM) errors occur when Spark does not have enough memory to\n",
        "process the data being handled. This can be due to:\n",
        "• Too many partitions or large datasets in a single partition.\n",
        "• Inefficient operations causing memory pressure.\n",
        "• Insufficient heap size or executor memory allocation.\n",
        "Solutions include optimizing partitioning, adjusting memory settings (e.g.,\n",
        "spark.executor.memory), or using disk storage for spilling.\n",
        "Driver Out of memory\n",
        "Executor out of memory\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_lYYsqQQljg"
      },
      "outputs": [],
      "source": [
        "# Revision of Spark and SQL for Data Engineer Interview for Capgemini\n",
        "#1. what this does this command do in pyspark\n",
        "#sc.parallelize([1, 2, 3, 4, 5])\n",
        "'''\n",
        "sc.parallelize([1, 2, 3, 4, 5])\n",
        "does the following:\n",
        "\n",
        "Creates an RDD (Resilient Distributed Dataset): This converts the Python list [1, 2, 3, 4, 5] into a distributed dataset (RDD).\n",
        "Distributes the data across Spark's cluster: The data is split into partitions and distributed across available worker nodes.\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
        "The collect() method retrieves the entire dataset from the distributed RDD back to the driver as a Python list.\n",
        "'''\n",
        "\n",
        "#2. What are transformations and actions in Spark?\n",
        "'''\n",
        "\n",
        "Transformations and Actions in PySpark\n",
        "In PySpark, there are two types of operations on RDDs (Resilient Distributed Datasets):\n",
        "\n",
        "Transformations: These are lazy operations that create a new RDD from an existing one. They do not execute immediately; they are only computed when an action is performed.\n",
        "\n",
        "Examples: map(), filter(), flatMap()\n",
        "Actions: These trigger the execution of transformations and return results to the driver program.\n",
        "\n",
        "Examples: collect(), count(), take()\n",
        "'''\n",
        "\n",
        "#3. What is the difference between map() and flatMap() and filter() transformations in Spark?\n",
        "'''\n",
        "1. map()\n",
        "The map() function applies a function to each element in the RDD and returns a new RDD with the transformed elements.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"TransformationsActions\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)  # Multiply each element by 2\n",
        "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
        "\n",
        "2. filter()\n",
        "The filter() function returns a new RDD containing only the elements that satisfy a given condition.\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers\n",
        "print(filtered_rdd.collect())  # Output: [2, 4]\n",
        "\n",
        "3. flatMap()\n",
        "The flatMap() function applies a function that returns an iterable and then flattens the result into a single RDD.\n",
        "rdd2 = sc.parallelize([\"hello world\", \"spark rdd\"])\n",
        "flat_mapped_rdd = rdd2.flatMap(lambda x: x.split(\" \"))  # Split each sentence into words\n",
        "print(flat_mapped_rdd.collect())  # Output: ['hello', 'world', 'spark', 'rdd']\n",
        "'''\n",
        "\n",
        "#4. What is difference between actions like collect(), count() and take() in Spark?\n",
        "'''\n",
        "1. collect()\n",
        "The collect() function retrieves all elements from the RDD and returns them as a list.\n",
        "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
        "⚠ Warning: Avoid using collect() on very large RDDs, as it can cause memory issues.\n",
        "\n",
        "2. count()\n",
        "The count() function returns the number of elements in the RDD.\n",
        "print(rdd.count())  # Output: 5\n",
        "\n",
        "3. take(n)\n",
        "The take(n) function retrieves the first n elements from the RDD.\n",
        "print(rdd.take(3))  # Output: [1, 2, 3]\n",
        "\n",
        "'''\n",
        "#5. explain with example in detail Lazy evaluation and DAG\n",
        "'''\n",
        "Lazy Evaluation and DAG in Spark\n",
        "Apache Spark follows a lazy evaluation model and processes computations using a Directed Acyclic Graph (DAG). Let’s break these concepts down in detail with an example.\n",
        "\n",
        "1. Lazy Evaluation\n",
        "Lazy evaluation means that Spark does not execute transformations immediately when they are called. Instead, it builds a logical execution plan and waits until an action (like .collect(), .show(), or .write()) is triggered. This approach optimizes query execution and reduces redundant computations.\n",
        "\n",
        "How Lazy Evaluation Works\n",
        "When you apply transformations (e.g., .filter(), .map(), .select()), Spark does not execute them immediately.\n",
        "Instead, it records them in a DAG (Directed Acyclic Graph).\n",
        "Once an action (e.g., .show(), .count(), .collect()) is triggered, Spark executes all transformations in an optimized way.\n",
        "\n",
        "2. DAG (Directed Acyclic Graph)\n",
        "A DAG is a logical representation of computations in Spark. It consists of:\n",
        "\n",
        "Vertices (Nodes) → Represent transformations like .filter(), .map(), etc.\n",
        "Edges (Lines between nodes) → Represent dependencies between transformations.\n",
        "'''\n",
        "#6. how to find the 2nd highest salary department-wise using both SQL (Oracle) and PySpark, using JOIN, CTE, and PARTITION BY.\n",
        "'''\n",
        "SQL (Oracle) Query:\n",
        "WITH RankedSalaries AS (\n",
        "    SELECT e.id, e.name, e.salary, e.dept_id, d.dept_name,\n",
        "           ROW_NUMBER() OVER (PARTITION BY e.dept_id ORDER BY e.salary DESC) AS rnk\n",
        "    FROM emp e\n",
        "    JOIN dept d ON e.dept_id = d.dept_id\n",
        ")\n",
        "SELECT id, name, salary, dept_id, dept_name\n",
        "FROM RankedSalaries\n",
        "WHERE rnk = 2;  -- 2nd highest salary per department\n",
        "\n",
        "PySpark Code:\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Spark Session\n",
        "spark = SparkSession.builder.appName(\"SecondHighestSalary\").getOrCreate()\n",
        "\n",
        "# Employee Dataset\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 5000, 1),\n",
        "    (2, \"Bob\", 7000, 1),\n",
        "    (3, \"Charlie\", 6000, 1),\n",
        "    (4, \"David\", 8000, 2),\n",
        "    (5, \"Eve\", 9000, 2),\n",
        "    (6, \"Frank\", 7500, 2),\n",
        "]\n",
        "\n",
        "dept_data = [\n",
        "    (1, \"HR\"),\n",
        "    (2, \"IT\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "emp_df = spark.createDataFrame(emp_data, [\"id\", \"name\", \"salary\", \"dept_id\"])\n",
        "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "Using JOIN + PARTITION BY + ROW_NUMBER()\n",
        "# Define Window Specification\n",
        "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Rank salaries department-wise\n",
        "ranked_df = emp_df.withColumn(\"rnk\", row_number().over(window_spec))\n",
        "\n",
        "# Filter for 2nd highest salary\n",
        "second_highest_df = ranked_df.filter(col(\"rnk\") == 2)\n",
        "\n",
        "# Join with Department DataFrame to get dept_name\n",
        "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
        "\n",
        "# Show Result\n",
        "result_df.show()\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTsUCfsZQljh"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Spark Session\n",
        "spark = SparkSession.builder.appName(\"SecondHighestSalary\").getOrCreate()\n",
        "\n",
        "# Employee Dataset\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 5000, 1),\n",
        "    (2, \"Bob\", 7000, 1),\n",
        "    (3, \"Charlie\", 6000, 1),\n",
        "    (4, \"David\", 8000, 2),\n",
        "    (5, \"Eve\", 9000, 2),\n",
        "    (6, \"Frank\", 7500, 2),\n",
        "]\n",
        "\n",
        "dept_data = [\n",
        "    (1, \"HR\"),\n",
        "    (2, \"IT\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "emp_df = spark.createDataFrame(emp_data, [\"id\", \"name\", \"salary\", \"dept_id\"])\n",
        "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "#Using JOIN + PARTITION BY + ROW_NUMBER()\n",
        "# Define Window Specification\n",
        "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Rank salaries department-wise\n",
        "ranked_df = emp_df.withColumn(\"rnk\", row_number().over(window_spec))\n",
        "\n",
        "# Filter for 2nd highest salary\n",
        "second_highest_df = ranked_df.filter(col(\"rnk\") == 2)\n",
        "\n",
        "# Join with Department DataFrame to get dept_name\n",
        "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
        "\n",
        "# Show Result\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "process 5 Tb of data in pyspark\n",
        "\n",
        "5tb=5*1024 GB=5*1024*1024Mb=5242880 Mb\n",
        "\n",
        "each partion default memory size vary from 128Mb to 256 Mb\n",
        "\n",
        "if I am considering 256 Mb\n",
        "then  total number of patitions= 5242880/256=20448 partitions\n",
        "\n",
        "As common thumb rules each partition contains 1-2 cpu cores\n",
        "20448*2=40896 CPU Cores\n",
        "\n",
        "Assuming each node can have 200-4000 partitions and each node contains 8 executors if we consider each node memory is 64 GB and each executer memory 8 gb\n",
        "\n",
        "so no. of nodes= no. of partitions/partitions per node\n",
        "\n",
        "20448/200=102 nodes\n",
        "\n",
        "let suppose no. of cpu cores per node:-16 cores\n",
        "\n",
        "'''\n",
        "\"A node typically has between 8 to 64 CPU cores, depending on the configuration and machine type. For example, in a cloud environment like AWS, an instance might have 16 or 32 cores.\"\n",
        "'''\n",
        "\n",
        "A common thumb rule will allocate 2-4 gb of memory per cpu core\n",
        "\n",
        "if u configure 4 executors per node\n",
        "\n",
        "if each executer get 4 cores\n",
        "\n",
        "so=4*4=16 cpu cores per node\n",
        "\n",
        "4 exec* 16 gb each exec memory=64 each node memory\n",
        "\n",
        "no. of executers cores=no. of nodes* 16 cpu cores per node\n",
        "=102*16=1632\n",
        "\n",
        "no. of executors=no. of exec cores/no. of cores per executers\n",
        "1632/4=408 executers\n",
        "\n",
        "each executer memory=no. of cores* each core memory\n",
        " 4*4=16 gb\n",
        "\n",
        " total memory required= no. of executors* each memory size\n",
        " 408*16=6528 GB\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "8E3BqWN1QqWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}