{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9T3pnoPPjw4"
      },
      "outputs": [],
      "source": [
        "#Coforge DE Interview skills(Spark and SQL) 11 jan 24\n",
        "\n",
        "#1. What is spark architecture?\n",
        "\"\"\"Apache Spark Architecture:\n",
        "1. Driver: The driver is the main control process that creates the SparkContext, connects to the cluster, and coordinates the execution of tasks. It translates the user program into tasks and schedules them to run on executors.\n",
        "2. Executors: Executors are worker nodes that run individual tasks in a distributed manner. Each executor has its own JVM and runs tasks assigned by the driver. Executors also provide in-memory storage for RDDs that are cached by user programs through SparkContext.\n",
        "3. Cluster Manager: The cluster manager is responsible for managing the resources across the cluster. Spark supports various cluster managers like Standalone, Apache Mesos, kubernates and Hadoop YARN. The cluster manager allocates resources to the driver and executors.\n",
        "4. RDD (Resilient Distributed Dataset): RDD is the fundamental data structure of Spark. It is an immutable distributed collection of objects that can be processed in parallel. RDDs support two types of operations: transformations (e.g., map, filter) and actions (e.g., count, collect).\n",
        "5. DAG (Directed Acyclic Graph): Spark creates a logical execution plan in the form of a DAG. The DAG represents the sequence of transformations applied to the data. The DAG scheduler divides the operators into stages of tasks and submits them to the task scheduler.\n",
        "6. Task Scheduler: The task scheduler is responsible for scheduling tasks on the executors. It takes the stages from the DAG scheduler and assigns tasks to executors based on data locality and resource availability.\n",
        "In summary, Spark architecture consists of a driver, executors, cluster manager, RDDs, DAG, and task scheduler, working together to process large-scale data in a distributed and fault-tolerant manner.\n",
        "\"\"\"\n",
        "\n",
        "#How does Spark distribute tasks for processing a 1 GB file, and how are tasks and executors allocated during execution?\n",
        "'''Spark divides the 1 GB file into partitions (default is 128 MB per partition for HDFS), creating about 8 partitions for a 1 GB file. Each partition becomes a task.\n",
        "\n",
        "Executors: Each executor processes multiple tasks based on available cores.\n",
        "Tasks per Executor: If an executor has 4 cores, it can run 4 tasks in parallel.'''\n",
        "\n",
        "# What is spark-submit explain in details?\n",
        "'''\n",
        "spark-submit is a command-line tool used to launch Spark applications on a cluster or locally. It handles resource allocation, configuration, and submission of jobs to Spark clusters like YARN, Kubernetes, Mesos, or standalone mode.\n",
        "\n",
        "Key Responsibilities:\n",
        "Distribute Application Code: Sends your Spark application JAR or Python script to the cluster.\n",
        "Resource Management: Allocates memory and CPU resources for the driver and executors.\n",
        "Configuration Management: Sets properties like application name, Spark master, deploy mode, and more.\n",
        "Job Execution: Manages the execution of tasks and monitors job progress.\n",
        "Command Syntax:\n",
        "\n",
        "spark-submit \\\n",
        "  --master <master-url> \\\n",
        "  --deploy-mode <deploy-mode> \\\n",
        "  --class <main-class> \\\n",
        "  --executor-memory <memory> \\\n",
        "  --executor-cores <cores> \\\n",
        "  application.jar <application-arguments>\n",
        "Common Parameters:\n",
        "Parameter\tDescription\n",
        "--master\tSpecifies the cluster manager (local, yarn, etc.)\n",
        "--deploy-mode\tDeployment mode (client or cluster)\n",
        "--class\tMain class of the application (for Java/Scala)\n",
        "--executor-memory\tMemory per executor (e.g., 2G)\n",
        "--executor-cores\tNumber of cores per executor\n",
        "--num-executors\tNumber of executors to launch\n",
        "--conf\tSet configuration properties (e.g., spark.executor.memory)\n",
        "Example Usage:\n",
        "Running a Spark Application Locally:\n",
        "\n",
        "spark-submit\n",
        "  --master local[2]\n",
        "  my_spark_app.py\n",
        "\n",
        "Running on a YARN Cluster:\n",
        "\n",
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --executor-memory 4G \\\n",
        "  --executor-cores 4 \\\n",
        "  my_spark_app.py\n",
        "Execution Modes:\n",
        "Client Mode: Driver runs on the machine where spark-submit is executed. Suitable for local testing.\n",
        "Cluster Mode: Driver runs on a worker node within the cluster. Ideal for large-scale production jobs.\n",
        "\n",
        "Important Points:\n",
        "Driver Program: Manages job execution and task scheduling.\n",
        "Executors: Run the actual tasks on worker nodes and store intermediate data.\n",
        "Partitions: Spark splits input data into smaller partitions for distributed processing.\n",
        "By leveraging spark-submit, Spark achieves scalability and parallel processing across large datasets efficiently.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj3leA1OPjw7",
        "outputId": "72a68775-b789-4bcc-cd22-ad3c109f54de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7839, 'KING', 'PRESIDENT', None, datetime.datetime(1981, 11, 17, 0, 0), 5100.0, None, 20)\n"
          ]
        }
      ],
      "source": [
        "# Synechron Interview Questions on Database for Data Engineer\n",
        "\n",
        "# Make Oracle DB connection in Python and write a query?\n",
        "\n",
        "#%pip install cx_Oracle\n",
        "import cx_Oracle\n",
        "\n",
        "dsn_tns=cx_Oracle.makedsn('localhost','1521',service_name='orcl')\n",
        "conn=cx_Oracle.connect(user='SivaAcademy',password='pass',dsn=dsn_tns) #while connecting only give req pass\n",
        "c=conn.cursor()\n",
        "c.execute('select * from emp')\n",
        "print(c.fetchone()) # we can use fetchall() for all or  for row in c:\n",
        "    #print(row)\n",
        "\n"
      ]
<<<<<<< HEAD
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Find products that were not sold in the last 7 months.\n",
    "query='''SELECT product_id\n",
    "FROM Sales_data\n",
    "WHERE product_id NOT IN (\n",
    "    SELECT DISTINCT product_id \n",
    "    FROM Sales_data \n",
    "    WHERE sales_date >= ADD_MONTHS(SYSDATE, -7)) group by product_id'''\n",
    "c.execute(query)\n",
    "for i in c:\n",
    "    print(i)\n",
    "\n",
    "'''\n",
    "Analysis:\n",
    "Suppose I have written this query\n",
    "SELECT DISTINCT product_id \n",
    "FROM Sales_data \n",
    "WHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\n",
    "\n",
    "this will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Berribot Interview Questions on ETL and pyspark for Wipro\n",
    "# Schedule AWS databrics spark job daily in midnight script\n",
    "'''\n",
    "To schedule an AWS Databricks Spark job to run daily at midnight, you can use Databricks Workflows (preferred) or an external scheduling tool like AWS Lambda or CloudWatch. Below is an example script for each approach:\n",
    "\n",
    "1. Using Databricks Workflows (Recommended)\n",
    "Create a Databricks Job:\n",
    "\n",
    "Go to Databricks Workspace.\n",
    "Navigate to Workflows > Jobs.\n",
    "Click Create Job.\n",
    "Add your notebook or Spark job details.\n",
    "Configure job clusters and libraries if required.\n",
    "Schedule the Job:\n",
    "\n",
    "In the Schedule tab, set the schedule to:\n",
    "Daily\n",
    "Start time: 00:00\n",
    "Save the workflow.\n",
    "2. Using AWS CloudWatch with Databricks REST API\n",
    "Prerequisites:\n",
    "A Databricks personal access token.\n",
    "Databricks job ID or notebook path.\n",
    "Python Script:\n",
    "\n",
    "import requests\n",
    "\n",
    "# Databricks Workspace URL\n",
    "DATABRICKS_HOST = \"https://<databricks-instance>.cloud.databricks.com\"\n",
    "\n",
    "# Databricks Personal Access Token\n",
    "DATABRICKS_TOKEN = \"<your-databricks-token>\"\n",
    "\n",
    "# Job ID\n",
    "JOB_ID = \"<your-job-id>\"\n",
    "\n",
    "# Trigger Job Function\n",
    "def trigger_job():\n",
    "    url = f\"{DATABRICKS_HOST}/api/2.1/jobs/run-now\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"job_id\": JOB_ID}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Job triggered successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger job: {response.content}\")\n",
    "\n",
    "# Run the function\n",
    "trigger_job()\n",
    "Schedule via AWS CloudWatch:\n",
    "Open AWS Management Console.\n",
    "Go to CloudWatch > Rules.\n",
    "Create a new rule:\n",
    "Event Source: Schedule.\n",
    "Schedule Expression: cron(0 0 * * ? *) (midnight daily).\n",
    "Target:\n",
    "Choose AWS Lambda.\n",
    "Add the Python script to your Lambda function.\n",
    "3. Using Crontab on EC2\n",
    "If you prefer running the script from an EC2 instance:\n",
    "\n",
    "SSH into the EC2 instance.\n",
    "Edit the crontab:\n",
    "\n",
    "crontab -e\n",
    "Add the following entry to trigger the script at midnight:\n",
    "\n",
    "0 0 * * * python3 /path/to/trigger_databricks_job.py\n",
    "Which approach do you prefer? I can guide you further.'''\n",
    "\n",
    "\n",
    "#Write AWS databrics notebook to calculate rolling average over time periods for q time series data set.\n",
    "\n",
    "#Input : time period '7 days' dataset: 'stock_prices''\n",
    "\n",
    "#Output: dataframe with 7-day rolling averages\n",
    "\n",
    "'''Here is a Databricks notebook code that calculates the 7-day rolling average for a time series dataset (stock_prices) using PySpark.\n",
    "\n",
    "Code for the Databricks Notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, window\n",
    "\n",
    "# Initialize SparkSession (this is usually already set up in Databricks)\n",
    "spark = SparkSession.builder.appName(\"RollingAverage\").getOrCreate()\n",
    "\n",
    "# Sample dataset: Replace this with your actual dataset\n",
    "data = [\n",
    "    (\"2025-01-01\", 100),\n",
    "    (\"2025-01-02\", 110),\n",
    "    (\"2025-01-03\", 105),\n",
    "    (\"2025-01-04\", 120),\n",
    "    (\"2025-01-05\", 130),\n",
    "    (\"2025-01-06\", 125),\n",
    "    (\"2025-01-07\", 140),\n",
    "    (\"2025-01-08\", 145),\n",
    "    (\"2025-01-09\", 150),\n",
    "    (\"2025-01-10\", 155)\n",
    "]\n",
    "\n",
    "# Define schema and create a DataFrame\n",
    "columns = [\"date\", \"price\"]\n",
    "stock_prices_df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Convert 'date' column to TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "stock_prices_df = stock_prices_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Calculate 7-day rolling average\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define window specification\n",
    "rolling_window = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Add rolling average column\n",
    "result_df = stock_prices_df.withColumn(\"7_day_avg\", avg(\"price\").over(rolling_window))\n",
    "\n",
    "# Show the results\n",
    "result_df.show()\n",
    "Explanation of the Code\n",
    "Dataset Initialization:\n",
    "\n",
    "Replace the data variable with your actual dataset from stock_prices.\n",
    "Date Conversion:\n",
    "\n",
    "Convert the date column to DateType for proper time series handling.\n",
    "Define Window Specification:\n",
    "\n",
    "Use Window.orderBy(\"date\").rowsBetween(-6, 0) to create a rolling window of 7 days (current row and 6 previous rows).\n",
    "Calculate Rolling Average:\n",
    "\n",
    "Use the avg function to compute the 7-day rolling average.\n",
    "Result:\n",
    "\n",
    "A new column 7_day_avg is added to the DataFrame showing the rolling average.\n",
    "Sample Output\n",
    "date\tprice\t7_day_avg\n",
    "2025-01-01\t100\t100.0\n",
    "2025-01-02\t110\t105.0\n",
    "2025-01-03\t105\t105.0\n",
    "2025-01-04\t120\t108.75\n",
    "2025-01-05\t130\t113.0\n",
    "2025-01-06\t125\t115.0\n",
    "2025-01-07\t140\t118.57\n",
    "2025-01-08\t145\t125.0\n",
    "2025-01-09\t150\t130.0\n",
    "2025-01-10\t155\t135.71\n",
    "Let me know if you'd like help loading the actual dataset from S3 or a database!'''\n",
    "\n",
    "\n",
    "#Partition a large dataframe in databrics by a given column and write to different parquet files.\n",
    "\n",
    "#Input: column: 'region'\n",
    "#Output: Partitioned parquet files by 'region' \n",
    "\n",
    "#Eg 2 Input: column: 'category'\n",
    "#Output: Partioned Parquet files by 'category'\n",
    "\n",
    "'''\n",
    "Here’s how to partition a large DataFrame in Databricks by a given column (e.g., region or category) and write it to different Parquet files:\n",
    "\n",
    "Code for Partitioning and Writing to Parquet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession (this is usually pre-configured in Databricks)\n",
    "spark = SparkSession.builder.appName(\"PartitionedParquet\").getOrCreate()\n",
    "\n",
    "# Sample dataset: Replace this with your actual dataset\n",
    "data = [\n",
    "    (\"East\", \"Electronics\", 100),\n",
    "    (\"West\", \"Furniture\", 200),\n",
    "    (\"East\", \"Clothing\", 150),\n",
    "    (\"North\", \"Electronics\", 300),\n",
    "    (\"South\", \"Clothing\", 250),\n",
    "    (\"West\", \"Electronics\", 400),\n",
    "    (\"East\", \"Furniture\", 350)\n",
    "]\n",
    "\n",
    "# Define schema and create a DataFrame\n",
    "columns = [\"region\", \"category\", \"sales\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "(((((((\n",
    "We can create separate explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "\n",
    "# Apply the schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    ")))))))\n",
    "\n",
    "# Partition column: Replace 'region' with 'category' as needed\n",
    "partition_column = \"region\"  # Change to \"category\" for the second example\n",
    "\n",
    "# Output path for Parquet files (e.g., S3, DBFS)\n",
    "output_path = \"dbfs:/mnt/output/partitioned_data\"  # Change as required\n",
    "\n",
    "# Write DataFrame to partitioned Parquet files\n",
    "df.write.partitionBy(partition_column).parquet(output_path)\n",
    "\n",
    "print(f\"Data written to Parquet files partitioned by {partition_column}\")\n",
    "Explanation\n",
    "Input Dataset:\n",
    "\n",
    "Replace the sample data with your actual DataFrame from a source like S3, Delta Lake, or a SQL database.\n",
    "Partitioning Column:\n",
    "\n",
    "Set the partition_column to the column by which you want to partition the Parquet files (e.g., region or category).\n",
    "Output Path:\n",
    "\n",
    "Specify the output_path where the partitioned Parquet files should be written. This could be:\n",
    "dbfs:/mnt/output/... for Databricks File System.\n",
    "s3://bucket-name/... for AWS S3.\n",
    "Partition and Write:\n",
    "\n",
    "Use the partitionBy() method to group data by the specified column, creating a folder for each unique value in the column.\n",
    "Parquet files for each partition are written inside the corresponding folder.\n",
    "Output Directory Structure\n",
    "If partition_column = 'region', the Parquet files are saved as:\n",
    "\n",
    "\n",
    "/mnt/output/partitioned_data/\n",
    "├── region=East/\n",
    "│   ├── part-00000.parquet\n",
    "│   └── part-00001.parquet\n",
    "├── region=West/\n",
    "│   ├── part-00000.parquet\n",
    "│   └── part-00001.parquet\n",
    "├── region=North/\n",
    "│   ├── part-00000.parquet\n",
    "├── region=South/\n",
    "│   ├── part-00000.parquet\n",
    "For partition_column = 'category', the folder names will follow category=Electronics, category=Furniture, etc.\n",
    "\n",
    "Verify Written Data\n",
    "To verify the Parquet files:\n",
    "\n",
    "\n",
    "# Load the partitioned Parquet files\n",
    "partitioned_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show data to verify\n",
    "partitioned_df.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are a data engineer explain this project in detail use Pyspark and python pandas and no SQL whereever required. And also focused on horizon insight database oracle\n",
    "'''\n",
    "I'll help explain this Horizon Insights data engineering project and provide implementation suggestions using PySpark and Pandas. Let me break down the architecture and provide some code examples.\n",
    "\n",
    "The project appears to be a comprehensive data platform serving since 2017 with impressive metrics: 800+ regular users, 5 queries/second average throughput, and 18TB+ of accumulated data. Let me break this down into key components:\n",
    "\n",
    "1. Data Sources & Ingestion:\n",
    "Let's handle the various data sources using PySpark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HorizonInsights\") \\\n",
    "    .config(\"spark.jars\", \"oracle-jdbc.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to read from various sources\n",
    "def ingest_service_data(source_type, path):\n",
    "    if source_type == \"jenkins\":\n",
    "        # Read Jenkins JSON data\n",
    "        df = spark.read.json(path)\n",
    "    elif source_type == \"jira\":\n",
    "        # Read Jira REST API data\n",
    "        df = spark.read.json(path)\n",
    "    elif source_type == \"qtest\":\n",
    "        # Read qTest data\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    return df\n",
    "```\n",
    "\n",
    "2. Data Transformation & Loading:\n",
    "Given the metrics (140K+ stories/month, 18K+ bugs/month), we need robust transformation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def transform_metrics_data(df, metric_type):\n",
    "    if metric_type == \"stories\":\n",
    "        # Transform stories data\n",
    "        df = df.withColumn(\"month\", month(col(\"created_date\"))) \\\n",
    "               .withColumn(\"year\", year(col(\"created_date\"))) \\\n",
    "               .groupBy(\"month\", \"year\") \\\n",
    "               .agg(count(\"story_id\").alias(\"story_count\"))\n",
    "    \n",
    "    elif metric_type == \"jenkins_jobs\":\n",
    "        # Transform Jenkins jobs data\n",
    "        df = df.withColumn(\"status\", when(col(\"result\") == \"SUCCESS\", 1).otherwise(0)) \\\n",
    "               .groupBy(\"job_name\") \\\n",
    "               .agg(\n",
    "                   sum(\"status\").alias(\"successful_builds\"),\n",
    "                   count(\"*\").alias(\"total_builds\")\n",
    "               )\n",
    "    return df\n",
    "```\n",
    "\n",
    "3. Oracle Database Integration:\n",
    "For the Horizon Insights DB, which appears to be Oracle-based:\n",
    "\n",
    "```python\n",
    "def write_to_oracle(df, table_name):\n",
    "    # Oracle connection properties\n",
    "    oracle_properties = {\n",
    "        \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "        \"url\": \"jdbc:oracle:thin:@//hostname:port/service\",\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\"\n",
    "    }\n",
    "    \n",
    "    # Write to Oracle\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", oracle_properties[\"url\"]) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", oracle_properties[\"user\"]) \\\n",
    "        .option(\"password\", oracle_properties[\"password\"]) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "```\n",
    "\n",
    "4. Reporting Pipeline:\n",
    "For the reporting requirements (MetricsHQ, SDLC, Compliance):\n",
    "\n",
    "```python\n",
    "def generate_metrics_report():\n",
    "    # Create a reporting dataframe\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    # Get Jenkins metrics\n",
    "    jenkins_metrics = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', timestamp) as month,\n",
    "            COUNT(*) as total_jobs,\n",
    "            SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_jobs\n",
    "        FROM jenkins_jobs\n",
    "        GROUP BY DATE_TRUNC('month', timestamp)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert to pandas for reporting\n",
    "    metrics_df['jenkins_success_rate'] = jenkins_metrics.toPandas()['successful_jobs'] / \\\n",
    "                                       jenkins_metrics.toPandas()['total_jobs']\n",
    "    \n",
    "    return metrics_df\n",
    "```\n",
    "\n",
    "Key Features to Note:\n",
    "1. Volume Handling:\n",
    "- Processing 470K+ Jenkins jobs/month\n",
    "- Managing 350K+ Jobs/Month from Tower\n",
    "- Handling 300K+ qTest runs/month\n",
    "\n",
    "2. Performance Optimization:\n",
    "```python\n",
    "# Configure Spark for optimal performance\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
    "```\n",
    "\n",
    "3. Data Quality Checks:\n",
    "```python\n",
    "def data_quality_check(df, table_name):\n",
    "    # Check for nulls\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "    \n",
    "    # Log quality metrics\n",
    "    print(f\"Quality checks for {table_name}\")\n",
    "    null_counts.show()\n",
    "    print(f\"Duplicate records: {duplicate_count}\")\n",
    "```\n",
    "\n",
    "The system seems to be processing a significant amount of DevOps and testing data, with various downstream consumers like Jira, Release Manager, and TechABS. The architecture uses SSIS for data ingestion, suggesting this is likely an enterprise-level solution.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtfocus Interview Questions on Spark  for Data Engineer\n",
    "\n",
    "#1. How to handle skewed data in spark job to prevent performance issues?\n",
    "'''\n",
    "To handle skewed data in Spark jobs and prevent performance issues, consider these strategies:\n",
    "\n",
    "Data Repartitioning\n",
    "\n",
    "Use repartition() or coalesce() to redistribute data more evenly\n",
    "Specify a custom partitioning strategy based on key distribution\n",
    "Salting Technique\n",
    "Add a random salt to skewed keys before aggregation\n",
    "Helps break up data hotspots and distribute processing load\n",
    "\n",
    "Broadcast Join for Small Datasets\n",
    "Use broadcast() for smaller datasets to reduce shuffle overhead\n",
    "Prevents skew in join operations with significantly different-sized tables\n",
    "Adaptive Query Execution\n",
    "Enable Spark's adaptive query execution (AQE)\n",
    "Dynamically coalesces and optimizes shuffle partitions at runtime\n",
    "\n",
    "Skew Hint in Spark SQL\n",
    "Use SKEW HINT to provide optimizer hints about data distribution\n",
    "Helps Spark manage skewed data more intelligently during joins\n",
    "Example of salting technique:\n",
    "\n",
    "def salt_key(key):\n",
    "    return f\"{key}_{random.randint(0, num_salt_buckets)}\"\n",
    "\n",
    "df = df.withColumn(\"salted_key\", salt_key(df.original_key))\n",
    "These approaches can significantly improve Spark job performance when dealing with unevenly distributed data.\n",
    "'''\n",
    "\n",
    "#1. What is diff bw spark session and spark context?\n",
    "'''\n",
    "Key differences between SparkSession and SparkContext:\n",
    "SparkSession:\n",
    "\n",
    "High-level entry point for Spark functionality\n",
    "Introduced in Spark 2.0\n",
    "Combines SparkContext, SQLContext, and HiveContext\n",
    "Single point of entry for DataFrame and SQL operations\n",
    "Easier to use, more comprehensive API\n",
    "\n",
    "SparkContext:\n",
    "\n",
    "Low-level API for creating RDDs\n",
    "Core entry point for Spark core functionality\n",
    "Manages cluster resources and coordination\n",
    "Existed before SparkSession\n",
    "Still used internally by SparkSession\n",
    "\n",
    "SparkSession (preferred in modern Spark)\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# SparkContext (can be accessed from SparkSession)\n",
    "sc = spark.sparkContext\n",
    "'''\n",
    "\n",
    "#3. How do you handle backpressure in Spark Streaming applications to manage data load effectively?\n",
    "'''\n",
    "Handling backpressure in Spark Streaming involves several key strategies:\n",
    "\n",
    "Trigger-based Backpressure\n",
    "\n",
    "\n",
    "Configure maxOffsetsPerTrigger to limit data processed per batch\n",
    "Prevents overwhelming processing resources\n",
    "\n",
    "\n",
    "Rate Limiting\n",
    "\n",
    "\n",
    "Use spark.streaming.receiver.maxRate to control input rate\n",
    "Dynamically adjust receiver rate based on processing capabilities\n",
    "\n",
    "\n",
    "Adaptive Batch Processing\n",
    "\n",
    "\n",
    "Enable adaptive query execution (AQE)\n",
    "Automatically adjust partition sizes and execution plans\n",
    "\n",
    "\n",
    "Windowing and Micro-batching\n",
    "\n",
    "\n",
    "Use sliding windows to control data ingestion\n",
    "Break large data streams into manageable micro-batches\n",
    "\n",
    "Example configuration:\n",
    "pythonCopyspark_conf = SparkConf().set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "                        .set(\"spark.streaming.backpressure.initialRate\", 100)\n",
    "                        .set(\"spark.streaming.receiver.maxRate\", 200)\n",
    "\n",
    "ssc = StreamingContext(sc, batchDuration=1, conf=spark_conf)\n",
    "\n",
    "Custom Backpressure Handling\n",
    "\n",
    "Implement custom rate controllers\n",
    "Monitor processing time and dynamically adjust input rates\n",
    "\n",
    "Key goal: Balance data ingestion with processing capacity to prevent system overload.\n",
    "'''\n",
    "\n",
    "#4. What is delta lake and how does it improve data reliability and performance in Spark applications?\n",
    "'''\n",
    "Delta Lake is an open-source storage layer that enhances Spark data lakes with key improvements:\n",
    "\n",
    "ACID Transactions\n",
    "\n",
    "Ensures data integrity\n",
    "Supports concurrent writes and reads\n",
    "Atomic and consistent operations\n",
    "\n",
    "Time Travel & Versioning\n",
    "\n",
    "Allows querying historical data versions\n",
    "Enables easy rollbacks and data recovery\n",
    "Tracks complete change history\n",
    "\n",
    "Performance Optimizations\n",
    "\n",
    "Supports data compaction\n",
    "Provides efficient metadata management\n",
    "Enables file-level optimization\n",
    "\n",
    "Schema Evolution\n",
    "\n",
    "Allows dynamic schema updates\n",
    "Supports adding/removing columns seamlessly\n",
    "Maintains backward compatibility\n",
    "\n",
    "Writing Delta Table\n",
    "df.write.format(\"delta\").save(\"/path/to/delta_table\")\n",
    "\n",
    "# Reading with time travel\n",
    "spark.read.format(\"delta\") \\\n",
    "     .option(\"versionAsOf\", 1) \\\n",
    "     .load(\"/path/to/delta_table\")\n",
    "'''\n",
    "\n",
    "#5. What is difference between cache and persist in Spark?\n",
    "'''\n",
    "Key differences between cache() and persist() in Spark:\n",
    "cache():\n",
    "\n",
    "Default storage level: MEMORY_ONLY\n",
    "Stores RDD/DataFrame in memory\n",
    "Least configurable persistence option\n",
    "Equivalent to persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "persist():\n",
    "\n",
    "More flexible storage level configuration\n",
    "Options include:\n",
    "\n",
    "MEMORY_ONLY\n",
    "MEMORY_AND_DISK\n",
    "DISK_ONLY\n",
    "Off-heap storage\n",
    "\n",
    "Allows explicit memory/disk trade-offs\n",
    "\n",
    "Cache (memory only)\n",
    "df.cache()\n",
    "\n",
    "# Persist with custom storage level\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "Both methods help optimize performance by avoiding redundant computations, but persist() offers more granular control.\n",
    "'''\n",
    "\n",
    "#6. What is difference between repartition and coalesce in Spark?\n",
    "'''\n",
    "Key differences between repartition() and coalesce() in Spark:\n",
    "repartition():\n",
    "\n",
    "Full shuffle of entire dataset\n",
    "Can increase or decrease partition count\n",
    "More expensive operation\n",
    "Provides uniform data distribution\n",
    "\n",
    "coalesce():\n",
    "\n",
    "Minimizes data movement\n",
    "Reduces partition count efficiently\n",
    "Recommended for decreasing partitions\n",
    "No full data shuffle\n",
    "Preserves existing partitioning\n",
    "\n",
    "Repartition (full shuffle)\n",
    "df.repartition(10)\n",
    "\n",
    "# Coalesce (minimal data movement)\n",
    "df.coalesce(5)\n",
    "'''\n",
    "#7. What is difference between partitioning and bucketing in Spark SQL?\n",
    "'''\n",
    "Partitioning: Divides data into directories based on column values\n",
    "Bucketing: Distributes data into fixed number of buckets for optimization\n",
    "\n",
    "Comprehensive Example:\n",
    "Partitioning\n",
    "df.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"/path/partitioned_data\")\n",
    "\n",
    "# Bucketing\n",
    "df.write \\\n",
    "    .bucketBy(10, \"user_id\") \\\n",
    "    .sortBy(\"timestamp\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"/path/bucketed_data\")\n",
    "\n",
    "# Query Optimization\n",
    "# Partitioning: Fast for filtering partitioned columns\n",
    "# Bucketing: Efficient for joins and aggregations on high-cardinality columns\n",
    "'''\n",
    "\n",
    "#8. What is the difference between broadcast join and shuffle join in Spark?\n",
    "'''\n",
    "Broadcast Join vs Shuffle Join in Spark:\n",
    "Broadcast Join:\n",
    "\n",
    "Small table is sent to all executor nodes\n",
    "No data shuffling required\n",
    "Efficient for small dimension tables\n",
    "Lower network overhead\n",
    "Best when one table is significantly smaller\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast join small dimension table\n",
    "result = large_df.join(\n",
    "    broadcast(small_df), \n",
    "    \"join_key\"\n",
    ")\n",
    "Shuffle Join:\n",
    "\n",
    "Data shuffled across all partitions\n",
    "Both tables partitioned and redistributed\n",
    "High network and shuffle overhead\n",
    "Suitable for large tables\n",
    "Default join strategy\n",
    "\n",
    "Performance Comparison:\n",
    "\n",
    "Broadcast Join: O(small_table_size)\n",
    "Shuffle Join: O(large_table1_size * large_table2_size)\n",
    "\n",
    "Key Recommendation: Use broadcast join for small tables to optimize performance.\n",
    "'''\n",
    "\n",
    "#9. What is the difference between broadcast variable and accumulator in Spark?\n",
    "'''\n",
    "Broadcast Variable vs Accumulator in Spark:\n",
    "Broadcast Variable:\n",
    "\n",
    "Read-only shared variable across cluster\n",
    "Efficiently distribute large, immutable data\n",
    "Cached on each executor\n",
    "Used for lookup tables, reference data\n",
    "Low communication overhead\n",
    "\n",
    "Broadcast lookup dictionary\n",
    "lookup_dict = sc.broadcast({\n",
    "    'A': 'High', \n",
    "    'B': 'Medium', \n",
    "    'C': 'Low'\n",
    "})\n",
    "\n",
    "def transform_data(row):\n",
    "    return lookup_dict.value.get(row.category, 'Unknown')\n",
    "\n",
    "df.rdd.map(transform_data)\n",
    "Accumulator:\n",
    "\n",
    "Mutable shared variable for aggregation\n",
    "Supports distributed counter/sum operations\n",
    "Only driver can read final accumulated value\n",
    "Useful for debugging, tracking metrics\n",
    "Supports numeric and custom types\n",
    "\n",
    "Count specific conditions\n",
    "error_count = sc.accumulator(0)\n",
    "\n",
    "def process_data(row):\n",
    "    if row.has_error:\n",
    "        error_count.add(1)\n",
    "    return row\n",
    "\n",
    "df.rdd.map(process_data).collect()\n",
    "print(error_count.value)\n",
    "Key Difference: Broadcast for read-only data distribution, Accumulators for aggregating metrics.\n",
    "'''\n",
    "#10. Difference between narrow and wide ?\n",
    "'''\n",
    "Narrow transformations involve operations that can be done within a single partition\n",
    "(no shuffle).\n",
    "Examples: map(), filter(), union(), sample()\n",
    "Wide transformations require data to be shuffled between partitions, which can be more\n",
    "resource-intensive.\n",
    "Examples: groupBy(), reduceByKey(), join(), distinct()\n",
    "'''\n",
    "#11. What is Out of Memory?\n",
    "'''\n",
    "Out of memory (OOM) errors occur when Spark does not have enough memory to\n",
    "process the data being handled. This can be due to:\n",
    "• Too many partitions or large datasets in a single partition.\n",
    "• Inefficient operations causing memory pressure.\n",
    "• Insufficient heap size or executor memory allocation.\n",
    "Solutions include optimizing partitioning, adjusting memory settings (e.g.,\n",
    "spark.executor.memory), or using disk storage for spilling.\n",
    "Driver Out of memory\n",
    "Executor out of memory\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revision of Spark and SQL for Data Engineer Interview for Capgemini\n",
    "#1. what this does this command do in pyspark\n",
    "#sc.parallelize([1, 2, 3, 4, 5])\n",
    "'''\n",
    "sc.parallelize([1, 2, 3, 4, 5])\n",
    "does the following:\n",
    "\n",
    "Creates an RDD (Resilient Distributed Dataset): This converts the Python list [1, 2, 3, 4, 5] into a distributed dataset (RDD).\n",
    "Distributes the data across Spark's cluster: The data is split into partitions and distributed across available worker nodes.\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
    "The collect() method retrieves the entire dataset from the distributed RDD back to the driver as a Python list.\n",
    "'''\n",
    "\n",
    "#2. What are transformations and actions in Spark?\n",
    "'''\n",
    "\n",
    "Transformations and Actions in PySpark\n",
    "In PySpark, there are two types of operations on RDDs (Resilient Distributed Datasets):\n",
    "\n",
    "Transformations: These are lazy operations that create a new RDD from an existing one. They do not execute immediately; they are only computed when an action is performed.\n",
    "\n",
    "Examples: map(), filter(), flatMap()\n",
    "Actions: These trigger the execution of transformations and return results to the driver program.\n",
    "\n",
    "Examples: collect(), count(), take()\n",
    "'''\n",
    "\n",
    "#3. What is the difference between map() and flatMap() and filter() transformations in Spark?\n",
    "'''\n",
    "1. map()\n",
    "The map() function applies a function to each element in the RDD and returns a new RDD with the transformed elements.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TransformationsActions\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)  # Multiply each element by 2\n",
    "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
    "\n",
    "2. filter()\n",
    "The filter() function returns a new RDD containing only the elements that satisfy a given condition.\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers\n",
    "print(filtered_rdd.collect())  # Output: [2, 4]\n",
    "\n",
    "3. flatMap()\n",
    "The flatMap() function applies a function that returns an iterable and then flattens the result into a single RDD.\n",
    "rdd2 = sc.parallelize([\"hello world\", \"spark rdd\"])\n",
    "flat_mapped_rdd = rdd2.flatMap(lambda x: x.split(\" \"))  # Split each sentence into words\n",
    "print(flat_mapped_rdd.collect())  # Output: ['hello', 'world', 'spark', 'rdd']\n",
    "'''\n",
    "\n",
    "#4. What is difference between actions like collect(), count() and take() in Spark?\n",
    "'''\n",
    "1. collect()\n",
    "The collect() function retrieves all elements from the RDD and returns them as a list.\n",
    "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
    "⚠ Warning: Avoid using collect() on very large RDDs, as it can cause memory issues.\n",
    "\n",
    "2. count()\n",
    "The count() function returns the number of elements in the RDD.\n",
    "print(rdd.count())  # Output: 5\n",
    "\n",
    "3. take(n)\n",
    "The take(n) function retrieves the first n elements from the RDD.\n",
    "print(rdd.take(3))  # Output: [1, 2, 3]\n",
    "\n",
    "'''\n",
    "#5. explain with example in detail Lazy evaluation and DAG\n",
    "'''\n",
    "Lazy Evaluation and DAG in Spark\n",
    "Apache Spark follows a lazy evaluation model and processes computations using a Directed Acyclic Graph (DAG). Let’s break these concepts down in detail with an example.\n",
    "\n",
    "1. Lazy Evaluation\n",
    "Lazy evaluation means that Spark does not execute transformations immediately when they are called. Instead, it builds a logical execution plan and waits until an action (like .collect(), .show(), or .write()) is triggered. This approach optimizes query execution and reduces redundant computations.\n",
    "\n",
    "How Lazy Evaluation Works\n",
    "When you apply transformations (e.g., .filter(), .map(), .select()), Spark does not execute them immediately.\n",
    "Instead, it records them in a DAG (Directed Acyclic Graph).\n",
    "Once an action (e.g., .show(), .count(), .collect()) is triggered, Spark executes all transformations in an optimized way.\n",
    "\n",
    "2. DAG (Directed Acyclic Graph)\n",
    "A DAG is a logical representation of computations in Spark. It consists of:\n",
    "\n",
    "Vertices (Nodes) → Represent transformations like .filter(), .map(), etc.\n",
    "Edges (Lines between nodes) → Represent dependencies between transformations.\n",
    "'''\n",
    "#6. how to find the 2nd highest salary department-wise using both SQL (Oracle) and PySpark, using JOIN, CTE, and PARTITION BY.\n",
    "'''\n",
    "SQL (Oracle) Query:\n",
    "WITH RankedSalaries AS (\n",
    "    SELECT e.id, e.name, e.salary, e.dept_id, d.dept_name,\n",
    "           ROW_NUMBER() OVER (PARTITION BY e.dept_id ORDER BY e.salary DESC) AS rnk\n",
    "    FROM emp e\n",
    "    JOIN dept d ON e.dept_id = d.dept_id\n",
    ")\n",
    "SELECT id, name, salary, dept_id, dept_name\n",
    "FROM RankedSalaries\n",
    "WHERE rnk = 2;  -- 2nd highest salary per department\n",
    "\n",
    "PySpark Code:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.appName(\"SecondHighestSalary\").getOrCreate()\n",
    "\n",
    "# Employee Dataset\n",
    "emp_data = [\n",
    "    (1, \"Alice\", 5000, 1),\n",
    "    (2, \"Bob\", 7000, 1),\n",
    "    (3, \"Charlie\", 6000, 1),\n",
    "    (4, \"David\", 8000, 2),\n",
    "    (5, \"Eve\", 9000, 2),\n",
    "    (6, \"Frank\", 7500, 2),\n",
    "]\n",
    "\n",
    "dept_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"IT\"),\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "emp_df = spark.createDataFrame(emp_data, [\"id\", \"name\", \"salary\", \"dept_id\"])\n",
    "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "Using JOIN + PARTITION BY + ROW_NUMBER()\n",
    "# Define Window Specification\n",
    "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Rank salaries department-wise\n",
    "ranked_df = emp_df.withColumn(\"rnk\", row_number().over(window_spec))\n",
    "\n",
    "# Filter for 2nd highest salary\n",
    "second_highest_df = ranked_df.filter(col(\"rnk\") == 2)\n",
    "\n",
    "# Join with Department DataFrame to get dept_name\n",
    "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
    "\n",
    "# Show Result\n",
    "result_df.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lead, when\n",
    "##import os\n",
    "\n",
    "# Set the environment variables\n",
    "##python_path = r'C:\\Program Files\\Python313'\n",
    "\n",
    "# Set the environment variables\n",
    "#os.environ['PYSPARK_PYTHON'] = python_path\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# Set Java home explicitly\n",
    "#os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-11\\bin'\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"EmployeeSalaryProcessing\").getOrCreate()\n",
    "# Input Data\n",
    "data = [\n",
    "    (1, \"Radha\", 3000, 10),\n",
    "    (2, \"Kirshna\", 2000, 10),\n",
    "    (3, \"rama\", 1000, 10),\n",
    "    (1, \"Venkata\", 6000, 20),\n",
    "    (2, \"Laxmana\", 4000, 20),\n",
    "    (3, \"Laxmi\", 2000, 20)\n",
    "]\n",
    "\n",
    "# Schema\n",
    "columns = [\"empno\", \"name\", \"sal\", \"Deptno\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define Window Specification\n",
    "window_spec = Window.partitionBy(\"Deptno\").orderBy(col(\"sal\").desc())\n",
    "\n",
    "# Calculate next highest salary using lead function\n",
    "df = df.withColumn(\"nexthighest\", lead(\"sal\").over(window_spec))\n",
    "\n",
    "# Calculate salary difference\n",
    "df = df.withColumn(\"saldiff\", when(col(\"nexthighest\").isNull(), col(\"sal\"))\n",
    "                   .otherwise(col(\"sal\") - col(\"nexthighest\")))\n",
    "\n",
    "# Fill null nexthighest values with 0 (as per requirement)\n",
    "df = df.withColumn(\"nexthighest\", when(col(\"nexthighest\").isNull(), 0).otherwise(col(\"nexthighest\")))\n",
    "\n",
    "# Show Output\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to calculate running total of salary department-wise using PySpark\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = r'C:\\Program Files\\Python313\\python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\\Program Files\\Python313\\python.exe'\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 10, 1000),\n",
    "    (2, 10, 1500),\n",
    "    (3, 20, 1200),\n",
    "    (4, 10, 1800),\n",
    "    (5, 20, 1100)\n",
    "]\n",
    "columns = [\"empid\", \"deptid\", \"sal\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define Window\n",
    "window_spec = Window.partitionBy(\"deptid\").orderBy(\"sal\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# unboundedFollowing is also an option if you want to include all rows after the current row\n",
    "# Apply cumulative sum\n",
    "df_with_running = df.withColumn(\"running_sal\", _sum(\"sal\").over(window_spec))\n",
    "\n",
    "df_with_running.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preparing for a PySpark interview, especially for a role requiring over five years of experience, involves a deep understanding of both fundamental and advanced concepts. Below is a compilation of 15 verbal (non-programming) interview questions along with their answers to aid in your preparation:\n",
    "\n",
    "What is PySpark, and how does it differ from Apache Spark?\n",
    "Answer: PySpark is the Python API for Apache Spark, allowing developers to write Spark applications using Python. While Apache Spark is primarily written in Scala and offers APIs in Java, Scala, and R, PySpark provides the flexibility to harness Spark's capabilities using Python's rich ecosystem. ​\n",
    "\n",
    "Can you explain the architecture of a Spark application?\n",
    "Answer: A Spark application follows a master-slave architecture. The central component is the Driver Program, which manages the application's lifecycle, translates user code into tasks, and schedules them across the cluster. The Cluster Manager allocates resources, and the Worker Nodes host the Executors that run the tasks. Executors process data and store results, communicating back to the driver. ​\n",
    "\n",
    "What are Resilient Distributed Datasets (RDDs) in Spark?\n",
    "Answer: RDDs are the fundamental data structures in Spark, representing an immutable, distributed collection of objects partitioned across nodes in a cluster. They support fault tolerance through lineage information and allow parallel processing. RDDs can be created by loading data from storage or by transforming existing RDDs. ​\n",
    "\n",
    "How do DataFrames differ from RDDs in PySpark?\n",
    "Answer: DataFrames are higher-level abstractions built on top of RDDs, representing data in a tabular format with named columns, similar to tables in relational databases. They offer optimizations like Catalyst for query optimization and Tungsten for efficient execution, making them more efficient than RDDs for most workloads. DataFrames also provide richer APIs and support SQL queries. ​\n",
    "\n",
    "What is a SparkSession, and why is it important in PySpark?\n",
    "Answer: Introduced in Spark 2.0, a SparkSession is a unified entry point for reading data, processing it, and interacting with various Spark components like DataFrame and SQL APIs. It consolidates functionalities previously handled separately by SparkContext, SQLContext, and HiveContext, simplifying the user experience. ​\n",
    "\n",
    "Explain the concept of lazy evaluation in Spark.\n",
    "Answer: Lazy evaluation means that Spark defers the execution of transformations until an action is invoked. When transformations are called, they are not executed immediately but are instead recorded in a lineage graph. This approach allows Spark to optimize the overall data processing workflow before execution, enhancing efficiency. ​\n",
    "\n",
    "What are the different types of cluster managers available in Spark?\n",
    "Answer: Spark supports several cluster managers for resource allocation and job scheduling:​\n",
    "\n",
    "Standalone: A simple built-in cluster manager that comes with Spark.​\n",
    "Apache Mesos: A general-purpose cluster manager that can run various distributed applications.​\n",
    "Hadoop YARN: The resource manager for Hadoop clusters, allowing Spark to run alongside other Hadoop services.​\n",
    "Kubernetes: An open-source system for automating deployment, scaling, and management of containerized applications, which Spark can run on. ​\n",
    "\n",
    "What is the role of the Catalyst optimizer in Spark SQL?\n",
    "Answer: Catalyst is Spark SQL's query optimizer that leverages advanced programming language features to build an extensible query optimization framework. It performs logical and physical plan optimizations, such as constant folding, predicate pushdown, and join reordering, to enhance query performance. ​\n",
    "\n",
    "*Predicate pushdown: Filtering data at the source rather than after loading it into Spark, reducing data transfer and improving performance.\n",
    "*Constant folding: Evaluating constant expressions at compile time rather than runtime, optimizing query execution.\n",
    "\n",
    "How does Spark ensure fault tolerance?\n",
    "Answer: Spark ensures fault tolerance through RDD lineage and data checkpointing. Each RDD maintains a lineage graph that records the sequence of operations used to build it. If a partition is lost due to a node failure, Spark can recompute it using the lineage information. For long lineage chains, checkpointing can be used to save RDDs to stable storage, truncating the lineage. ​\n",
    "\n",
    "What is the difference between narrow and wide transformations in Spark?\n",
    "Answer: In Spark:\n",
    "Narrow Transformations: These are transformations where each input partition contributes to only one output partition. Examples include map, filter, and flatMap. They are pipelined and do not require data shuffling.\n",
    "Wide Transformations: These involve shuffling data between partitions, as each input partition can contribute to multiple output partitions. Examples include groupByKey, reduceByKey, and join. They require a shuffle phase, which can be more expensive.\n",
    "\n",
    "Describe how you would handle skewed data in a Spark job.\n",
    "Answer: Skewed data means the uneven distribution of data across the partitions. \n",
    "Handling skewed data involves strategies such as:\n",
    "Salting: Adding a random prefix to keys to distribute data more evenly across partitions.\n",
    "Broadcasting small datasets: Using broadcast joins to avoid shuffling large datasets.\n",
    "Increasing parallelism: Adjusting the number of partitions to better distribute the data.\n",
    "Custom partitioning: Implementing a custom partitioner to control data distribution explicitly.\n",
    "\n",
    "What are the benefits of using DataFrames over RDDs in PySpark?\n",
    "Answer: DataFrames offer several advantages over RDDs:\n",
    "Optimizations: They benefit from the Catalyst optimizer and Tungsten execution engine, leading to better performance.\n",
    "Ease of Use: DataFrames provide high-level APIs and support SQL queries, making data manipulation more straightforward.\n",
    "\n",
    "13. What is a Broadcast variable in PySpark?\n",
    "Answer:\n",
    "A Broadcast variable allows the caching of a large, read-only dataset in memory across all worker nodes, reducing the need for repeated data transfer. It is useful for scenarios like lookup tables in joins.\n",
    "\n",
    "14. What is an Accumulator in PySpark?\n",
    "Answer:\n",
    "Accumulators are variables used for aggregating information across multiple nodes, such as counters or sums. They are write-only for workers and readable only by the driver.\n",
    "\n",
    "15. Explain the difference between map() and flatMap().\n",
    "Answer:\n",
    "\n",
    "map(): Transforms each input element into exactly one output element.\n",
    "\n",
    "flatMap(): Can return multiple outputs per input, flattening the results.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "rdd = sc.parallelize([\"hello world\"])\n",
    "rdd.map(lambda x: x.split(\" \")).collect()  # [['hello', 'world']]\n",
    "rdd.flatMap(lambda x: x.split(\" \")).collect()  # ['hello', 'world']\n",
    "16. What are the different types of joins in PySpark?\n",
    "Answer:\n",
    "PySpark supports:\n",
    "\n",
    "Inner Join (df1.join(df2, \"id\", \"inner\"))\n",
    "\n",
    "Left Join (df1.join(df2, \"id\", \"left\"))\n",
    "\n",
    "Right Join (df1.join(df2, \"id\", \"right\"))\n",
    "\n",
    "Full Outer Join (df1.join(df2, \"id\", \"outer\"))\n",
    "\n",
    "Cross Join (df1.crossJoin(df2))\n",
    "\n",
    "17. How does PySpark handle missing (null) values?\n",
    "Answer:\n",
    "\n",
    "dropna(): Removes rows with null values.\n",
    "\n",
    "fillna(value): Replaces null values with a default.\n",
    "\n",
    "replace(): Substitutes specific values.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.fillna({'age': 25}).show()\n",
    "18. What is the difference between repartition() and coalesce()?\n",
    "Answer:\n",
    "\n",
    "repartition(n): Increases or decreases partitions, but may shuffle data.\n",
    "\n",
    "coalesce(n): Reduces partitions without full shuffle, improving performance.\n",
    "\n",
    "19. How does caching work in PySpark?\n",
    "Answer:\n",
    "cache() and persist() store DataFrames/RDDs in memory for reuse, reducing computation time.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.persist()  # Default is MEMORY_AND_DISK\n",
    "df.cache()  # Equivalent to persist(MEMORY_ONLY)\n",
    "20. Explain the concept of Checkpointing in PySpark.\n",
    "Answer:\n",
    "Checkpointing saves RDDs/DataFrames to storage, truncating lineage to prevent excessive memory usage.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "sc.setCheckpointDir(\"/checkpoint\")\n",
    "df.checkpoint()\n",
    "21. How do you optimize a PySpark job?\n",
    "Answer:\n",
    "\n",
    "Use DataFrames instead of RDDs.\n",
    "\n",
    "Utilize broadcast variables.\n",
    "\n",
    "Cache/Persist when needed.\n",
    "\n",
    "Optimize shuffle operations (e.g., reduceByKey() over groupByKey()).\n",
    "\n",
    "Tune parallelism.\n",
    "\n",
    "22. What is the default storage level of persist()?\n",
    "Answer:\n",
    "MEMORY_AND_DISK – Data is cached in memory, but spills to disk if necessary.\n",
    "\n",
    "23. How do you read and write data in PySpark?\n",
    "Answer:\n",
    "\n",
    "Read from CSV:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "Write to Parquet:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.write.parquet(\"output.parquet\")\n",
    "\n",
    "24. How do you handle skewed joins in PySpark?\n",
    "\n",
    "Answer: Skewness occurs due to ununiform distribution of data across partitions.\n",
    "Strategies to handle skewed joins include:\n",
    "\n",
    "Use broadcast joins (broadcast(df2)) if one dataset is small.\n",
    "\n",
    "Apply salting by adding a random key.\n",
    "\n",
    "Use skew hints (df.hint(\"skew\")).\n",
    "\n",
    "25. Explain Window Functions in PySpark.\n",
    "Answer:\n",
    "Window functions operate over partitions (e.g., ranking, cumulative sums).\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"rank\", rank().over(window_spec)).show()\n",
    "26. What is the difference between show(), collect(), and take()?\n",
    "Answer:\n",
    "\n",
    "show(n): Displays n rows (default 20).\n",
    "\n",
    "collect(): Retrieves all rows (use cautiously).\n",
    "\n",
    "take(n): Returns n rows as a list.\n",
    "\n",
    "27. What is the use of explode() in PySpark?\n",
    "Answer:\n",
    "explode() transforms an array or map column into multiple rows.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.select(explode(df.tags)).show()\n",
    "28. What is the difference between union() and unionAll()?\n",
    "Answer:\n",
    "\n",
    "union(): Works in PySpark contains duplicates (To removes duplicates use distinct after it).\n",
    "\n",
    "unionAll(): Deprecated, replaced by union().\n",
    "\n",
    "29. How do you convert a PySpark DataFrame to Pandas?\n",
    "Answer:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "pdf = df.toPandas()\n",
    "Be cautious with large datasets.\n",
    "\n",
    "30. How does PySpark support machine learning?\n",
    "Answer:\n",
    "PySpark MLlib provides scalable machine learning algorithms for classification, regression, and clustering.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "31. What is the difference between PySpark MLlib and ML?\n",
    "Answer:\n",
    "\n",
    "MLlib: RDD-based, older API.\n",
    "\n",
    "ML: DataFrame-based, newer and preferred.\n",
    "\n",
    "32. How do you execute SQL queries in PySpark?\n",
    "Answer:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "spark.sql(\"SELECT * FROM table_name\").show()\n",
    "33. What is Parquet, and why is it preferred?\n",
    "Answer:\n",
    "Parquet is a columnar storage format optimized for performance, compression, and schema evolution.\n",
    "\n",
    "33.1 What is orc and difference between parquet and orc? Explain both advantages and use cases of both?\n",
    "Answer: Orc is a columnar storage format optimized for Hadoop and Hive. It provides efficient compression and encoding schemes, making it suitable for big data processing. Orc stands for Optimized Row Columnar format.\n",
    "Parquet is a columnar storage format optimized for performance, compression, and schema evolution. It is widely used in the Hadoop ecosystem and is compatible with various data processing frameworks like Apache Spark, Hive, and Impala.\n",
    "Now advantages of orc over parquet are:\n",
    "1. Compression: Orc uses lightweight compression algorithms, making it more efficient for large datasets.\n",
    "2. Predicate Pushdown: Orc supports predicate pushdown, allowing queries to skip unnecessary data reads, improving performance.\n",
    "3. Hive Compatibility: Orc is optimized for Hive, making it a preferred choice for Hive-based applications.\n",
    "\n",
    "Cases where only use parquet and why:\n",
    "1. Spark Compatibility: Parquet is natively supported by Spark, making it a better choice for Spark-based applications.\n",
    "2. Schema Evolution: Parquet supports schema evolution, allowing you to add or remove columns without rewriting the entire dataset.\n",
    "3. Performance: Parquet is optimized for read-heavy workloads, making it ideal for analytical queries.\n",
    "\n",
    "34. How do you debug a PySpark application?\n",
    "Answer:\n",
    "\n",
    "Enable debug logging.\n",
    "\n",
    "Use explain() to check query plans.\n",
    "\n",
    "Run small-scale tests using limit().\n",
    "\n",
    "35. How do you monitor Spark jobs?\n",
    "Answer:\n",
    "\n",
    "Spark UI (http://localhost:4040).\n",
    "\n",
    "Spark event logs.\n",
    "\n",
    "Ganglia or Prometheus.\n",
    "\n",
    "36. How do you submit a PySpark job?\n",
    "Answer:\n",
    "\n",
    "sh\n",
    "Copy\n",
    "Edit\n",
    "spark-submit --master yarn my_script.py\n",
    "37. What is the difference between Spark Streaming and Structured Streaming?\n",
    "Answer:\n",
    "\n",
    "Spark Streaming: Uses DStreams (older API).\n",
    "\n",
    "Structured Streaming: Built on DataFrames, preferred for modern applications.\n",
    "\n",
    "38. What are Kafka and PySpark used together for?\n",
    "Answer:\n",
    "Kafka acts as a real-time data pipeline, and PySpark processes streaming data from Kafka topics.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = spark.readStream.format(\"kafka\").option(\"subscribe\", \"topic_name\").load()\n",
    "39. What is the use of the foreachBatch() function in Structured Streaming?\n",
    "Answer:\n",
    "It allows batch-wise processing of streaming data using DataFrame operations.\n",
    "\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "query = df.writeStream.foreachBatch(process_batch).start()\n",
    "40. Explain the difference between foreach() and foreachPartition().\n",
    "Answer:\n",
    "\n",
    "foreach(): Applies a function to each row.\n",
    "\n",
    "foreachPartition(): Applies a function to each partition, reducing overhead.\n",
    "\n",
    "41. How do you store results from a PySpark DataFrame to a database?\n",
    "Answer:\n",
    "Use JDBC:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.write.format(\"jdbc\").option(\"url\", \"jdbc:mysql://host/db\").save()\n",
    "42. What is Kryo serialization, and why is it useful?\n",
    "Answer:\n",
    "Kryo is a faster, optimized serialization format compared to Java serialization.\n",
    "\n",
    "Enable it:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "43. How do you define UDFs (User-Defined Functions) in PySpark?\n",
    "Answer:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(StringType())\n",
    "def custom_function(x):\n",
    "    return x.upper()\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLQsa7w6Pjw8"
      },
      "outputs": [],
      "source": [
        "# 1. Find the sales amount each year, product-wise  [Sales_data table]\n",
        "# Solution 1\n",
        "query='''WITH cte AS\n",
        "          (\n",
        "    SELECT product_id,\n",
        "           EXTRACT(YEAR FROM sales_date) AS sale_year,\n",
        "           SUM(amount) OVER (PARTITION BY product_id, EXTRACT(YEAR FROM sales_date)) AS cumulative_sum,\n",
        "           DENSE_RANK() OVER (PARTITION BY product_id ORDER BY EXTRACT(YEAR FROM sales_date)) AS rn\n",
        "    FROM Sales_data\n",
        ")\n",
        "SELECT product_id, sale_year, cumulative_sum\n",
        "FROM cte\n",
        "GROUP BY product_id, sale_year, cumulative_sum\n",
        "ORDER BY product_id, sale_year'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "543qcDvtPjw9",
        "outputId": "91e185c4-0de1-469c-e104-f42ca2c0afee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('P1', 2023, 1301.15)\n",
            "('P10', 2023, 1085.16)\n",
            "('P2', 2023, 1310.25)\n",
            "('P3', 2023, 1235.66)\n",
            "('P4', 2023, 1064.48)\n",
            "('P5', 2023, 1259.51)\n",
            "('P6', 2023, 1310.55)\n",
            "('P7', 2023, 1154.54)\n",
            "('P8', 2023, 1220.01)\n",
            "('P9', 2023, 1383.75)\n"
          ]
        }
      ],
      "source": [
        "# solution 2\n",
        "query='''SELECT product_id AS prod_id,\n",
        "    EXTRACT(YEAR FROM sales_date) AS sale_year,\n",
        "    SUM(amount) AS total_sales\n",
        "FROM Sales_data\n",
        "GROUP BY product_id, EXTRACT(YEAR FROM sales_date)\n",
        "ORDER BY product_id, sale_year'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n",
        "'''\n",
        "EXTRACT(YEAR FROM sale_dt): 2025\n",
        "TO_CHAR(sale_dt, 'YYYY'): '2025' (as a string)\n",
        "TRUNC(sale_dt, 'YYYY'): 01-JAN-2025 (the first day of the year)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-51mAlvVPjw9",
        "outputId": "ebc08ed7-112d-458b-d831-b5c02bae0732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('P6',)\n",
            "('P8',)\n",
            "('P5',)\n",
            "('P1',)\n",
            "('P2',)\n",
            "('P3',)\n",
            "('P9',)\n",
            "('P10',)\n",
            "('P4',)\n",
            "('P7',)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nAnalysis:\\nSuppose I have written this query\\nSELECT DISTINCT product_id \\nFROM Sales_data \\nWHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\\n\\nthis will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\\n\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2. Find products that were not sold in the last 7 months.\n",
        "query='''SELECT product_id\n",
        "FROM Sales_data\n",
        "WHERE product_id NOT IN (\n",
        "    SELECT DISTINCT product_id\n",
        "    FROM Sales_data\n",
        "    WHERE sales_date >= ADD_MONTHS(SYSDATE, -7)) group by product_id'''\n",
        "c.execute(query)\n",
        "for i in c:\n",
        "    print(i)\n",
        "\n",
        "'''\n",
        "Analysis:\n",
        "Suppose I have written this query\n",
        "SELECT DISTINCT product_id\n",
        "FROM Sales_data\n",
        "WHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\n",
        "\n",
        "this will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAd9bcCQPjw-"
      },
      "outputs": [],
      "source": [
        "#Berribot Interview Questions on ETL and pyspark for Wipro\n",
        "# Schedule AWS databrics spark job daily in midnight script\n",
        "'''\n",
        "To schedule an AWS Databricks Spark job to run daily at midnight, you can use Databricks Workflows (preferred) or an external scheduling tool like AWS Lambda or CloudWatch. Below is an example script for each approach:\n",
        "\n",
        "1. Using Databricks Workflows (Recommended)\n",
        "Create a Databricks Job:\n",
        "\n",
        "Go to Databricks Workspace.\n",
        "Navigate to Workflows > Jobs.\n",
        "Click Create Job.\n",
        "Add your notebook or Spark job details.\n",
        "Configure job clusters and libraries if required.\n",
        "Schedule the Job:\n",
        "\n",
        "In the Schedule tab, set the schedule to:\n",
        "Daily\n",
        "Start time: 00:00\n",
        "Save the workflow.\n",
        "2. Using AWS CloudWatch with Databricks REST API\n",
        "Prerequisites:\n",
        "A Databricks personal access token.\n",
        "Databricks job ID or notebook path.\n",
        "Python Script:\n",
        "\n",
        "import requests\n",
        "\n",
        "# Databricks Workspace URL\n",
        "DATABRICKS_HOST = \"https://<databricks-instance>.cloud.databricks.com\"\n",
        "\n",
        "# Databricks Personal Access Token\n",
        "DATABRICKS_TOKEN = \"<your-databricks-token>\"\n",
        "\n",
        "# Job ID\n",
        "JOB_ID = \"<your-job-id>\"\n",
        "\n",
        "# Trigger Job Function\n",
        "def trigger_job():\n",
        "    url = f\"{DATABRICKS_HOST}/api/2.1/jobs/run-now\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"job_id\": JOB_ID}\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Job triggered successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to trigger job: {response.content}\")\n",
        "\n",
        "# Run the function\n",
        "trigger_job()\n",
        "Schedule via AWS CloudWatch:\n",
        "Open AWS Management Console.\n",
        "Go to CloudWatch > Rules.\n",
        "Create a new rule:\n",
        "Event Source: Schedule.\n",
        "Schedule Expression: cron(0 0 * * ? *) (midnight daily).\n",
        "Target:\n",
        "Choose AWS Lambda.\n",
        "Add the Python script to your Lambda function.\n",
        "3. Using Crontab on EC2\n",
        "If you prefer running the script from an EC2 instance:\n",
        "\n",
        "SSH into the EC2 instance.\n",
        "Edit the crontab:\n",
        "\n",
        "crontab -e\n",
        "Add the following entry to trigger the script at midnight:\n",
        "\n",
        "0 0 * * * python3 /path/to/trigger_databricks_job.py\n",
        "Which approach do you prefer? I can guide you further.'''\n",
        "\n",
        "\n",
        "#Write AWS databrics notebook to calculate rolling average over time periods for q time series data set.\n",
        "\n",
        "#Input : time period '7 days' dataset: 'stock_prices''\n",
        "\n",
        "#Output: dataframe with 7-day rolling averages\n",
        "\n",
        "'''Here is a Databricks notebook code that calculates the 7-day rolling average for a time series dataset (stock_prices) using PySpark.\n",
        "\n",
        "Code for the Databricks Notebook\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, window\n",
        "\n",
        "# Initialize SparkSession (this is usually already set up in Databricks)\n",
        "spark = SparkSession.builder.appName(\"RollingAverage\").getOrCreate()\n",
        "\n",
        "# Sample dataset: Replace this with your actual dataset\n",
        "data = [\n",
        "    (\"2025-01-01\", 100),\n",
        "    (\"2025-01-02\", 110),\n",
        "    (\"2025-01-03\", 105),\n",
        "    (\"2025-01-04\", 120),\n",
        "    (\"2025-01-05\", 130),\n",
        "    (\"2025-01-06\", 125),\n",
        "    (\"2025-01-07\", 140),\n",
        "    (\"2025-01-08\", 145),\n",
        "    (\"2025-01-09\", 150),\n",
        "    (\"2025-01-10\", 155)\n",
        "]\n",
        "\n",
        "# Define schema and create a DataFrame\n",
        "columns = [\"date\", \"price\"]\n",
        "stock_prices_df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Convert 'date' column to TimestampType\n",
        "from pyspark.sql.functions import to_date\n",
        "stock_prices_df = stock_prices_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Calculate 7-day rolling average\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Define window specification\n",
        "rolling_window = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "result_df = stock_prices_df.withColumn(\"7_day_avg\", avg(\"price\").over(rolling_window))\n",
        "\n",
        "# Show the results\n",
        "result_df.show()\n",
        "Explanation of the Code\n",
        "Dataset Initialization:\n",
        "\n",
        "Replace the data variable with your actual dataset from stock_prices.\n",
        "Date Conversion:\n",
        "\n",
        "Convert the date column to DateType for proper time series handling.\n",
        "Define Window Specification:\n",
        "\n",
        "Use Window.orderBy(\"date\").rowsBetween(-6, 0) to create a rolling window of 7 days (current row and 6 previous rows).\n",
        "Calculate Rolling Average:\n",
        "\n",
        "Use the avg function to compute the 7-day rolling average.\n",
        "Result:\n",
        "\n",
        "A new column 7_day_avg is added to the DataFrame showing the rolling average.\n",
        "Sample Output\n",
        "date\tprice\t7_day_avg\n",
        "2025-01-01\t100\t100.0\n",
        "2025-01-02\t110\t105.0\n",
        "2025-01-03\t105\t105.0\n",
        "2025-01-04\t120\t108.75\n",
        "2025-01-05\t130\t113.0\n",
        "2025-01-06\t125\t115.0\n",
        "2025-01-07\t140\t118.57\n",
        "2025-01-08\t145\t125.0\n",
        "2025-01-09\t150\t130.0\n",
        "2025-01-10\t155\t135.71\n",
        "Let me know if you'd like help loading the actual dataset from S3 or a database!'''\n",
        "\n",
        "\n",
        "#Partition a large dataframe in databrics by a given column and write to different parquet files.\n",
        "\n",
        "#Input: column: 'region'\n",
        "#Output: Partitioned parquet files by 'region'\n",
        "\n",
        "#Eg 2 Input: column: 'category'\n",
        "#Output: Partioned Parquet files by 'category'\n",
        "\n",
        "'''\n",
        "Here’s how to partition a large DataFrame in Databricks by a given column (e.g., region or category) and write it to different Parquet files:\n",
        "\n",
        "Code for Partitioning and Writing to Parquet\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession (this is usually pre-configured in Databricks)\n",
        "spark = SparkSession.builder.appName(\"PartitionedParquet\").getOrCreate()\n",
        "\n",
        "# Sample dataset: Replace this with your actual dataset\n",
        "data = [\n",
        "    (\"East\", \"Electronics\", 100),\n",
        "    (\"West\", \"Furniture\", 200),\n",
        "    (\"East\", \"Clothing\", 150),\n",
        "    (\"North\", \"Electronics\", 300),\n",
        "    (\"South\", \"Clothing\", 250),\n",
        "    (\"West\", \"Electronics\", 400),\n",
        "    (\"East\", \"Furniture\", 350)\n",
        "]\n",
        "\n",
        "# Define schema and create a DataFrame\n",
        "columns = [\"region\", \"category\", \"sales\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "(((((((\n",
        "We can create separate explicit schema\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define schema explicitly\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "\n",
        "# Apply the schema\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.printSchema()\n",
        ")))))))\n",
        "\n",
        "# Partition column: Replace 'region' with 'category' as needed\n",
        "partition_column = \"region\"  # Change to \"category\" for the second example\n",
        "\n",
        "# Output path for Parquet files (e.g., S3, DBFS)\n",
        "output_path = \"dbfs:/mnt/output/partitioned_data\"  # Change as required\n",
        "\n",
        "# Write DataFrame to partitioned Parquet files\n",
        "df.write.partitionBy(partition_column).parquet(output_path)\n",
        "\n",
        "print(f\"Data written to Parquet files partitioned by {partition_column}\")\n",
        "Explanation\n",
        "Input Dataset:\n",
        "\n",
        "Replace the sample data with your actual DataFrame from a source like S3, Delta Lake, or a SQL database.\n",
        "Partitioning Column:\n",
        "\n",
        "Set the partition_column to the column by which you want to partition the Parquet files (e.g., region or category).\n",
        "Output Path:\n",
        "\n",
        "Specify the output_path where the partitioned Parquet files should be written. This could be:\n",
        "dbfs:/mnt/output/... for Databricks File System.\n",
        "s3://bucket-name/... for AWS S3.\n",
        "Partition and Write:\n",
        "\n",
        "Use the partitionBy() method to group data by the specified column, creating a folder for each unique value in the column.\n",
        "Parquet files for each partition are written inside the corresponding folder.\n",
        "Output Directory Structure\n",
        "If partition_column = 'region', the Parquet files are saved as:\n",
        "\n",
        "\n",
        "/mnt/output/partitioned_data/\n",
        "├── region=East/\n",
        "│   ├── part-00000.parquet\n",
        "│   └── part-00001.parquet\n",
        "├── region=West/\n",
        "│   ├── part-00000.parquet\n",
        "│   └── part-00001.parquet\n",
        "├── region=North/\n",
        "│   ├── part-00000.parquet\n",
        "├── region=South/\n",
        "│   ├── part-00000.parquet\n",
        "For partition_column = 'category', the folder names will follow category=Electronics, category=Furniture, etc.\n",
        "\n",
        "Verify Written Data\n",
        "To verify the Parquet files:\n",
        "\n",
        "\n",
        "# Load the partitioned Parquet files\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "\n",
        "# Show data to verify\n",
        "partitioned_df.show()\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJB9q9-oPjw_"
      },
      "outputs": [],
      "source": [
        "#You are a data engineer explain this project in detail use Pyspark and python pandas and no SQL whereever required. And also focused on horizon insight database oracle\n",
        "'''\n",
        "I'll help explain this Horizon Insights data engineering project and provide implementation suggestions using PySpark and Pandas. Let me break down the architecture and provide some code examples.\n",
        "\n",
        "The project appears to be a comprehensive data platform serving since 2017 with impressive metrics: 800+ regular users, 5 queries/second average throughput, and 18TB+ of accumulated data. Let me break this down into key components:\n",
        "\n",
        "1. Data Sources & Ingestion:\n",
        "Let's handle the various data sources using PySpark:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HorizonInsights\") \\\n",
        "    .config(\"spark.jars\", \"oracle-jdbc.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Function to read from various sources\n",
        "def ingest_service_data(source_type, path):\n",
        "    if source_type == \"jenkins\":\n",
        "        # Read Jenkins JSON data\n",
        "        df = spark.read.json(path)\n",
        "    elif source_type == \"jira\":\n",
        "        # Read Jira REST API data\n",
        "        df = spark.read.json(path)\n",
        "    elif source_type == \"qtest\":\n",
        "        # Read qTest data\n",
        "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
        "    return df\n",
        "```\n",
        "\n",
        "2. Data Transformation & Loading:\n",
        "Given the metrics (140K+ stories/month, 18K+ bugs/month), we need robust transformation:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "def transform_metrics_data(df, metric_type):\n",
        "    if metric_type == \"stories\":\n",
        "        # Transform stories data\n",
        "        df = df.withColumn(\"month\", month(col(\"created_date\"))) \\\n",
        "               .withColumn(\"year\", year(col(\"created_date\"))) \\\n",
        "               .groupBy(\"month\", \"year\") \\\n",
        "               .agg(count(\"story_id\").alias(\"story_count\"))\n",
        "\n",
        "    elif metric_type == \"jenkins_jobs\":\n",
        "        # Transform Jenkins jobs data\n",
        "        df = df.withColumn(\"status\", when(col(\"result\") == \"SUCCESS\", 1).otherwise(0)) \\\n",
        "               .groupBy(\"job_name\") \\\n",
        "               .agg(\n",
        "                   sum(\"status\").alias(\"successful_builds\"),\n",
        "                   count(\"*\").alias(\"total_builds\")\n",
        "               )\n",
        "    return df\n",
        "```\n",
        "\n",
        "3. Oracle Database Integration:\n",
        "For the Horizon Insights DB, which appears to be Oracle-based:\n",
        "\n",
        "```python\n",
        "def write_to_oracle(df, table_name):\n",
        "    # Oracle connection properties\n",
        "    oracle_properties = {\n",
        "        \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
        "        \"url\": \"jdbc:oracle:thin:@//hostname:port/service\",\n",
        "        \"user\": \"username\",\n",
        "        \"password\": \"password\"\n",
        "    }\n",
        "\n",
        "    # Write to Oracle\n",
        "    df.write \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", oracle_properties[\"url\"]) \\\n",
        "        .option(\"dbtable\", table_name) \\\n",
        "        .option(\"user\", oracle_properties[\"user\"]) \\\n",
        "        .option(\"password\", oracle_properties[\"password\"]) \\\n",
        "        .mode(\"append\") \\\n",
        "        .save()\n",
        "```\n",
        "\n",
        "4. Reporting Pipeline:\n",
        "For the reporting requirements (MetricsHQ, SDLC, Compliance):\n",
        "\n",
        "```python\n",
        "def generate_metrics_report():\n",
        "    # Create a reporting dataframe\n",
        "    metrics_df = pd.DataFrame()\n",
        "\n",
        "    # Get Jenkins metrics\n",
        "    jenkins_metrics = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            DATE_TRUNC('month', timestamp) as month,\n",
        "            COUNT(*) as total_jobs,\n",
        "            SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_jobs\n",
        "        FROM jenkins_jobs\n",
        "        GROUP BY DATE_TRUNC('month', timestamp)\n",
        "    \"\"\")\n",
        "\n",
        "    # Convert to pandas for reporting\n",
        "    metrics_df['jenkins_success_rate'] = jenkins_metrics.toPandas()['successful_jobs'] / \\\n",
        "                                       jenkins_metrics.toPandas()['total_jobs']\n",
        "\n",
        "    return metrics_df\n",
        "```\n",
        "\n",
        "Key Features to Note:\n",
        "1. Volume Handling:\n",
        "- Processing 470K+ Jenkins jobs/month\n",
        "- Managing 350K+ Jobs/Month from Tower\n",
        "- Handling 300K+ qTest runs/month\n",
        "\n",
        "2. Performance Optimization:\n",
        "```python\n",
        "# Configure Spark for optimal performance\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
        "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
        "```\n",
        "\n",
        "3. Data Quality Checks:\n",
        "```python\n",
        "def data_quality_check(df, table_name):\n",
        "    # Check for nulls\n",
        "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "    # Check for duplicates\n",
        "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
        "\n",
        "    # Log quality metrics\n",
        "    print(f\"Quality checks for {table_name}\")\n",
        "    null_counts.show()\n",
        "    print(f\"Duplicate records: {duplicate_count}\")\n",
        "```\n",
        "\n",
        "The system seems to be processing a significant amount of DevOps and testing data, with various downstream consumers like Jira, Release Manager, and TechABS. The architecture uses SSIS for data ingestion, suggesting this is likely an enterprise-level solution.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSV61qUzPjxA"
      },
      "outputs": [],
      "source": [
        "# Thoughtfocus Interview Questions on Spark  for Data Engineer\n",
        "\n",
        "#1. How to handle skewed data in spark job to prevent performance issues?\n",
        "'''\n",
        "To handle skewed data in Spark jobs and prevent performance issues, consider these strategies:\n",
        "\n",
        "Data Repartitioning\n",
        "\n",
        "Use repartition() or coalesce() to redistribute data more evenly\n",
        "Specify a custom partitioning strategy based on key distribution\n",
        "Salting Technique\n",
        "Add a random salt to skewed keys before aggregation\n",
        "Helps break up data hotspots and distribute processing load\n",
        "\n",
        "Broadcast Join for Small Datasets\n",
        "Use broadcast() for smaller datasets to reduce shuffle overhead\n",
        "Prevents skew in join operations with significantly different-sized tables\n",
        "Adaptive Query Execution\n",
        "Enable Spark's adaptive query execution (AQE)\n",
        "Dynamically coalesces and optimizes shuffle partitions at runtime\n",
        "\n",
        "Skew Hint in Spark SQL\n",
        "Use SKEW HINT to provide optimizer hints about data distribution\n",
        "Helps Spark manage skewed data more intelligently during joins\n",
        "Example of salting technique:\n",
        "\n",
        "def salt_key(key):\n",
        "    return f\"{key}_{random.randint(0, num_salt_buckets)}\"\n",
        "\n",
        "df = df.withColumn(\"salted_key\", salt_key(df.original_key))\n",
        "These approaches can significantly improve Spark job performance when dealing with unevenly distributed data.\n",
        "'''\n",
        "\n",
        "#1. What is diff bw spark session and spark context?\n",
        "'''\n",
        "Key differences between SparkSession and SparkContext:\n",
        "SparkSession:\n",
        "\n",
        "High-level entry point for Spark functionality\n",
        "Introduced in Spark 2.0\n",
        "Combines SparkContext, SQLContext, and HiveContext\n",
        "Single point of entry for DataFrame and SQL operations\n",
        "Easier to use, more comprehensive API\n",
        "\n",
        "SparkContext:\n",
        "\n",
        "Low-level API for creating RDDs\n",
        "Core entry point for Spark core functionality\n",
        "Manages cluster resources and coordination\n",
        "Existed before SparkSession\n",
        "Still used internally by SparkSession\n",
        "\n",
        "SparkSession (preferred in modern Spark)\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "# SparkContext (can be accessed from SparkSession)\n",
        "sc = spark.sparkContext\n",
        "'''\n",
        "\n",
        "#3. How do you handle backpressure in Spark Streaming applications to manage data load effectively?\n",
        "'''\n",
        "Handling backpressure in Spark Streaming involves several key strategies:\n",
        "\n",
        "Trigger-based Backpressure\n",
        "\n",
        "\n",
        "Configure maxOffsetsPerTrigger to limit data processed per batch\n",
        "Prevents overwhelming processing resources\n",
        "\n",
        "\n",
        "Rate Limiting\n",
        "\n",
        "\n",
        "Use spark.streaming.receiver.maxRate to control input rate\n",
        "Dynamically adjust receiver rate based on processing capabilities\n",
        "\n",
        "\n",
        "Adaptive Batch Processing\n",
        "\n",
        "\n",
        "Enable adaptive query execution (AQE)\n",
        "Automatically adjust partition sizes and execution plans\n",
        "\n",
        "\n",
        "Windowing and Micro-batching\n",
        "\n",
        "\n",
        "Use sliding windows to control data ingestion\n",
        "Break large data streams into manageable micro-batches\n",
        "\n",
        "Example configuration:\n",
        "pythonCopyspark_conf = SparkConf().set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
        "                        .set(\"spark.streaming.backpressure.initialRate\", 100)\n",
        "                        .set(\"spark.streaming.receiver.maxRate\", 200)\n",
        "\n",
        "ssc = StreamingContext(sc, batchDuration=1, conf=spark_conf)\n",
        "\n",
        "Custom Backpressure Handling\n",
        "\n",
        "Implement custom rate controllers\n",
        "Monitor processing time and dynamically adjust input rates\n",
        "\n",
        "Key goal: Balance data ingestion with processing capacity to prevent system overload.\n",
        "'''\n",
        "\n",
        "#4. What is delta lake and how does it improve data reliability and performance in Spark applications?\n",
        "'''\n",
        "Delta Lake is an open-source storage layer that enhances Spark data lakes with key improvements:\n",
        "\n",
        "ACID Transactions\n",
        "\n",
        "Ensures data integrity\n",
        "Supports concurrent writes and reads\n",
        "Atomic and consistent operations\n",
        "\n",
        "Time Travel & Versioning\n",
        "\n",
        "Allows querying historical data versions\n",
        "Enables easy rollbacks and data recovery\n",
        "Tracks complete change history\n",
        "\n",
        "Performance Optimizations\n",
        "\n",
        "Supports data compaction\n",
        "Provides efficient metadata management\n",
        "Enables file-level optimization\n",
        "\n",
        "Schema Evolution\n",
        "\n",
        "Allows dynamic schema updates\n",
        "Supports adding/removing columns seamlessly\n",
        "Maintains backward compatibility\n",
        "\n",
        "Writing Delta Table\n",
        "df.write.format(\"delta\").save(\"/path/to/delta_table\")\n",
        "\n",
        "# Reading with time travel\n",
        "spark.read.format(\"delta\") \\\n",
        "     .option(\"versionAsOf\", 1) \\\n",
        "     .load(\"/path/to/delta_table\")\n",
        "'''\n",
        "\n",
        "#5. What is difference between cache and persist in Spark?\n",
        "'''\n",
        "Key differences between cache() and persist() in Spark:\n",
        "cache():\n",
        "\n",
        "Default storage level: MEMORY_ONLY\n",
        "Stores RDD/DataFrame in memory\n",
        "Least configurable persistence option\n",
        "Equivalent to persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "persist():\n",
        "\n",
        "More flexible storage level configuration\n",
        "Options include:\n",
        "\n",
        "MEMORY_ONLY\n",
        "MEMORY_AND_DISK\n",
        "DISK_ONLY\n",
        "Off-heap storage\n",
        "\n",
        "Allows explicit memory/disk trade-offs\n",
        "\n",
        "Cache (memory only)\n",
        "df.cache()\n",
        "\n",
        "# Persist with custom storage level\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "Both methods help optimize performance by avoiding redundant computations, but persist() offers more granular control.\n",
        "'''\n",
        "\n",
        "#6. What is difference between repartition and coalesce in Spark?\n",
        "'''\n",
        "Key differences between repartition() and coalesce() in Spark:\n",
        "repartition():\n",
        "\n",
        "Full shuffle of entire dataset\n",
        "Can increase or decrease partition count\n",
        "More expensive operation\n",
        "Provides uniform data distribution\n",
        "\n",
        "coalesce():\n",
        "\n",
        "Minimizes data movement\n",
        "Reduces partition count efficiently\n",
        "Recommended for decreasing partitions\n",
        "No full data shuffle\n",
        "Preserves existing partitioning\n",
        "\n",
        "Repartition (full shuffle)\n",
        "df.repartition(10)\n",
        "\n",
        "# Coalesce (minimal data movement)\n",
        "df.coalesce(5)\n",
        "'''\n",
        "#7. What is difference between partitioning and bucketing in Spark SQL?\n",
        "'''\n",
        "Partitioning: Divides data into directories based on column values\n",
        "Bucketing: Distributes data into fixed number of buckets for optimization\n",
        "\n",
        "Comprehensive Example:\n",
        "Partitioning\n",
        "df.write \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .save(\"/path/partitioned_data\")\n",
        "\n",
        "# Bucketing\n",
        "df.write \\\n",
        "    .bucketBy(10, \"user_id\") \\\n",
        "    .sortBy(\"timestamp\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .save(\"/path/bucketed_data\")\n",
        "\n",
        "# Query Optimization\n",
        "# Partitioning: Fast for filtering partitioned columns\n",
        "# Bucketing: Efficient for joins and aggregations on high-cardinality columns\n",
        "'''\n",
        "\n",
        "#8. What is the difference between broadcast join and shuffle join in Spark?\n",
        "'''\n",
        "Broadcast Join vs Shuffle Join in Spark:\n",
        "Broadcast Join:\n",
        "\n",
        "Small table is sent to all executor nodes\n",
        "No data shuffling required\n",
        "Efficient for small dimension tables\n",
        "Lower network overhead\n",
        "Best when one table is significantly smaller\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Broadcast join small dimension table\n",
        "result = large_df.join(\n",
        "    broadcast(small_df),\n",
        "    \"join_key\"\n",
        ")\n",
        "Shuffle Join:\n",
        "\n",
        "Data shuffled across all partitions\n",
        "Both tables partitioned and redistributed\n",
        "High network and shuffle overhead\n",
        "Suitable for large tables\n",
        "Default join strategy\n",
        "\n",
        "Performance Comparison:\n",
        "\n",
        "Broadcast Join: O(small_table_size)\n",
        "Shuffle Join: O(large_table1_size * large_table2_size)\n",
        "\n",
        "Key Recommendation: Use broadcast join for small tables to optimize performance.\n",
        "'''\n",
        "\n",
        "#9. What is the difference between broadcast variable and accumulator in Spark?\n",
        "'''\n",
        "Broadcast Variable vs Accumulator in Spark:\n",
        "Broadcast Variable:\n",
        "\n",
        "Read-only shared variable across cluster\n",
        "Efficiently distribute large, immutable data\n",
        "Cached on each executor\n",
        "Used for lookup tables, reference data\n",
        "Low communication overhead\n",
        "\n",
        "Broadcast lookup dictionary\n",
        "lookup_dict = sc.broadcast({\n",
        "    'A': 'High',\n",
        "    'B': 'Medium',\n",
        "    'C': 'Low'\n",
        "})\n",
        "\n",
        "def transform_data(row):\n",
        "    return lookup_dict.value.get(row.category, 'Unknown')\n",
        "\n",
        "df.rdd.map(transform_data)\n",
        "Accumulator:\n",
        "\n",
        "Mutable shared variable for aggregation\n",
        "Supports distributed counter/sum operations\n",
        "Only driver can read final accumulated value\n",
        "Useful for debugging, tracking metrics\n",
        "Supports numeric and custom types\n",
        "\n",
        "Count specific conditions\n",
        "error_count = sc.accumulator(0)\n",
        "\n",
        "def process_data(row):\n",
        "    if row.has_error:\n",
        "        error_count.add(1)\n",
        "    return row\n",
        "\n",
        "df.rdd.map(process_data).collect()\n",
        "print(error_count.value)\n",
        "Key Difference: Broadcast for read-only data distribution, Accumulators for aggregating metrics.\n",
        "'''\n",
        "#10. Difference between narrow and wide ?\n",
        "'''\n",
        "Narrow transformations involve operations that can be done within a single partition\n",
        "(no shuffle).\n",
        "Examples: map(), filter(), union(), sample()\n",
        "Wide transformations require data to be shuffled between partitions, which can be more\n",
        "resource-intensive.\n",
        "Examples: groupBy(), reduceByKey(), join(), distinct()\n",
        "'''\n",
        "#11. What is Out of Memory?\n",
        "'''\n",
        "Out of memory (OOM) errors occur when Spark does not have enough memory to\n",
        "process the data being handled. This can be due to:\n",
        "• Too many partitions or large datasets in a single partition.\n",
        "• Inefficient operations causing memory pressure.\n",
        "• Insufficient heap size or executor memory allocation.\n",
        "Solutions include optimizing partitioning, adjusting memory settings (e.g.,\n",
        "spark.executor.memory), or using disk storage for spilling.\n",
        "Driver Out of memory\n",
        "Executor out of memory\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kpa5OqZPjxA"
      },
      "outputs": [],
      "source": [
        "# Revision of Spark and SQL for Data Engineer Interview for Capgemini\n",
        "#1. what this does this command do in pyspark\n",
        "#sc.parallelize([1, 2, 3, 4, 5])\n",
        "'''\n",
        "sc.parallelize([1, 2, 3, 4, 5])\n",
        "does the following:\n",
        "\n",
        "Creates an RDD (Resilient Distributed Dataset): This converts the Python list [1, 2, 3, 4, 5] into a distributed dataset (RDD).\n",
        "Distributes the data across Spark's cluster: The data is split into partitions and distributed across available worker nodes.\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
        "The collect() method retrieves the entire dataset from the distributed RDD back to the driver as a Python list.\n",
        "'''\n",
        "\n",
        "#2. What are transformations and actions in Spark?\n",
        "'''\n",
        "\n",
        "Transformations and Actions in PySpark\n",
        "In PySpark, there are two types of operations on RDDs (Resilient Distributed Datasets):\n",
        "\n",
        "Transformations: These are lazy operations that create a new RDD from an existing one. They do not execute immediately; they are only computed when an action is performed.\n",
        "\n",
        "Examples: map(), filter(), flatMap()\n",
        "Actions: These trigger the execution of transformations and return results to the driver program.\n",
        "\n",
        "Examples: collect(), count(), take()\n",
        "'''\n",
        "\n",
        "#3. What is the difference between map() and flatMap() and filter() transformations in Spark?\n",
        "'''\n",
        "1. map()\n",
        "The map() function applies a function to each element in the RDD and returns a new RDD with the transformed elements.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"TransformationsActions\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)  # Multiply each element by 2\n",
        "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
        "\n",
        "2. filter()\n",
        "The filter() function returns a new RDD containing only the elements that satisfy a given condition.\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers\n",
        "print(filtered_rdd.collect())  # Output: [2, 4]\n",
        "\n",
        "3. flatMap()\n",
        "The flatMap() function applies a function that returns an iterable and then flattens the result into a single RDD.\n",
        "rdd2 = sc.parallelize([\"hello world\", \"spark rdd\"])\n",
        "flat_mapped_rdd = rdd2.flatMap(lambda x: x.split(\" \"))  # Split each sentence into words\n",
        "print(flat_mapped_rdd.collect())  # Output: ['hello', 'world', 'spark', 'rdd']\n",
        "'''\n",
        "\n",
        "#4. What is difference between actions like collect(), count() and take() in Spark?\n",
        "'''\n",
        "1. collect()\n",
        "The collect() function retrieves all elements from the RDD and returns them as a list.\n",
        "print(rdd.collect())  # Output: [1, 2, 3, 4, 5]\n",
        "⚠ Warning: Avoid using collect() on very large RDDs, as it can cause memory issues.\n",
        "\n",
        "2. count()\n",
        "The count() function returns the number of elements in the RDD.\n",
        "print(rdd.count())  # Output: 5\n",
        "\n",
        "3. take(n)\n",
        "The take(n) function retrieves the first n elements from the RDD.\n",
        "print(rdd.take(3))  # Output: [1, 2, 3]\n",
        "\n",
        "'''\n",
        "#5. explain with example in detail Lazy evaluation and DAG\n",
        "'''\n",
        "Lazy Evaluation and DAG in Spark\n",
        "Apache Spark follows a lazy evaluation model and processes computations using a Directed Acyclic Graph (DAG). Let’s break these concepts down in detail with an example.\n",
        "\n",
        "1. Lazy Evaluation\n",
        "Lazy evaluation means that Spark does not execute transformations immediately when they are called. Instead, it builds a logical execution plan and waits until an action (like .collect(), .show(), or .write()) is triggered. This approach optimizes query execution and reduces redundant computations.\n",
        "\n",
        "How Lazy Evaluation Works\n",
        "When you apply transformations (e.g., .filter(), .map(), .select()), Spark does not execute them immediately.\n",
        "Instead, it records them in a DAG (Directed Acyclic Graph).\n",
        "Once an action (e.g., .show(), .count(), .collect()) is triggered, Spark executes all transformations in an optimized way.\n",
        "\n",
        "2. DAG (Directed Acyclic Graph)\n",
        "A DAG is a logical representation of computations in Spark. It consists of:\n",
        "\n",
        "Vertices (Nodes) → Represent transformations like .filter(), .map(), etc.\n",
        "Edges (Lines between nodes) → Represent dependencies between transformations.\n",
        "'''\n",
        "#6. how to find the 2nd highest salary department-wise using both SQL (Oracle) and PySpark, using JOIN, CTE, and PARTITION BY.\n",
        "'''\n",
        "SQL (Oracle) Query:\n",
        "WITH RankedSalaries AS (\n",
        "    SELECT e.id, e.name, e.salary, e.dept_id, d.dept_name,\n",
        "           ROW_NUMBER() OVER (PARTITION BY e.dept_id ORDER BY e.salary DESC) AS rnk\n",
        "    FROM emp e\n",
        "    JOIN dept d ON e.dept_id = d.dept_id\n",
        ")\n",
        "SELECT id, name, salary, dept_id, dept_name\n",
        "FROM RankedSalaries\n",
        "WHERE rnk = 2;  -- 2nd highest salary per department\n",
        "\n",
        "PySpark Code:\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Spark Session\n",
        "spark = SparkSession.builder.appName(\"SecondHighestSalary\").getOrCreate()\n",
        "\n",
        "# Employee Dataset\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 5000, 1),\n",
        "    (2, \"Bob\", 7000, 1),\n",
        "    (3, \"Charlie\", 6000, 1),\n",
        "    (4, \"David\", 8000, 2),\n",
        "    (5, \"Eve\", 9000, 2),\n",
        "    (6, \"Frank\", 7500, 2),\n",
        "]\n",
        "\n",
        "dept_data = [\n",
        "    (1, \"HR\"),\n",
        "    (2, \"IT\"),\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "emp_df = spark.createDataFrame(emp_data, [\"id\", \"name\", \"salary\", \"dept_id\"])\n",
        "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "Using JOIN + PARTITION BY + ROW_NUMBER()\n",
        "# Define Window Specification\n",
        "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Rank salaries department-wise\n",
        "ranked_df = emp_df.withColumn(\"rnk\", row_number().over(window_spec))\n",
        "\n",
        "# Filter for 2nd highest salary\n",
        "second_highest_df = ranked_df.filter(col(\"rnk\") == 2)\n",
        "\n",
        "# Join with Department DataFrame to get dept_name\n",
        "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
        "\n",
        "# Show Result\n",
        "result_df.show()\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNjsnBK0PjxB"
      },
      "outputs": [],
      "source": [
        "#in any folder path type cmd and then jupyter notebook then jupyter notebook will open in that folder\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rank\n",
        "from pyspark.sql.window import Window\n",
        "import sys\n",
        "\n",
        "sys.path.append(r'C:\\Users\\khila\\AppData\\Roaming\\Python\\Python313\\site-packages')\n",
        "\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Second Highest Salary\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.python.worker.max\", \"2\")\n",
        "\n",
        "\n",
        "# Sample data for employees and departments\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 5000, 1),\n",
        "    (2, \"Bob\", 7000, 1),\n",
        "    (3, \"Charlie\", 6000, 1),\n",
        "    (4, \"David\", 8000, 2),\n",
        "    (5, \"Eve\", 9000, 2),\n",
        "    (6, \"Frank\", 7500, 2),\n",
        "]\n",
        "\n",
        "dept_data = [\n",
        "    (1, \"HR\"),\n",
        "    (2, \"IT\"),\n",
        "]\n",
        "\n",
        "# Define schema for employee and department data\n",
        "emp_columns = [\"id\", \"name\", \"salary\", \"dept_id\"]\n",
        "dept_columns = [\"dept_id\", \"dept_name\"]\n",
        "\n",
        "# Create DataFrames\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "# Window specification to rank employees by salary within each department\n",
        "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Add rank to the employee DataFrame\n",
        "ranked_emp_df = emp_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to get second highest salary (rank = 2)\n",
        "second_highest_df = ranked_emp_df.filter(col(\"rank\") == 2)\n",
        "\n",
        "# Join with department DataFrame to get the department name\n",
        "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
        "\n",
        "# Show the result\n",
        "result_df.limit(10).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ6thq56PjxB",
        "outputId": "af2a67b6-91ae-4ba5-b53f-91c09423deb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+---------+\n",
            "| id|   name|salary|dept_name|\n",
            "+---+-------+------+---------+\n",
            "|  3|Charlie|  6000|       HR|\n",
            "|  4|  David|  8000|       IT|\n",
            "+---+-------+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#in any folder path type cmd and then jupyter notebook then jupyter notebook will open in that folder\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Second Highest Salary\") \\\n",
        "    .getOrCreate()\n",
        "# Sample data for employees and departments\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 5000, 1),\n",
        "    (2, \"Bob\", 7000, 1),\n",
        "    (3, \"Charlie\", 6000, 1),\n",
        "    (4, \"David\", 8000, 2),\n",
        "    (5, \"Eve\", 9000, 2),\n",
        "    (6, \"Frank\", 7500, 2),\n",
        "]\n",
        "\n",
        "dept_data = [\n",
        "    (1, \"HR\"),\n",
        "    (2, \"IT\"),\n",
        "]\n",
        "\n",
        "# Define schema for employee and department data\n",
        "emp_columns = [\"id\", \"name\", \"salary\", \"dept_id\"]\n",
        "dept_columns = [\"dept_id\", \"dept_name\"]\n",
        "\n",
        "# Create DataFrames\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "# Window specification to rank employees by salary within each department\n",
        "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Add rank to the employee DataFrame\n",
        "ranked_emp_df = emp_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to get second highest salary (rank = 2)\n",
        "second_highest_df = ranked_emp_df.filter(col(\"rank\") == 2)\n",
        "\n",
        "# Join with department DataFrame to get the department name\n",
        "result_df = second_highest_df.join(dept_df, \"dept_id\").select(\"id\", \"name\", \"salary\", \"dept_name\")\n",
        "# Show the result\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wcpsP37PjxC",
        "outputId": "f65f037f-65b4-40f8-931d-361819b0b71b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+----+------+-----------+-------+\n",
            "|empno|   name| sal|Deptno|nexthighest|saldiff|\n",
            "+-----+-------+----+------+-----------+-------+\n",
            "|    1|  Radha|3000|    10|       2000|   1000|\n",
            "|    2|Kirshna|2000|    10|       1000|   1000|\n",
            "|    3|   rama|1000|    10|       NULL|   1000|\n",
            "|    1|Venkata|6000|    20|       4000|   2000|\n",
            "|    2|Laxmana|4000|    20|       2000|   2000|\n",
            "|    3|  Laxmi|2000|    20|       NULL|   2000|\n",
            "+-----+-------+----+------+-----------+-------+\n",
            "\n",
            "+-----+-------+----+------+-----------+\n",
            "|empno|   name| sal|Deptno|nexthighest|\n",
            "+-----+-------+----+------+-----------+\n",
            "|    1|  Radha|3000|    10|       2000|\n",
            "|    2|Kirshna|2000|    10|       1000|\n",
            "|    3|   rama|1000|    10|          0|\n",
            "|    1|Venkata|6000|    20|       4000|\n",
            "|    2|Laxmana|4000|    20|       2000|\n",
            "|    3|  Laxmi|2000|    20|          0|\n",
            "+-----+-------+----+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, lead, when\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Next Highest Salary\") \\\n",
        "    .getOrCreate()\n",
        "# Input Data\n",
        "data = [\n",
        "    (1, \"Radha\", 3000, 10),\n",
        "    (2, \"Kirshna\", 2000, 10),\n",
        "    (3, \"rama\", 1000, 10),\n",
        "    (1, \"Venkata\", 6000, 20),\n",
        "    (2, \"Laxmana\", 4000, 20),\n",
        "    (3, \"Laxmi\", 2000, 20)\n",
        "]\n",
        "\n",
        "# Schema\n",
        "columns = [\"empno\", \"name\", \"sal\", \"Deptno\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define Window Specification\n",
        "window_spec = Window.partitionBy(\"Deptno\").orderBy(col(\"sal\").desc())\n",
        "\n",
        "# Calculate next highest salary using lead function\n",
        "df = df.withColumn(\"nexthighest\", lead(\"sal\").over(window_spec))\n",
        "\n",
        "# Calculate salary difference\n",
        "df_saldiff = df.withColumn(\"saldiff\", when(col(\"nexthighest\").isNull(), col(\"sal\"))\n",
        "                   .otherwise(col(\"sal\") - col(\"nexthighest\")))\n",
        "\n",
        "# Fill null nexthighest values with 0 (as per requirement)\n",
        "df_nexthighest = df.withColumn(\"nexthighest\", when(col(\"nexthighest\").isNull(), 0).otherwise(col(\"nexthighest\")))\n",
        "\n",
        "# Show Output\n",
        "df_saldiff.show()\n",
        "df_nexthighest.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "l5KA9RgvPwt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
>>>>>>> e882a07789b3b5da4d6664e4a7158918732ff5e9
  },
  "nbformat": 4,
  "nbformat_minor": 0
}