{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coforge DE Interview skills(Spark and SQL) 11 jan 24\n",
    "\n",
    "#1. What is spark architecture?\n",
    "\"\"\"Apache Spark Architecture:\n",
    "1. Driver: The driver is the main control process that creates the SparkContext, connects to the cluster, and coordinates the execution of tasks. It translates the user program into tasks and schedules them to run on executors.\n",
    "2. Executors: Executors are worker nodes that run individual tasks in a distributed manner. Each executor has its own JVM and runs tasks assigned by the driver. Executors also provide in-memory storage for RDDs that are cached by user programs through SparkContext.\n",
    "3. Cluster Manager: The cluster manager is responsible for managing the resources across the cluster. Spark supports various cluster managers like Standalone, Apache Mesos, kubernates and Hadoop YARN. The cluster manager allocates resources to the driver and executors.\n",
    "4. RDD (Resilient Distributed Dataset): RDD is the fundamental data structure of Spark. It is an immutable distributed collection of objects that can be processed in parallel. RDDs support two types of operations: transformations (e.g., map, filter) and actions (e.g., count, collect).\n",
    "5. DAG (Directed Acyclic Graph): Spark creates a logical execution plan in the form of a DAG. The DAG represents the sequence of transformations applied to the data. The DAG scheduler divides the operators into stages of tasks and submits them to the task scheduler.\n",
    "6. Task Scheduler: The task scheduler is responsible for scheduling tasks on the executors. It takes the stages from the DAG scheduler and assigns tasks to executors based on data locality and resource availability.\n",
    "In summary, Spark architecture consists of a driver, executors, cluster manager, RDDs, DAG, and task scheduler, working together to process large-scale data in a distributed and fault-tolerant manner.\n",
    "\"\"\"\n",
    "\n",
    "#How does Spark distribute tasks for processing a 1 GB file, and how are tasks and executors allocated during execution?\n",
    "'''Spark divides the 1 GB file into partitions (default is 128 MB per partition for HDFS), creating about 8 partitions for a 1 GB file. Each partition becomes a task.\n",
    "\n",
    "Executors: Each executor processes multiple tasks based on available cores.\n",
    "Tasks per Executor: If an executor has 4 cores, it can run 4 tasks in parallel.'''\n",
    "\n",
    "# What is spark-submit explain in details?\n",
    "'''\n",
    "spark-submit is a command-line tool used to launch Spark applications on a cluster or locally. It handles resource allocation, configuration, and submission of jobs to Spark clusters like YARN, Kubernetes, Mesos, or standalone mode.\n",
    "\n",
    "Key Responsibilities:\n",
    "Distribute Application Code: Sends your Spark application JAR or Python script to the cluster.\n",
    "Resource Management: Allocates memory and CPU resources for the driver and executors.\n",
    "Configuration Management: Sets properties like application name, Spark master, deploy mode, and more.\n",
    "Job Execution: Manages the execution of tasks and monitors job progress.\n",
    "Command Syntax:\n",
    "\n",
    "spark-submit \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --class <main-class> \\\n",
    "  --executor-memory <memory> \\\n",
    "  --executor-cores <cores> \\\n",
    "  application.jar <application-arguments>\n",
    "Common Parameters:\n",
    "Parameter\tDescription\n",
    "--master\tSpecifies the cluster manager (local, yarn, etc.)\n",
    "--deploy-mode\tDeployment mode (client or cluster)\n",
    "--class\tMain class of the application (for Java/Scala)\n",
    "--executor-memory\tMemory per executor (e.g., 2G)\n",
    "--executor-cores\tNumber of cores per executor\n",
    "--num-executors\tNumber of executors to launch\n",
    "--conf\tSet configuration properties (e.g., spark.executor.memory)\n",
    "Example Usage:\n",
    "Running a Spark Application Locally:\n",
    "\n",
    "spark-submit \n",
    "  --master local[2] \n",
    "  my_spark_app.py\n",
    "\n",
    "Running on a YARN Cluster:\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --executor-memory 4G \\\n",
    "  --executor-cores 4 \\\n",
    "  my_spark_app.py\n",
    "Execution Modes:\n",
    "Client Mode: Driver runs on the machine where spark-submit is executed. Suitable for local testing.\n",
    "Cluster Mode: Driver runs on a worker node within the cluster. Ideal for large-scale production jobs.\n",
    "\n",
    "Important Points:\n",
    "Driver Program: Manages job execution and task scheduling.\n",
    "Executors: Run the actual tasks on worker nodes and store intermediate data.\n",
    "Partitions: Spark splits input data into smaller partitions for distributed processing.\n",
    "By leveraging spark-submit, Spark achieves scalability and parallel processing across large datasets efficiently.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7839, 'KING', 'PRESIDENT', None, datetime.datetime(1981, 11, 17, 0, 0), 5100.0, None, 20)\n"
     ]
    }
   ],
   "source": [
    "# Synechron Interview Questions on Database for Data Engineer\n",
    "\n",
    "# Make Oracle DB connection in Python and write a query?\n",
    "\n",
    "#%pip install cx_Oracle\n",
    "import cx_Oracle\n",
    "\n",
    "dsn_tns=cx_Oracle.makedsn('localhost','1521',service_name='orcl')\n",
    "conn=cx_Oracle.connect(user='SivaAcademy',password='pass',dsn=dsn_tns) #while connecting only give req pass\n",
    "c=conn.cursor()\n",
    "c.execute('select * from emp')\n",
    "print(c.fetchone()) # we can use fetchall() for all or  for row in c:\n",
    "    #print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the sales amount each year, product-wise  [Sales_data table]\n",
    "# Solution 1\n",
    "query='''WITH cte AS \n",
    "          (\n",
    "    SELECT product_id, \n",
    "           EXTRACT(YEAR FROM sales_date) AS sale_year, \n",
    "           SUM(amount) OVER (PARTITION BY product_id, EXTRACT(YEAR FROM sales_date)) AS cumulative_sum,\n",
    "           DENSE_RANK() OVER (PARTITION BY product_id ORDER BY EXTRACT(YEAR FROM sales_date)) AS rn\n",
    "    FROM Sales_data\n",
    ")\n",
    "SELECT product_id, sale_year, cumulative_sum\n",
    "FROM cte\n",
    "GROUP BY product_id, sale_year, cumulative_sum\n",
    "ORDER BY product_id, sale_year'''\n",
    "c.execute(query)\n",
    "for i in c:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P1', 2023, 1301.15)\n",
      "('P10', 2023, 1085.16)\n",
      "('P2', 2023, 1310.25)\n",
      "('P3', 2023, 1235.66)\n",
      "('P4', 2023, 1064.48)\n",
      "('P5', 2023, 1259.51)\n",
      "('P6', 2023, 1310.55)\n",
      "('P7', 2023, 1154.54)\n",
      "('P8', 2023, 1220.01)\n",
      "('P9', 2023, 1383.75)\n"
     ]
    }
   ],
   "source": [
    "# solution 2\n",
    "query='''SELECT product_id AS prod_id, \n",
    "    EXTRACT(YEAR FROM sales_date) AS sale_year, \n",
    "    SUM(amount) AS total_sales\n",
    "FROM Sales_data\n",
    "GROUP BY product_id, EXTRACT(YEAR FROM sales_date)\n",
    "ORDER BY product_id, sale_year'''\n",
    "c.execute(query)\n",
    "for i in c:\n",
    "    print(i)\n",
    "'''\n",
    "EXTRACT(YEAR FROM sale_dt): 2025\n",
    "TO_CHAR(sale_dt, 'YYYY'): '2025' (as a string)\n",
    "TRUNC(sale_dt, 'YYYY'): 01-JAN-2025 (the first day of the year)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P6',)\n",
      "('P8',)\n",
      "('P5',)\n",
      "('P1',)\n",
      "('P2',)\n",
      "('P3',)\n",
      "('P9',)\n",
      "('P10',)\n",
      "('P4',)\n",
      "('P7',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnalysis:\\nSuppose I have written this query\\nSELECT DISTINCT product_id \\nFROM Sales_data \\nWHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\\n\\nthis will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Find products that were not sold in the last 7 months.\n",
    "query='''SELECT product_id\n",
    "FROM Sales_data\n",
    "WHERE product_id NOT IN (\n",
    "    SELECT DISTINCT product_id \n",
    "    FROM Sales_data \n",
    "    WHERE sales_date >= ADD_MONTHS(SYSDATE, -7)) group by product_id'''\n",
    "c.execute(query)\n",
    "for i in c:\n",
    "    print(i)\n",
    "\n",
    "'''\n",
    "Analysis:\n",
    "Suppose I have written this query\n",
    "SELECT DISTINCT product_id \n",
    "FROM Sales_data \n",
    "WHERE sales_date <= ADD_MONTHS(SYSDATE, -7)\n",
    "\n",
    "this will show sales before 7 months ago but it can't exclude the products that were not at all sold in the last 7 months. It may includes 1 sale in last 7 months.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Berribot Interview Questions on ETL and pyspark for Wipro\n",
    "# Schedule AWS databrics spark job daily in midnight script\n",
    "'''\n",
    "To schedule an AWS Databricks Spark job to run daily at midnight, you can use Databricks Workflows (preferred) or an external scheduling tool like AWS Lambda or CloudWatch. Below is an example script for each approach:\n",
    "\n",
    "1. Using Databricks Workflows (Recommended)\n",
    "Create a Databricks Job:\n",
    "\n",
    "Go to Databricks Workspace.\n",
    "Navigate to Workflows > Jobs.\n",
    "Click Create Job.\n",
    "Add your notebook or Spark job details.\n",
    "Configure job clusters and libraries if required.\n",
    "Schedule the Job:\n",
    "\n",
    "In the Schedule tab, set the schedule to:\n",
    "Daily\n",
    "Start time: 00:00\n",
    "Save the workflow.\n",
    "2. Using AWS CloudWatch with Databricks REST API\n",
    "Prerequisites:\n",
    "A Databricks personal access token.\n",
    "Databricks job ID or notebook path.\n",
    "Python Script:\n",
    "\n",
    "import requests\n",
    "\n",
    "# Databricks Workspace URL\n",
    "DATABRICKS_HOST = \"https://<databricks-instance>.cloud.databricks.com\"\n",
    "\n",
    "# Databricks Personal Access Token\n",
    "DATABRICKS_TOKEN = \"<your-databricks-token>\"\n",
    "\n",
    "# Job ID\n",
    "JOB_ID = \"<your-job-id>\"\n",
    "\n",
    "# Trigger Job Function\n",
    "def trigger_job():\n",
    "    url = f\"{DATABRICKS_HOST}/api/2.1/jobs/run-now\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"job_id\": JOB_ID}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Job triggered successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger job: {response.content}\")\n",
    "\n",
    "# Run the function\n",
    "trigger_job()\n",
    "Schedule via AWS CloudWatch:\n",
    "Open AWS Management Console.\n",
    "Go to CloudWatch > Rules.\n",
    "Create a new rule:\n",
    "Event Source: Schedule.\n",
    "Schedule Expression: cron(0 0 * * ? *) (midnight daily).\n",
    "Target:\n",
    "Choose AWS Lambda.\n",
    "Add the Python script to your Lambda function.\n",
    "3. Using Crontab on EC2\n",
    "If you prefer running the script from an EC2 instance:\n",
    "\n",
    "SSH into the EC2 instance.\n",
    "Edit the crontab:\n",
    "\n",
    "crontab -e\n",
    "Add the following entry to trigger the script at midnight:\n",
    "\n",
    "0 0 * * * python3 /path/to/trigger_databricks_job.py\n",
    "Which approach do you prefer? I can guide you further.'''\n",
    "\n",
    "\n",
    "#Write AWS databrics notebook to calculate rolling average over time periods for q time series data set.\n",
    "\n",
    "#Input : time period '7 days' dataset: 'stock_prices''\n",
    "\n",
    "#Output: dataframe with 7-day rolling averages\n",
    "\n",
    "'''Here is a Databricks notebook code that calculates the 7-day rolling average for a time series dataset (stock_prices) using PySpark.\n",
    "\n",
    "Code for the Databricks Notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, window\n",
    "\n",
    "# Initialize SparkSession (this is usually already set up in Databricks)\n",
    "spark = SparkSession.builder.appName(\"RollingAverage\").getOrCreate()\n",
    "\n",
    "# Sample dataset: Replace this with your actual dataset\n",
    "data = [\n",
    "    (\"2025-01-01\", 100),\n",
    "    (\"2025-01-02\", 110),\n",
    "    (\"2025-01-03\", 105),\n",
    "    (\"2025-01-04\", 120),\n",
    "    (\"2025-01-05\", 130),\n",
    "    (\"2025-01-06\", 125),\n",
    "    (\"2025-01-07\", 140),\n",
    "    (\"2025-01-08\", 145),\n",
    "    (\"2025-01-09\", 150),\n",
    "    (\"2025-01-10\", 155)\n",
    "]\n",
    "\n",
    "# Define schema and create a DataFrame\n",
    "columns = [\"date\", \"price\"]\n",
    "stock_prices_df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Convert 'date' column to TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "stock_prices_df = stock_prices_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Calculate 7-day rolling average\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define window specification\n",
    "rolling_window = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Add rolling average column\n",
    "result_df = stock_prices_df.withColumn(\"7_day_avg\", avg(\"price\").over(rolling_window))\n",
    "\n",
    "# Show the results\n",
    "result_df.show()\n",
    "Explanation of the Code\n",
    "Dataset Initialization:\n",
    "\n",
    "Replace the data variable with your actual dataset from stock_prices.\n",
    "Date Conversion:\n",
    "\n",
    "Convert the date column to DateType for proper time series handling.\n",
    "Define Window Specification:\n",
    "\n",
    "Use Window.orderBy(\"date\").rowsBetween(-6, 0) to create a rolling window of 7 days (current row and 6 previous rows).\n",
    "Calculate Rolling Average:\n",
    "\n",
    "Use the avg function to compute the 7-day rolling average.\n",
    "Result:\n",
    "\n",
    "A new column 7_day_avg is added to the DataFrame showing the rolling average.\n",
    "Sample Output\n",
    "date\tprice\t7_day_avg\n",
    "2025-01-01\t100\t100.0\n",
    "2025-01-02\t110\t105.0\n",
    "2025-01-03\t105\t105.0\n",
    "2025-01-04\t120\t108.75\n",
    "2025-01-05\t130\t113.0\n",
    "2025-01-06\t125\t115.0\n",
    "2025-01-07\t140\t118.57\n",
    "2025-01-08\t145\t125.0\n",
    "2025-01-09\t150\t130.0\n",
    "2025-01-10\t155\t135.71\n",
    "Let me know if you'd like help loading the actual dataset from S3 or a database!'''\n",
    "\n",
    "\n",
    "#Partition a large dataframe in databrics by a given column and write to different parquet files.\n",
    "\n",
    "#Input: column: 'region'\n",
    "#Output: Partitioned parquet files by 'region' \n",
    "\n",
    "#Eg 2 Input: column: 'category'\n",
    "#Output: Partioned Parquet files by 'category'\n",
    "\n",
    "'''\n",
    "Here’s how to partition a large DataFrame in Databricks by a given column (e.g., region or category) and write it to different Parquet files:\n",
    "\n",
    "Code for Partitioning and Writing to Parquet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession (this is usually pre-configured in Databricks)\n",
    "spark = SparkSession.builder.appName(\"PartitionedParquet\").getOrCreate()\n",
    "\n",
    "# Sample dataset: Replace this with your actual dataset\n",
    "data = [\n",
    "    (\"East\", \"Electronics\", 100),\n",
    "    (\"West\", \"Furniture\", 200),\n",
    "    (\"East\", \"Clothing\", 150),\n",
    "    (\"North\", \"Electronics\", 300),\n",
    "    (\"South\", \"Clothing\", 250),\n",
    "    (\"West\", \"Electronics\", 400),\n",
    "    (\"East\", \"Furniture\", 350)\n",
    "]\n",
    "\n",
    "# Define schema and create a DataFrame\n",
    "columns = [\"region\", \"category\", \"sales\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "(((((((\n",
    "We can create separate explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "\n",
    "# Apply the schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    ")))))))\n",
    "\n",
    "# Partition column: Replace 'region' with 'category' as needed\n",
    "partition_column = \"region\"  # Change to \"category\" for the second example\n",
    "\n",
    "# Output path for Parquet files (e.g., S3, DBFS)\n",
    "output_path = \"dbfs:/mnt/output/partitioned_data\"  # Change as required\n",
    "\n",
    "# Write DataFrame to partitioned Parquet files\n",
    "df.write.partitionBy(partition_column).parquet(output_path)\n",
    "\n",
    "print(f\"Data written to Parquet files partitioned by {partition_column}\")\n",
    "Explanation\n",
    "Input Dataset:\n",
    "\n",
    "Replace the sample data with your actual DataFrame from a source like S3, Delta Lake, or a SQL database.\n",
    "Partitioning Column:\n",
    "\n",
    "Set the partition_column to the column by which you want to partition the Parquet files (e.g., region or category).\n",
    "Output Path:\n",
    "\n",
    "Specify the output_path where the partitioned Parquet files should be written. This could be:\n",
    "dbfs:/mnt/output/... for Databricks File System.\n",
    "s3://bucket-name/... for AWS S3.\n",
    "Partition and Write:\n",
    "\n",
    "Use the partitionBy() method to group data by the specified column, creating a folder for each unique value in the column.\n",
    "Parquet files for each partition are written inside the corresponding folder.\n",
    "Output Directory Structure\n",
    "If partition_column = 'region', the Parquet files are saved as:\n",
    "\n",
    "\n",
    "/mnt/output/partitioned_data/\n",
    "├── region=East/\n",
    "│   ├── part-00000.parquet\n",
    "│   └── part-00001.parquet\n",
    "├── region=West/\n",
    "│   ├── part-00000.parquet\n",
    "│   └── part-00001.parquet\n",
    "├── region=North/\n",
    "│   ├── part-00000.parquet\n",
    "├── region=South/\n",
    "│   ├── part-00000.parquet\n",
    "For partition_column = 'category', the folder names will follow category=Electronics, category=Furniture, etc.\n",
    "\n",
    "Verify Written Data\n",
    "To verify the Parquet files:\n",
    "\n",
    "\n",
    "# Load the partitioned Parquet files\n",
    "partitioned_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show data to verify\n",
    "partitioned_df.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are a data engineer explain this project in detail use Pyspark and python pandas and no SQL whereever required. And also focused on horizon insight database oracle\n",
    "'''\n",
    "I'll help explain this Horizon Insights data engineering project and provide implementation suggestions using PySpark and Pandas. Let me break down the architecture and provide some code examples.\n",
    "\n",
    "The project appears to be a comprehensive data platform serving since 2017 with impressive metrics: 800+ regular users, 5 queries/second average throughput, and 18TB+ of accumulated data. Let me break this down into key components:\n",
    "\n",
    "1. Data Sources & Ingestion:\n",
    "Let's handle the various data sources using PySpark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HorizonInsights\") \\\n",
    "    .config(\"spark.jars\", \"oracle-jdbc.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to read from various sources\n",
    "def ingest_service_data(source_type, path):\n",
    "    if source_type == \"jenkins\":\n",
    "        # Read Jenkins JSON data\n",
    "        df = spark.read.json(path)\n",
    "    elif source_type == \"jira\":\n",
    "        # Read Jira REST API data\n",
    "        df = spark.read.json(path)\n",
    "    elif source_type == \"qtest\":\n",
    "        # Read qTest data\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    return df\n",
    "```\n",
    "\n",
    "2. Data Transformation & Loading:\n",
    "Given the metrics (140K+ stories/month, 18K+ bugs/month), we need robust transformation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def transform_metrics_data(df, metric_type):\n",
    "    if metric_type == \"stories\":\n",
    "        # Transform stories data\n",
    "        df = df.withColumn(\"month\", month(col(\"created_date\"))) \\\n",
    "               .withColumn(\"year\", year(col(\"created_date\"))) \\\n",
    "               .groupBy(\"month\", \"year\") \\\n",
    "               .agg(count(\"story_id\").alias(\"story_count\"))\n",
    "    \n",
    "    elif metric_type == \"jenkins_jobs\":\n",
    "        # Transform Jenkins jobs data\n",
    "        df = df.withColumn(\"status\", when(col(\"result\") == \"SUCCESS\", 1).otherwise(0)) \\\n",
    "               .groupBy(\"job_name\") \\\n",
    "               .agg(\n",
    "                   sum(\"status\").alias(\"successful_builds\"),\n",
    "                   count(\"*\").alias(\"total_builds\")\n",
    "               )\n",
    "    return df\n",
    "```\n",
    "\n",
    "3. Oracle Database Integration:\n",
    "For the Horizon Insights DB, which appears to be Oracle-based:\n",
    "\n",
    "```python\n",
    "def write_to_oracle(df, table_name):\n",
    "    # Oracle connection properties\n",
    "    oracle_properties = {\n",
    "        \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "        \"url\": \"jdbc:oracle:thin:@//hostname:port/service\",\n",
    "        \"user\": \"username\",\n",
    "        \"password\": \"password\"\n",
    "    }\n",
    "    \n",
    "    # Write to Oracle\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", oracle_properties[\"url\"]) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", oracle_properties[\"user\"]) \\\n",
    "        .option(\"password\", oracle_properties[\"password\"]) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "```\n",
    "\n",
    "4. Reporting Pipeline:\n",
    "For the reporting requirements (MetricsHQ, SDLC, Compliance):\n",
    "\n",
    "```python\n",
    "def generate_metrics_report():\n",
    "    # Create a reporting dataframe\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    # Get Jenkins metrics\n",
    "    jenkins_metrics = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', timestamp) as month,\n",
    "            COUNT(*) as total_jobs,\n",
    "            SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_jobs\n",
    "        FROM jenkins_jobs\n",
    "        GROUP BY DATE_TRUNC('month', timestamp)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert to pandas for reporting\n",
    "    metrics_df['jenkins_success_rate'] = jenkins_metrics.toPandas()['successful_jobs'] / \\\n",
    "                                       jenkins_metrics.toPandas()['total_jobs']\n",
    "    \n",
    "    return metrics_df\n",
    "```\n",
    "\n",
    "Key Features to Note:\n",
    "1. Volume Handling:\n",
    "- Processing 470K+ Jenkins jobs/month\n",
    "- Managing 350K+ Jobs/Month from Tower\n",
    "- Handling 300K+ qTest runs/month\n",
    "\n",
    "2. Performance Optimization:\n",
    "```python\n",
    "# Configure Spark for optimal performance\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
    "```\n",
    "\n",
    "3. Data Quality Checks:\n",
    "```python\n",
    "def data_quality_check(df, table_name):\n",
    "    # Check for nulls\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "    \n",
    "    # Log quality metrics\n",
    "    print(f\"Quality checks for {table_name}\")\n",
    "    null_counts.show()\n",
    "    print(f\"Duplicate records: {duplicate_count}\")\n",
    "```\n",
    "\n",
    "The system seems to be processing a significant amount of DevOps and testing data, with various downstream consumers like Jira, Release Manager, and TechABS. The architecture uses SSIS for data ingestion, suggesting this is likely an enterprise-level solution.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
