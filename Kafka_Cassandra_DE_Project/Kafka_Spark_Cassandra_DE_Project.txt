Steps to perform some work on Spark Streaming, Kafka, Cassandra End to End Project

--------------------------------------
Version(It is not mandatory)

spark-2.1.1-bin-hadoop2.7

apache-cassandra-3.9

kafka_2.11-0.9.0.0

all these services running on which application 
28432 Master     ---Spark Master node
43815 Jps        ---Java Process Status
31785 Kafka      ---Kafka Server process
28602 Worker     ---Spark Worker node
30492 QuorumPeerMain---This process is part of Zookeeper

Zookeeper is used for distributed coordination and synchronization, often serving as the coordination service for services like Kafka, Spark, and Hadoop. QuorumPeerMain is a Zookeeper server process that manages the state and coordination of the nodes in a Zookeeper ensemble.

=========================================================================
This one will do 1st
2. Start Spark 

sbin/start-all.sh


1. Start Kafka 



Zookeeper

bin/zookeeper-server-start.sh config/zookeeper.properties



Kafka

bin/kafka-server-start.sh config/server.properties



Create topic

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic

bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic mytopic  (In newer version)
###--bootstrap-server  instead of --zookeeper in new version Kafka 3.9.0 (kafka_2.13-3.9.0)

bin/kafka-topics.sh --list --zookeeper localhost:2181 ###Since you're using Kafka 3.9.0 (kafka_2.13-3.9.0), it should be running in KRaft mode, meaning you no longer need Zookeeper for managing Kafka. In this mode, you should use the --bootstrap-server flag rather than the --zookeeper flag. and ip of --bootstrap-server is 9092

    



Producer 

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mytopic



Consumer

bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic mytopic --from-beginning

or

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic mytopic --from-beginning

=========================================================================

2. Start Spark 

sbin/start-all.sh

=========================================================================

3. Start Cassandra



bin/cassandra -f 



create keyspace sparkdata with replication ={'class':'SimpleStrategy','replication_factor':1};



use sparkdata;



CREATE TABLE cust_data (fname text , lname text , url text,product text , cnt counter ,primary key (fname,lname,url,product));

select * from cust_data;



=========================================================================

Spark Kafka Cassandra Streaming Code

Start the Spark Shell with below command 

./bin/spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.11:2.0.2","org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0"


this one is as per my cassandra 5.0 and kafka version:-  Kafka 3.9.0 (kafka_2.13-3.9.0)---2.13 is scala version
./bin/spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.13:3.5.0","org.apache.spark:spark-streaming-kafka-0-10_2.13:3.2.1"
###it works

Run this code in the spark shell

./bin/spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.13:3.5.1", \
"org.apache.spark:spark-streaming-kafka-0-10_2.13:3.5.3"


========================Practical work on Project========================
Run this code in the spark shell

./bin/spark-shell --packages "com.datastax.spark:spark-cassandra-connector_2.13:3.5.1", \
"org.apache.spark:spark-streaming-kafka-0-10_2.13:3.5.3"

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org. apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.kafka010._
import com.datastax.spark.connector._
###import com.datastax. spark.connector.SomeColumns
###import com.datastax.spark.connector.cql.CassandraConnector
###import com.datastax.spark.connector.streaming._

val sparkConf = new SparkConf().setAppName ("KafkaSparkStreaming").set("spark. cassandra. connection.host", "127. 0.0.1")

spark.stop

val ssc= new StreamingContext(sparkConf, Seconds(5))

val topicpMap = "mytopic".split (",").map((_, 1.toInt)).toMap

val lines = KafkaUtils.createStream(ssc, "localhost: 2181", "sparkgroup", topicpMap).map(_._2)

lines.print();

lines.map(line => { val arr = line.split(","); (arr(0),arr(1), arr(2),arr(3),arr (4)) }) .saveToCassandra("sparkdata", "cust_data", SomeColumns("fname", "lname", "url", "product", "cnt"))

ssc.start

ssc.awaitTermination